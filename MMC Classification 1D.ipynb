{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f8ea7d41",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c32e9d59",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-06-22 14:15:00.164566: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "'''--------- 1. Data Manipulation ---------'''\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "'''--------- 2. Data Preprocessing ---------'''\n",
    "from sklearn.preprocessing import LabelEncoder #Encode Non-numeric Var\n",
    "from sklearn.preprocessing import MinMaxScaler # Feature Scaling\n",
    "from sklearn.model_selection import train_test_split #Train Test Validation Split\n",
    "'''--------- 3. Data Visualization ---------'''\n",
    "import matplotlib.pyplot as plt\n",
    "'''--------- 4. Model Training ---------'''\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Conv1D, Conv2D, MaxPooling1D, MaxPooling2D, Flatten, Dropout, Dense\n",
    "from tensorflow.keras.layers import BatchNormalization\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "'''--------- 5. Model Evluation ---------'''\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, classification_report, ConfusionMatrixDisplay"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07327a4a",
   "metadata": {},
   "source": [
    "## 1. Dataset\n",
    "### Dataset 1\n",
    "**About dataset**\n",
    "- 900 ~30 second audio clips gathered from AllMusic API\n",
    "- Annotated into 4 quadrants based on valance and arousal according to Russell's model\n",
    "- Audios are organized in 4 folders (Q1 to Q4)\n",
    "- Equally stratified dataset with each classes 250 songs\n",
    "\n",
    "**Acknowledgements**<br>\n",
    "Source: http://mir.dei.uc.pt/downloads.html\n",
    "\n",
    "**If you use it, please cite the following article(s):**\n",
    "\n",
    "Panda R., Malheiro R. & Paiva R. P. (2018). \"Novel audio features for music emotion recognition\". IEEE Transactions on Affective Computing (IEEE early access). DOI: 10.1109/TAFFC.2018.2820691.\n",
    "\n",
    "Panda R., Malheiro R., Paiva R. P. (2018). \"Musical Texture and Expressivity Features for Music Emotion Recognition\". 19th International Society for Music Information Retrieval Conference -- ISMIR 2018, Paris, France."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2b5fd3ba",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>file</th>\n",
       "      <th>scale</th>\n",
       "      <th>key</th>\n",
       "      <th>tempo</th>\n",
       "      <th>rms_mean</th>\n",
       "      <th>rms_var</th>\n",
       "      <th>chroma_mean</th>\n",
       "      <th>chroma_var</th>\n",
       "      <th>centroid_mean</th>\n",
       "      <th>centroid_var</th>\n",
       "      <th>...</th>\n",
       "      <th>mfcc_var_16</th>\n",
       "      <th>mfcc_mean_17</th>\n",
       "      <th>mfcc_var_17</th>\n",
       "      <th>mfcc_mean_18</th>\n",
       "      <th>mfcc_var_18</th>\n",
       "      <th>mfcc_mean_19</th>\n",
       "      <th>mfcc_var_19</th>\n",
       "      <th>mfcc_mean_20</th>\n",
       "      <th>mfcc_var_20</th>\n",
       "      <th>mood</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Dataset25/MER25/Q1/MT0009845271.mp3</td>\n",
       "      <td>major</td>\n",
       "      <td>C</td>\n",
       "      <td>129.199219</td>\n",
       "      <td>0.122140</td>\n",
       "      <td>0.001837</td>\n",
       "      <td>0.401058</td>\n",
       "      <td>0.082307</td>\n",
       "      <td>2642.221588</td>\n",
       "      <td>271537.203181</td>\n",
       "      <td>...</td>\n",
       "      <td>62.597446</td>\n",
       "      <td>-5.950494</td>\n",
       "      <td>48.630459</td>\n",
       "      <td>1.334569</td>\n",
       "      <td>50.004402</td>\n",
       "      <td>-5.540088</td>\n",
       "      <td>64.204506</td>\n",
       "      <td>1.117945</td>\n",
       "      <td>66.048088</td>\n",
       "      <td>happy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Dataset25/MER25/Q1/MT0012742379.mp3</td>\n",
       "      <td>major</td>\n",
       "      <td>A</td>\n",
       "      <td>123.046875</td>\n",
       "      <td>0.083209</td>\n",
       "      <td>0.003111</td>\n",
       "      <td>0.345329</td>\n",
       "      <td>0.090639</td>\n",
       "      <td>1801.451903</td>\n",
       "      <td>139289.262872</td>\n",
       "      <td>...</td>\n",
       "      <td>94.285316</td>\n",
       "      <td>2.379663</td>\n",
       "      <td>45.529087</td>\n",
       "      <td>-0.897912</td>\n",
       "      <td>63.964500</td>\n",
       "      <td>2.081507</td>\n",
       "      <td>75.064636</td>\n",
       "      <td>-6.191793</td>\n",
       "      <td>77.692085</td>\n",
       "      <td>happy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Dataset25/MER25/Q1/MT0009188643.mp3</td>\n",
       "      <td>major</td>\n",
       "      <td>E</td>\n",
       "      <td>99.384014</td>\n",
       "      <td>0.153985</td>\n",
       "      <td>0.001419</td>\n",
       "      <td>0.484920</td>\n",
       "      <td>0.081994</td>\n",
       "      <td>3368.652103</td>\n",
       "      <td>605562.058604</td>\n",
       "      <td>...</td>\n",
       "      <td>41.972660</td>\n",
       "      <td>0.604512</td>\n",
       "      <td>55.172565</td>\n",
       "      <td>5.850554</td>\n",
       "      <td>55.045155</td>\n",
       "      <td>-1.423651</td>\n",
       "      <td>46.238407</td>\n",
       "      <td>3.996233</td>\n",
       "      <td>56.848827</td>\n",
       "      <td>happy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Dataset25/MER25/Q1/MT0011560587.mp3</td>\n",
       "      <td>major</td>\n",
       "      <td>A</td>\n",
       "      <td>172.265625</td>\n",
       "      <td>0.149997</td>\n",
       "      <td>0.002240</td>\n",
       "      <td>0.312995</td>\n",
       "      <td>0.086087</td>\n",
       "      <td>1318.592756</td>\n",
       "      <td>189719.701093</td>\n",
       "      <td>...</td>\n",
       "      <td>60.148708</td>\n",
       "      <td>-3.539526</td>\n",
       "      <td>72.778801</td>\n",
       "      <td>-4.562426</td>\n",
       "      <td>86.510620</td>\n",
       "      <td>-4.923912</td>\n",
       "      <td>70.068069</td>\n",
       "      <td>-8.135331</td>\n",
       "      <td>110.109657</td>\n",
       "      <td>happy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Dataset25/MER25/Q1/MT0001605268.mp3</td>\n",
       "      <td>major</td>\n",
       "      <td>E</td>\n",
       "      <td>92.285156</td>\n",
       "      <td>0.124284</td>\n",
       "      <td>0.001862</td>\n",
       "      <td>0.366223</td>\n",
       "      <td>0.086975</td>\n",
       "      <td>2403.937296</td>\n",
       "      <td>546816.767836</td>\n",
       "      <td>...</td>\n",
       "      <td>50.798794</td>\n",
       "      <td>-11.507196</td>\n",
       "      <td>90.738411</td>\n",
       "      <td>0.821440</td>\n",
       "      <td>52.887070</td>\n",
       "      <td>-6.566545</td>\n",
       "      <td>59.563255</td>\n",
       "      <td>1.389795</td>\n",
       "      <td>53.002163</td>\n",
       "      <td>happy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>893</th>\n",
       "      <td>Dataset25/MER25/Q3/MT0010804974.mp3</td>\n",
       "      <td>major</td>\n",
       "      <td>A</td>\n",
       "      <td>135.999178</td>\n",
       "      <td>0.153649</td>\n",
       "      <td>0.003739</td>\n",
       "      <td>0.250131</td>\n",
       "      <td>0.098683</td>\n",
       "      <td>856.545811</td>\n",
       "      <td>118651.564210</td>\n",
       "      <td>...</td>\n",
       "      <td>144.309845</td>\n",
       "      <td>-0.304753</td>\n",
       "      <td>160.793076</td>\n",
       "      <td>2.532327</td>\n",
       "      <td>152.150955</td>\n",
       "      <td>2.110771</td>\n",
       "      <td>163.557083</td>\n",
       "      <td>-2.066999</td>\n",
       "      <td>142.535400</td>\n",
       "      <td>sad</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>894</th>\n",
       "      <td>Dataset25/MER25/Q3/MT0000796526.mp3</td>\n",
       "      <td>major</td>\n",
       "      <td>B</td>\n",
       "      <td>161.499023</td>\n",
       "      <td>0.086236</td>\n",
       "      <td>0.002395</td>\n",
       "      <td>0.322800</td>\n",
       "      <td>0.089179</td>\n",
       "      <td>2152.750986</td>\n",
       "      <td>730050.006374</td>\n",
       "      <td>...</td>\n",
       "      <td>58.887882</td>\n",
       "      <td>-10.346969</td>\n",
       "      <td>113.920319</td>\n",
       "      <td>1.367891</td>\n",
       "      <td>55.805199</td>\n",
       "      <td>-3.380867</td>\n",
       "      <td>84.275322</td>\n",
       "      <td>-2.286275</td>\n",
       "      <td>78.514404</td>\n",
       "      <td>sad</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>895</th>\n",
       "      <td>Dataset25/MER25/Q3/MT0015664499.mp3</td>\n",
       "      <td>minor</td>\n",
       "      <td>A#</td>\n",
       "      <td>99.384014</td>\n",
       "      <td>0.107483</td>\n",
       "      <td>0.002811</td>\n",
       "      <td>0.376557</td>\n",
       "      <td>0.105393</td>\n",
       "      <td>1836.995316</td>\n",
       "      <td>545339.380176</td>\n",
       "      <td>...</td>\n",
       "      <td>44.633171</td>\n",
       "      <td>2.953913</td>\n",
       "      <td>43.044193</td>\n",
       "      <td>4.850079</td>\n",
       "      <td>42.006420</td>\n",
       "      <td>-2.364613</td>\n",
       "      <td>38.483833</td>\n",
       "      <td>2.754930</td>\n",
       "      <td>31.010529</td>\n",
       "      <td>sad</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>896</th>\n",
       "      <td>Dataset25/MER25/Q3/MT0012292188.mp3</td>\n",
       "      <td>minor</td>\n",
       "      <td>G#</td>\n",
       "      <td>107.666016</td>\n",
       "      <td>0.099635</td>\n",
       "      <td>0.001647</td>\n",
       "      <td>0.370873</td>\n",
       "      <td>0.102202</td>\n",
       "      <td>853.677506</td>\n",
       "      <td>131473.967491</td>\n",
       "      <td>...</td>\n",
       "      <td>57.660786</td>\n",
       "      <td>1.490195</td>\n",
       "      <td>56.502377</td>\n",
       "      <td>-0.173234</td>\n",
       "      <td>52.937866</td>\n",
       "      <td>-0.707987</td>\n",
       "      <td>52.222660</td>\n",
       "      <td>-2.389481</td>\n",
       "      <td>50.750061</td>\n",
       "      <td>sad</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>897</th>\n",
       "      <td>Dataset25/MER25/Q3/MT0004342301.mp3</td>\n",
       "      <td>major</td>\n",
       "      <td>G</td>\n",
       "      <td>117.453835</td>\n",
       "      <td>0.145235</td>\n",
       "      <td>0.005659</td>\n",
       "      <td>0.298197</td>\n",
       "      <td>0.090131</td>\n",
       "      <td>1561.606293</td>\n",
       "      <td>386055.207888</td>\n",
       "      <td>...</td>\n",
       "      <td>111.426071</td>\n",
       "      <td>9.588392</td>\n",
       "      <td>53.757309</td>\n",
       "      <td>-1.011325</td>\n",
       "      <td>79.747833</td>\n",
       "      <td>10.141459</td>\n",
       "      <td>65.912148</td>\n",
       "      <td>-2.369084</td>\n",
       "      <td>65.630173</td>\n",
       "      <td>sad</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>898 rows × 59 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                    file  scale key       tempo  rms_mean  \\\n",
       "0    Dataset25/MER25/Q1/MT0009845271.mp3  major   C  129.199219  0.122140   \n",
       "1    Dataset25/MER25/Q1/MT0012742379.mp3  major   A  123.046875  0.083209   \n",
       "2    Dataset25/MER25/Q1/MT0009188643.mp3  major   E   99.384014  0.153985   \n",
       "3    Dataset25/MER25/Q1/MT0011560587.mp3  major   A  172.265625  0.149997   \n",
       "4    Dataset25/MER25/Q1/MT0001605268.mp3  major   E   92.285156  0.124284   \n",
       "..                                   ...    ...  ..         ...       ...   \n",
       "893  Dataset25/MER25/Q3/MT0010804974.mp3  major   A  135.999178  0.153649   \n",
       "894  Dataset25/MER25/Q3/MT0000796526.mp3  major   B  161.499023  0.086236   \n",
       "895  Dataset25/MER25/Q3/MT0015664499.mp3  minor  A#   99.384014  0.107483   \n",
       "896  Dataset25/MER25/Q3/MT0012292188.mp3  minor  G#  107.666016  0.099635   \n",
       "897  Dataset25/MER25/Q3/MT0004342301.mp3  major   G  117.453835  0.145235   \n",
       "\n",
       "      rms_var  chroma_mean  chroma_var  centroid_mean   centroid_var  ...  \\\n",
       "0    0.001837     0.401058    0.082307    2642.221588  271537.203181  ...   \n",
       "1    0.003111     0.345329    0.090639    1801.451903  139289.262872  ...   \n",
       "2    0.001419     0.484920    0.081994    3368.652103  605562.058604  ...   \n",
       "3    0.002240     0.312995    0.086087    1318.592756  189719.701093  ...   \n",
       "4    0.001862     0.366223    0.086975    2403.937296  546816.767836  ...   \n",
       "..        ...          ...         ...            ...            ...  ...   \n",
       "893  0.003739     0.250131    0.098683     856.545811  118651.564210  ...   \n",
       "894  0.002395     0.322800    0.089179    2152.750986  730050.006374  ...   \n",
       "895  0.002811     0.376557    0.105393    1836.995316  545339.380176  ...   \n",
       "896  0.001647     0.370873    0.102202     853.677506  131473.967491  ...   \n",
       "897  0.005659     0.298197    0.090131    1561.606293  386055.207888  ...   \n",
       "\n",
       "     mfcc_var_16  mfcc_mean_17  mfcc_var_17  mfcc_mean_18  mfcc_var_18  \\\n",
       "0      62.597446     -5.950494    48.630459      1.334569    50.004402   \n",
       "1      94.285316      2.379663    45.529087     -0.897912    63.964500   \n",
       "2      41.972660      0.604512    55.172565      5.850554    55.045155   \n",
       "3      60.148708     -3.539526    72.778801     -4.562426    86.510620   \n",
       "4      50.798794    -11.507196    90.738411      0.821440    52.887070   \n",
       "..           ...           ...          ...           ...          ...   \n",
       "893   144.309845     -0.304753   160.793076      2.532327   152.150955   \n",
       "894    58.887882    -10.346969   113.920319      1.367891    55.805199   \n",
       "895    44.633171      2.953913    43.044193      4.850079    42.006420   \n",
       "896    57.660786      1.490195    56.502377     -0.173234    52.937866   \n",
       "897   111.426071      9.588392    53.757309     -1.011325    79.747833   \n",
       "\n",
       "     mfcc_mean_19  mfcc_var_19  mfcc_mean_20  mfcc_var_20   mood  \n",
       "0       -5.540088    64.204506      1.117945    66.048088  happy  \n",
       "1        2.081507    75.064636     -6.191793    77.692085  happy  \n",
       "2       -1.423651    46.238407      3.996233    56.848827  happy  \n",
       "3       -4.923912    70.068069     -8.135331   110.109657  happy  \n",
       "4       -6.566545    59.563255      1.389795    53.002163  happy  \n",
       "..            ...          ...           ...          ...    ...  \n",
       "893      2.110771   163.557083     -2.066999   142.535400    sad  \n",
       "894     -3.380867    84.275322     -2.286275    78.514404    sad  \n",
       "895     -2.364613    38.483833      2.754930    31.010529    sad  \n",
       "896     -0.707987    52.222660     -2.389481    50.750061    sad  \n",
       "897     10.141459    65.912148     -2.369084    65.630173    sad  \n",
       "\n",
       "[898 rows x 59 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Read dataset1\n",
    "df1 = pd.read_excel(\"Features1D/Features/Features.xlsx\")\n",
    "df1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8c88b98b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 898 entries, 0 to 897\n",
      "Data columns (total 59 columns):\n",
      " #   Column         Non-Null Count  Dtype  \n",
      "---  ------         --------------  -----  \n",
      " 0   file           898 non-null    object \n",
      " 1   scale          898 non-null    object \n",
      " 2   key            898 non-null    object \n",
      " 3   tempo          898 non-null    float64\n",
      " 4   rms_mean       898 non-null    float64\n",
      " 5   rms_var        898 non-null    float64\n",
      " 6   chroma_mean    898 non-null    float64\n",
      " 7   chroma_var     898 non-null    float64\n",
      " 8   centroid_mean  898 non-null    float64\n",
      " 9   centroid_var   898 non-null    float64\n",
      " 10  rolloff_mean   898 non-null    float64\n",
      " 11  roll_off_var   898 non-null    float64\n",
      " 12  zcr_mean       898 non-null    float64\n",
      " 13  zcr_var        898 non-null    float64\n",
      " 14  tonnetz_mean   898 non-null    float64\n",
      " 15  tonnetz_var    898 non-null    float64\n",
      " 16  mel_mean       898 non-null    float64\n",
      " 17  mel_var        898 non-null    float64\n",
      " 18  mfcc_mean_1    898 non-null    float64\n",
      " 19  mfcc_var_1     898 non-null    float64\n",
      " 20  mfcc_mean_2    898 non-null    float64\n",
      " 21  mfcc_var_2     898 non-null    float64\n",
      " 22  mfcc_mean_3    898 non-null    float64\n",
      " 23  mfcc_var_3     898 non-null    float64\n",
      " 24  mfcc_mean_4    898 non-null    float64\n",
      " 25  mfcc_var_4     898 non-null    float64\n",
      " 26  mfcc_mean_5    898 non-null    float64\n",
      " 27  mfcc_var_5     898 non-null    float64\n",
      " 28  mfcc_mean_6    898 non-null    float64\n",
      " 29  mfcc_var_6     898 non-null    float64\n",
      " 30  mfcc_mean_7    898 non-null    float64\n",
      " 31  mfcc_var_7     898 non-null    float64\n",
      " 32  mfcc_mean_8    898 non-null    float64\n",
      " 33  mfcc_var_8     898 non-null    float64\n",
      " 34  mfcc_mean_9    898 non-null    float64\n",
      " 35  mfcc_var_9     898 non-null    float64\n",
      " 36  mfcc_mean_10   898 non-null    float64\n",
      " 37  mfcc_var_10    898 non-null    float64\n",
      " 38  mfcc_mean_11   898 non-null    float64\n",
      " 39  mfcc_var_11    898 non-null    float64\n",
      " 40  mfcc_mean_12   898 non-null    float64\n",
      " 41  mfcc_var_12    898 non-null    float64\n",
      " 42  mfcc_mean_13   898 non-null    float64\n",
      " 43  mfcc_var_13    898 non-null    float64\n",
      " 44  mfcc_mean_14   898 non-null    float64\n",
      " 45  mfcc_var_14    898 non-null    float64\n",
      " 46  mfcc_mean_15   898 non-null    float64\n",
      " 47  mfcc_var_15    898 non-null    float64\n",
      " 48  mfcc_mean_16   898 non-null    float64\n",
      " 49  mfcc_var_16    898 non-null    float64\n",
      " 50  mfcc_mean_17   898 non-null    float64\n",
      " 51  mfcc_var_17    898 non-null    float64\n",
      " 52  mfcc_mean_18   898 non-null    float64\n",
      " 53  mfcc_var_18    898 non-null    float64\n",
      " 54  mfcc_mean_19   898 non-null    float64\n",
      " 55  mfcc_var_19    898 non-null    float64\n",
      " 56  mfcc_mean_20   898 non-null    float64\n",
      " 57  mfcc_var_20    898 non-null    float64\n",
      " 58  mood           898 non-null    object \n",
      "dtypes: float64(55), object(4)\n",
      "memory usage: 414.0+ KB\n"
     ]
    }
   ],
   "source": [
    "df1.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d8a41ec3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tempo</th>\n",
       "      <th>rms_mean</th>\n",
       "      <th>rms_var</th>\n",
       "      <th>chroma_mean</th>\n",
       "      <th>chroma_var</th>\n",
       "      <th>centroid_mean</th>\n",
       "      <th>centroid_var</th>\n",
       "      <th>rolloff_mean</th>\n",
       "      <th>roll_off_var</th>\n",
       "      <th>zcr_mean</th>\n",
       "      <th>...</th>\n",
       "      <th>mfcc_mean_16</th>\n",
       "      <th>mfcc_var_16</th>\n",
       "      <th>mfcc_mean_17</th>\n",
       "      <th>mfcc_var_17</th>\n",
       "      <th>mfcc_mean_18</th>\n",
       "      <th>mfcc_var_18</th>\n",
       "      <th>mfcc_mean_19</th>\n",
       "      <th>mfcc_var_19</th>\n",
       "      <th>mfcc_mean_20</th>\n",
       "      <th>mfcc_var_20</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>898.000000</td>\n",
       "      <td>898.000000</td>\n",
       "      <td>898.000000</td>\n",
       "      <td>898.000000</td>\n",
       "      <td>898.000000</td>\n",
       "      <td>898.000000</td>\n",
       "      <td>8.980000e+02</td>\n",
       "      <td>898.000000</td>\n",
       "      <td>8.980000e+02</td>\n",
       "      <td>898.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>898.000000</td>\n",
       "      <td>898.000000</td>\n",
       "      <td>898.000000</td>\n",
       "      <td>898.000000</td>\n",
       "      <td>898.000000</td>\n",
       "      <td>898.000000</td>\n",
       "      <td>898.000000</td>\n",
       "      <td>898.000000</td>\n",
       "      <td>898.000000</td>\n",
       "      <td>898.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>122.192119</td>\n",
       "      <td>0.140620</td>\n",
       "      <td>0.003567</td>\n",
       "      <td>0.376747</td>\n",
       "      <td>0.085305</td>\n",
       "      <td>2059.988511</td>\n",
       "      <td>3.909208e+05</td>\n",
       "      <td>4187.814830</td>\n",
       "      <td>1.675962e+06</td>\n",
       "      <td>0.100632</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.790507</td>\n",
       "      <td>64.153565</td>\n",
       "      <td>-2.054695</td>\n",
       "      <td>65.153429</td>\n",
       "      <td>-2.161143</td>\n",
       "      <td>66.155017</td>\n",
       "      <td>-1.077997</td>\n",
       "      <td>68.366924</td>\n",
       "      <td>-2.015693</td>\n",
       "      <td>72.219889</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>20.708462</td>\n",
       "      <td>0.067875</td>\n",
       "      <td>0.003130</td>\n",
       "      <td>0.093622</td>\n",
       "      <td>0.010212</td>\n",
       "      <td>685.458229</td>\n",
       "      <td>3.273618e+05</td>\n",
       "      <td>1522.875967</td>\n",
       "      <td>1.314115e+06</td>\n",
       "      <td>0.042594</td>\n",
       "      <td>...</td>\n",
       "      <td>5.714870</td>\n",
       "      <td>33.982765</td>\n",
       "      <td>5.854216</td>\n",
       "      <td>37.509313</td>\n",
       "      <td>5.396882</td>\n",
       "      <td>39.399692</td>\n",
       "      <td>5.390178</td>\n",
       "      <td>42.152986</td>\n",
       "      <td>4.876502</td>\n",
       "      <td>47.802374</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>75.999540</td>\n",
       "      <td>0.015824</td>\n",
       "      <td>0.000055</td>\n",
       "      <td>0.131688</td>\n",
       "      <td>0.041983</td>\n",
       "      <td>387.401161</td>\n",
       "      <td>4.007574e+03</td>\n",
       "      <td>523.105966</td>\n",
       "      <td>1.089884e+04</td>\n",
       "      <td>0.019527</td>\n",
       "      <td>...</td>\n",
       "      <td>-23.835148</td>\n",
       "      <td>16.496901</td>\n",
       "      <td>-16.260069</td>\n",
       "      <td>16.212341</td>\n",
       "      <td>-20.297907</td>\n",
       "      <td>15.476705</td>\n",
       "      <td>-17.380840</td>\n",
       "      <td>17.147591</td>\n",
       "      <td>-17.055731</td>\n",
       "      <td>14.376126</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>107.666016</td>\n",
       "      <td>0.090460</td>\n",
       "      <td>0.001534</td>\n",
       "      <td>0.309014</td>\n",
       "      <td>0.081920</td>\n",
       "      <td>1523.621647</td>\n",
       "      <td>1.572228e+05</td>\n",
       "      <td>3064.712155</td>\n",
       "      <td>6.972916e+05</td>\n",
       "      <td>0.066929</td>\n",
       "      <td>...</td>\n",
       "      <td>-5.492516</td>\n",
       "      <td>42.567407</td>\n",
       "      <td>-6.142178</td>\n",
       "      <td>42.141800</td>\n",
       "      <td>-5.623221</td>\n",
       "      <td>41.091060</td>\n",
       "      <td>-4.471170</td>\n",
       "      <td>43.613956</td>\n",
       "      <td>-4.983186</td>\n",
       "      <td>42.542788</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>123.046875</td>\n",
       "      <td>0.128408</td>\n",
       "      <td>0.002628</td>\n",
       "      <td>0.362656</td>\n",
       "      <td>0.087398</td>\n",
       "      <td>2078.002781</td>\n",
       "      <td>2.988196e+05</td>\n",
       "      <td>4289.452270</td>\n",
       "      <td>1.360052e+06</td>\n",
       "      <td>0.095536</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.872392</td>\n",
       "      <td>57.365828</td>\n",
       "      <td>-3.344803</td>\n",
       "      <td>55.556923</td>\n",
       "      <td>-1.133976</td>\n",
       "      <td>57.262060</td>\n",
       "      <td>-2.023335</td>\n",
       "      <td>58.444538</td>\n",
       "      <td>-1.465007</td>\n",
       "      <td>59.946037</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>135.999178</td>\n",
       "      <td>0.184625</td>\n",
       "      <td>0.004582</td>\n",
       "      <td>0.437746</td>\n",
       "      <td>0.091539</td>\n",
       "      <td>2546.979116</td>\n",
       "      <td>5.314662e+05</td>\n",
       "      <td>5360.334807</td>\n",
       "      <td>2.263938e+06</td>\n",
       "      <td>0.128580</td>\n",
       "      <td>...</td>\n",
       "      <td>2.426354</td>\n",
       "      <td>77.580990</td>\n",
       "      <td>1.334129</td>\n",
       "      <td>77.358486</td>\n",
       "      <td>1.951706</td>\n",
       "      <td>78.075478</td>\n",
       "      <td>1.448640</td>\n",
       "      <td>83.002417</td>\n",
       "      <td>1.369652</td>\n",
       "      <td>86.225466</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>198.768029</td>\n",
       "      <td>0.395023</td>\n",
       "      <td>0.023626</td>\n",
       "      <td>0.656680</td>\n",
       "      <td>0.111998</td>\n",
       "      <td>3703.229757</td>\n",
       "      <td>2.775075e+06</td>\n",
       "      <td>7815.641639</td>\n",
       "      <td>8.834975e+06</td>\n",
       "      <td>0.254733</td>\n",
       "      <td>...</td>\n",
       "      <td>16.275961</td>\n",
       "      <td>435.665466</td>\n",
       "      <td>15.933263</td>\n",
       "      <td>514.815186</td>\n",
       "      <td>12.374401</td>\n",
       "      <td>444.994934</td>\n",
       "      <td>16.365273</td>\n",
       "      <td>588.261475</td>\n",
       "      <td>15.270178</td>\n",
       "      <td>551.592407</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>8 rows × 55 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            tempo    rms_mean     rms_var  chroma_mean  chroma_var  \\\n",
       "count  898.000000  898.000000  898.000000   898.000000  898.000000   \n",
       "mean   122.192119    0.140620    0.003567     0.376747    0.085305   \n",
       "std     20.708462    0.067875    0.003130     0.093622    0.010212   \n",
       "min     75.999540    0.015824    0.000055     0.131688    0.041983   \n",
       "25%    107.666016    0.090460    0.001534     0.309014    0.081920   \n",
       "50%    123.046875    0.128408    0.002628     0.362656    0.087398   \n",
       "75%    135.999178    0.184625    0.004582     0.437746    0.091539   \n",
       "max    198.768029    0.395023    0.023626     0.656680    0.111998   \n",
       "\n",
       "       centroid_mean  centroid_var  rolloff_mean  roll_off_var    zcr_mean  \\\n",
       "count     898.000000  8.980000e+02    898.000000  8.980000e+02  898.000000   \n",
       "mean     2059.988511  3.909208e+05   4187.814830  1.675962e+06    0.100632   \n",
       "std       685.458229  3.273618e+05   1522.875967  1.314115e+06    0.042594   \n",
       "min       387.401161  4.007574e+03    523.105966  1.089884e+04    0.019527   \n",
       "25%      1523.621647  1.572228e+05   3064.712155  6.972916e+05    0.066929   \n",
       "50%      2078.002781  2.988196e+05   4289.452270  1.360052e+06    0.095536   \n",
       "75%      2546.979116  5.314662e+05   5360.334807  2.263938e+06    0.128580   \n",
       "max      3703.229757  2.775075e+06   7815.641639  8.834975e+06    0.254733   \n",
       "\n",
       "       ...  mfcc_mean_16  mfcc_var_16  mfcc_mean_17  mfcc_var_17  \\\n",
       "count  ...    898.000000   898.000000    898.000000   898.000000   \n",
       "mean   ...     -1.790507    64.153565     -2.054695    65.153429   \n",
       "std    ...      5.714870    33.982765      5.854216    37.509313   \n",
       "min    ...    -23.835148    16.496901    -16.260069    16.212341   \n",
       "25%    ...     -5.492516    42.567407     -6.142178    42.141800   \n",
       "50%    ...     -0.872392    57.365828     -3.344803    55.556923   \n",
       "75%    ...      2.426354    77.580990      1.334129    77.358486   \n",
       "max    ...     16.275961   435.665466     15.933263   514.815186   \n",
       "\n",
       "       mfcc_mean_18  mfcc_var_18  mfcc_mean_19  mfcc_var_19  mfcc_mean_20  \\\n",
       "count    898.000000   898.000000    898.000000   898.000000    898.000000   \n",
       "mean      -2.161143    66.155017     -1.077997    68.366924     -2.015693   \n",
       "std        5.396882    39.399692      5.390178    42.152986      4.876502   \n",
       "min      -20.297907    15.476705    -17.380840    17.147591    -17.055731   \n",
       "25%       -5.623221    41.091060     -4.471170    43.613956     -4.983186   \n",
       "50%       -1.133976    57.262060     -2.023335    58.444538     -1.465007   \n",
       "75%        1.951706    78.075478      1.448640    83.002417      1.369652   \n",
       "max       12.374401   444.994934     16.365273   588.261475     15.270178   \n",
       "\n",
       "       mfcc_var_20  \n",
       "count   898.000000  \n",
       "mean     72.219889  \n",
       "std      47.802374  \n",
       "min      14.376126  \n",
       "25%      42.542788  \n",
       "50%      59.946037  \n",
       "75%      86.225466  \n",
       "max     551.592407  \n",
       "\n",
       "[8 rows x 55 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df1.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "30fce297",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tempo</th>\n",
       "      <th>rms_mean</th>\n",
       "      <th>rms_var</th>\n",
       "      <th>chroma_mean</th>\n",
       "      <th>chroma_var</th>\n",
       "      <th>centroid_mean</th>\n",
       "      <th>centroid_var</th>\n",
       "      <th>rolloff_mean</th>\n",
       "      <th>roll_off_var</th>\n",
       "      <th>zcr_mean</th>\n",
       "      <th>...</th>\n",
       "      <th>mfcc_mean_16</th>\n",
       "      <th>mfcc_var_16</th>\n",
       "      <th>mfcc_mean_17</th>\n",
       "      <th>mfcc_var_17</th>\n",
       "      <th>mfcc_mean_18</th>\n",
       "      <th>mfcc_var_18</th>\n",
       "      <th>mfcc_mean_19</th>\n",
       "      <th>mfcc_var_19</th>\n",
       "      <th>mfcc_mean_20</th>\n",
       "      <th>mfcc_var_20</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mood</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>angry</th>\n",
       "      <td>120.608432</td>\n",
       "      <td>0.195935</td>\n",
       "      <td>0.004543</td>\n",
       "      <td>0.485090</td>\n",
       "      <td>0.075074</td>\n",
       "      <td>2584.532564</td>\n",
       "      <td>344406.472181</td>\n",
       "      <td>5237.362224</td>\n",
       "      <td>1.264628e+06</td>\n",
       "      <td>0.136600</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.930292</td>\n",
       "      <td>44.363887</td>\n",
       "      <td>-1.284212</td>\n",
       "      <td>44.808423</td>\n",
       "      <td>-1.977401</td>\n",
       "      <td>46.113114</td>\n",
       "      <td>0.421466</td>\n",
       "      <td>48.109930</td>\n",
       "      <td>-1.777066</td>\n",
       "      <td>48.157392</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>happy</th>\n",
       "      <td>123.450826</td>\n",
       "      <td>0.149859</td>\n",
       "      <td>0.003879</td>\n",
       "      <td>0.381774</td>\n",
       "      <td>0.086511</td>\n",
       "      <td>2316.987041</td>\n",
       "      <td>412191.587768</td>\n",
       "      <td>4803.631991</td>\n",
       "      <td>1.607647e+06</td>\n",
       "      <td>0.114884</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.783055</td>\n",
       "      <td>60.065572</td>\n",
       "      <td>-1.757794</td>\n",
       "      <td>60.666303</td>\n",
       "      <td>-1.741446</td>\n",
       "      <td>62.121580</td>\n",
       "      <td>-1.139971</td>\n",
       "      <td>67.214375</td>\n",
       "      <td>-1.887622</td>\n",
       "      <td>69.893941</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>relaxed</th>\n",
       "      <td>122.453003</td>\n",
       "      <td>0.107175</td>\n",
       "      <td>0.002817</td>\n",
       "      <td>0.318259</td>\n",
       "      <td>0.089239</td>\n",
       "      <td>1663.728268</td>\n",
       "      <td>419919.822595</td>\n",
       "      <td>3368.101718</td>\n",
       "      <td>2.066409e+06</td>\n",
       "      <td>0.074023</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.632271</td>\n",
       "      <td>80.336390</td>\n",
       "      <td>-2.060138</td>\n",
       "      <td>82.309746</td>\n",
       "      <td>-1.813682</td>\n",
       "      <td>86.293389</td>\n",
       "      <td>-1.424376</td>\n",
       "      <td>85.156412</td>\n",
       "      <td>-1.918423</td>\n",
       "      <td>91.165267</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>sad</th>\n",
       "      <td>122.267405</td>\n",
       "      <td>0.109595</td>\n",
       "      <td>0.003033</td>\n",
       "      <td>0.321909</td>\n",
       "      <td>0.090405</td>\n",
       "      <td>1676.990600</td>\n",
       "      <td>387354.481163</td>\n",
       "      <td>3347.637316</td>\n",
       "      <td>1.764555e+06</td>\n",
       "      <td>0.077147</td>\n",
       "      <td>...</td>\n",
       "      <td>-2.816344</td>\n",
       "      <td>71.812074</td>\n",
       "      <td>-3.113996</td>\n",
       "      <td>72.789358</td>\n",
       "      <td>-3.108311</td>\n",
       "      <td>70.056133</td>\n",
       "      <td>-2.169657</td>\n",
       "      <td>72.976734</td>\n",
       "      <td>-2.478521</td>\n",
       "      <td>79.642282</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>4 rows × 55 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "              tempo  rms_mean   rms_var  chroma_mean  chroma_var  \\\n",
       "mood                                                               \n",
       "angry    120.608432  0.195935  0.004543     0.485090    0.075074   \n",
       "happy    123.450826  0.149859  0.003879     0.381774    0.086511   \n",
       "relaxed  122.453003  0.107175  0.002817     0.318259    0.089239   \n",
       "sad      122.267405  0.109595  0.003033     0.321909    0.090405   \n",
       "\n",
       "         centroid_mean   centroid_var  rolloff_mean  roll_off_var  zcr_mean  \\\n",
       "mood                                                                          \n",
       "angry      2584.532564  344406.472181   5237.362224  1.264628e+06  0.136600   \n",
       "happy      2316.987041  412191.587768   4803.631991  1.607647e+06  0.114884   \n",
       "relaxed    1663.728268  419919.822595   3368.101718  2.066409e+06  0.074023   \n",
       "sad        1676.990600  387354.481163   3347.637316  1.764555e+06  0.077147   \n",
       "\n",
       "         ...  mfcc_mean_16  mfcc_var_16  mfcc_mean_17  mfcc_var_17  \\\n",
       "mood     ...                                                         \n",
       "angry    ...     -0.930292    44.363887     -1.284212    44.808423   \n",
       "happy    ...     -1.783055    60.065572     -1.757794    60.666303   \n",
       "relaxed  ...     -1.632271    80.336390     -2.060138    82.309746   \n",
       "sad      ...     -2.816344    71.812074     -3.113996    72.789358   \n",
       "\n",
       "         mfcc_mean_18  mfcc_var_18  mfcc_mean_19  mfcc_var_19  mfcc_mean_20  \\\n",
       "mood                                                                          \n",
       "angry       -1.977401    46.113114      0.421466    48.109930     -1.777066   \n",
       "happy       -1.741446    62.121580     -1.139971    67.214375     -1.887622   \n",
       "relaxed     -1.813682    86.293389     -1.424376    85.156412     -1.918423   \n",
       "sad         -3.108311    70.056133     -2.169657    72.976734     -2.478521   \n",
       "\n",
       "         mfcc_var_20  \n",
       "mood                  \n",
       "angry      48.157392  \n",
       "happy      69.893941  \n",
       "relaxed    91.165267  \n",
       "sad        79.642282  \n",
       "\n",
       "[4 rows x 55 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df1.groupby('mood').mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e803fa86",
   "metadata": {},
   "source": [
    "### Dataset 2\n",
    "**About dataset**\n",
    "- 400 ~30 second audio clips gathered from erbal and non-verbal music from different genres of Turkish music\n",
    "- Annotated into 4 quadrants based on valance and arousal according to Russell's model.\n",
    "- Audios are organized in 4 categories - Happy, Sad, Angry, Relax\n",
    "- Equally stratified dataset with each classes 100 songs\n",
    "\n",
    "**Acknowledgements**<br>\n",
    "source:https://www.kaggle.com/datasets/blaler/turkish-music-emotion-dataset\n",
    "\n",
    "**If you use it, please cite the following article(s):**<br>\n",
    "Bilal Er, M., & Aydilek, I. B. (2019). Music emotion recognition by using chroma spectrogram and deep visual features. Journal of Computational Intelligent Systems, 12(2), 1622–1634. International Journal of Computational Intelligence Systems, DOI: [Web Link] https://doi.org/10.2991/ijcis.d.191216.001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "926d715e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>file</th>\n",
       "      <th>scale</th>\n",
       "      <th>key</th>\n",
       "      <th>tempo</th>\n",
       "      <th>rms_mean</th>\n",
       "      <th>rms_var</th>\n",
       "      <th>chroma_mean</th>\n",
       "      <th>chroma_var</th>\n",
       "      <th>centroid_mean</th>\n",
       "      <th>centroid_var</th>\n",
       "      <th>...</th>\n",
       "      <th>mfcc_var_16</th>\n",
       "      <th>mfcc_mean_17</th>\n",
       "      <th>mfcc_var_17</th>\n",
       "      <th>mfcc_mean_18</th>\n",
       "      <th>mfcc_var_18</th>\n",
       "      <th>mfcc_mean_19</th>\n",
       "      <th>mfcc_var_19</th>\n",
       "      <th>mfcc_mean_20</th>\n",
       "      <th>mfcc_var_20</th>\n",
       "      <th>mood</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Dataset25/MER_Extra25/happy/ovalar_gurup_yol.mp3</td>\n",
       "      <td>minor</td>\n",
       "      <td>C#</td>\n",
       "      <td>107.666016</td>\n",
       "      <td>0.203715</td>\n",
       "      <td>0.004113</td>\n",
       "      <td>0.355963</td>\n",
       "      <td>0.087506</td>\n",
       "      <td>2257.202789</td>\n",
       "      <td>3.633545e+05</td>\n",
       "      <td>...</td>\n",
       "      <td>39.745495</td>\n",
       "      <td>-5.036633</td>\n",
       "      <td>51.429291</td>\n",
       "      <td>-3.394783</td>\n",
       "      <td>50.277916</td>\n",
       "      <td>-5.994911</td>\n",
       "      <td>51.078880</td>\n",
       "      <td>-2.758130</td>\n",
       "      <td>67.629799</td>\n",
       "      <td>happy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Dataset25/MER_Extra25/happy/cekirge_oguz_yilma...</td>\n",
       "      <td>major</td>\n",
       "      <td>D</td>\n",
       "      <td>99.384014</td>\n",
       "      <td>0.134899</td>\n",
       "      <td>0.006532</td>\n",
       "      <td>0.337127</td>\n",
       "      <td>0.094463</td>\n",
       "      <td>1963.637200</td>\n",
       "      <td>2.299410e+05</td>\n",
       "      <td>...</td>\n",
       "      <td>53.625290</td>\n",
       "      <td>-10.358211</td>\n",
       "      <td>58.315762</td>\n",
       "      <td>-0.972885</td>\n",
       "      <td>80.311607</td>\n",
       "      <td>-4.383526</td>\n",
       "      <td>114.317406</td>\n",
       "      <td>6.196268</td>\n",
       "      <td>106.006302</td>\n",
       "      <td>happy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Dataset25/MER_Extra25/happy/baran_bayraktar_go...</td>\n",
       "      <td>major</td>\n",
       "      <td>G</td>\n",
       "      <td>117.453835</td>\n",
       "      <td>0.202237</td>\n",
       "      <td>0.013383</td>\n",
       "      <td>0.377444</td>\n",
       "      <td>0.090938</td>\n",
       "      <td>2558.051976</td>\n",
       "      <td>1.368935e+06</td>\n",
       "      <td>...</td>\n",
       "      <td>51.113770</td>\n",
       "      <td>-2.307574</td>\n",
       "      <td>66.429535</td>\n",
       "      <td>0.962885</td>\n",
       "      <td>38.927681</td>\n",
       "      <td>-3.003113</td>\n",
       "      <td>57.289982</td>\n",
       "      <td>1.009082</td>\n",
       "      <td>73.439346</td>\n",
       "      <td>happy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Dataset25/MER_Extra25/happy/bu_gece_uyumamisan...</td>\n",
       "      <td>major</td>\n",
       "      <td>F</td>\n",
       "      <td>129.199219</td>\n",
       "      <td>0.077799</td>\n",
       "      <td>0.000701</td>\n",
       "      <td>0.482537</td>\n",
       "      <td>0.070852</td>\n",
       "      <td>3172.962069</td>\n",
       "      <td>2.144511e+05</td>\n",
       "      <td>...</td>\n",
       "      <td>33.953434</td>\n",
       "      <td>-12.987661</td>\n",
       "      <td>43.694424</td>\n",
       "      <td>3.631064</td>\n",
       "      <td>29.647392</td>\n",
       "      <td>-10.251694</td>\n",
       "      <td>40.205898</td>\n",
       "      <td>-4.413622</td>\n",
       "      <td>40.858986</td>\n",
       "      <td>happy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Dataset25/MER_Extra25/happy/kim_arar_nilufer.mp3</td>\n",
       "      <td>major</td>\n",
       "      <td>F</td>\n",
       "      <td>129.199219</td>\n",
       "      <td>0.134208</td>\n",
       "      <td>0.000974</td>\n",
       "      <td>0.376593</td>\n",
       "      <td>0.075194</td>\n",
       "      <td>2966.197823</td>\n",
       "      <td>2.636523e+05</td>\n",
       "      <td>...</td>\n",
       "      <td>43.511322</td>\n",
       "      <td>-2.495036</td>\n",
       "      <td>33.745182</td>\n",
       "      <td>0.276652</td>\n",
       "      <td>27.181576</td>\n",
       "      <td>-0.029217</td>\n",
       "      <td>34.924503</td>\n",
       "      <td>3.552903</td>\n",
       "      <td>46.541862</td>\n",
       "      <td>happy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>395</th>\n",
       "      <td>Dataset25/MER_Extra25/relaxed/sumru_agir_yuruy...</td>\n",
       "      <td>major</td>\n",
       "      <td>D</td>\n",
       "      <td>112.347147</td>\n",
       "      <td>0.142323</td>\n",
       "      <td>0.004840</td>\n",
       "      <td>0.339998</td>\n",
       "      <td>0.095134</td>\n",
       "      <td>1123.696387</td>\n",
       "      <td>5.788810e+05</td>\n",
       "      <td>...</td>\n",
       "      <td>38.897163</td>\n",
       "      <td>8.553452</td>\n",
       "      <td>82.796318</td>\n",
       "      <td>8.461837</td>\n",
       "      <td>67.124542</td>\n",
       "      <td>1.787519</td>\n",
       "      <td>73.779991</td>\n",
       "      <td>-0.676014</td>\n",
       "      <td>55.244400</td>\n",
       "      <td>relaxed</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>396</th>\n",
       "      <td>Dataset25/MER_Extra25/relaxed/onur_mete_bu_ask...</td>\n",
       "      <td>major</td>\n",
       "      <td>B</td>\n",
       "      <td>135.999178</td>\n",
       "      <td>0.152394</td>\n",
       "      <td>0.003350</td>\n",
       "      <td>0.326165</td>\n",
       "      <td>0.084352</td>\n",
       "      <td>2107.054510</td>\n",
       "      <td>6.873373e+05</td>\n",
       "      <td>...</td>\n",
       "      <td>42.552208</td>\n",
       "      <td>-2.485002</td>\n",
       "      <td>40.545063</td>\n",
       "      <td>3.581853</td>\n",
       "      <td>51.561790</td>\n",
       "      <td>-2.310211</td>\n",
       "      <td>46.843857</td>\n",
       "      <td>-4.614769</td>\n",
       "      <td>53.502258</td>\n",
       "      <td>relaxed</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>397</th>\n",
       "      <td>Dataset25/MER_Extra25/relaxed/demir_demircan_a...</td>\n",
       "      <td>major</td>\n",
       "      <td>D</td>\n",
       "      <td>129.199219</td>\n",
       "      <td>0.161983</td>\n",
       "      <td>0.002465</td>\n",
       "      <td>0.300543</td>\n",
       "      <td>0.089547</td>\n",
       "      <td>1408.713888</td>\n",
       "      <td>5.549858e+05</td>\n",
       "      <td>...</td>\n",
       "      <td>61.977123</td>\n",
       "      <td>-6.842248</td>\n",
       "      <td>49.472839</td>\n",
       "      <td>-4.599386</td>\n",
       "      <td>79.167732</td>\n",
       "      <td>-6.601953</td>\n",
       "      <td>53.350906</td>\n",
       "      <td>-1.959686</td>\n",
       "      <td>81.524139</td>\n",
       "      <td>relaxed</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>398</th>\n",
       "      <td>Dataset25/MER_Extra25/relaxed/yalin_ki_sen.mp3</td>\n",
       "      <td>major</td>\n",
       "      <td>A</td>\n",
       "      <td>86.132812</td>\n",
       "      <td>0.182141</td>\n",
       "      <td>0.003011</td>\n",
       "      <td>0.302875</td>\n",
       "      <td>0.094239</td>\n",
       "      <td>1568.452968</td>\n",
       "      <td>1.262180e+06</td>\n",
       "      <td>...</td>\n",
       "      <td>68.513573</td>\n",
       "      <td>-2.284982</td>\n",
       "      <td>64.041695</td>\n",
       "      <td>-6.485914</td>\n",
       "      <td>52.975601</td>\n",
       "      <td>-11.130312</td>\n",
       "      <td>54.331902</td>\n",
       "      <td>-3.631683</td>\n",
       "      <td>62.101868</td>\n",
       "      <td>relaxed</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>399</th>\n",
       "      <td>Dataset25/MER_Extra25/relaxed/sarp_madem_ortad...</td>\n",
       "      <td>minor</td>\n",
       "      <td>G#</td>\n",
       "      <td>99.384014</td>\n",
       "      <td>0.078060</td>\n",
       "      <td>0.000220</td>\n",
       "      <td>0.344734</td>\n",
       "      <td>0.084124</td>\n",
       "      <td>375.472632</td>\n",
       "      <td>1.150694e+04</td>\n",
       "      <td>...</td>\n",
       "      <td>61.291878</td>\n",
       "      <td>-9.897033</td>\n",
       "      <td>66.397682</td>\n",
       "      <td>-6.967515</td>\n",
       "      <td>64.210945</td>\n",
       "      <td>-5.476778</td>\n",
       "      <td>66.888565</td>\n",
       "      <td>-8.320798</td>\n",
       "      <td>102.913956</td>\n",
       "      <td>relaxed</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>400 rows × 59 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                  file  scale key       tempo  \\\n",
       "0     Dataset25/MER_Extra25/happy/ovalar_gurup_yol.mp3  minor  C#  107.666016   \n",
       "1    Dataset25/MER_Extra25/happy/cekirge_oguz_yilma...  major   D   99.384014   \n",
       "2    Dataset25/MER_Extra25/happy/baran_bayraktar_go...  major   G  117.453835   \n",
       "3    Dataset25/MER_Extra25/happy/bu_gece_uyumamisan...  major   F  129.199219   \n",
       "4     Dataset25/MER_Extra25/happy/kim_arar_nilufer.mp3  major   F  129.199219   \n",
       "..                                                 ...    ...  ..         ...   \n",
       "395  Dataset25/MER_Extra25/relaxed/sumru_agir_yuruy...  major   D  112.347147   \n",
       "396  Dataset25/MER_Extra25/relaxed/onur_mete_bu_ask...  major   B  135.999178   \n",
       "397  Dataset25/MER_Extra25/relaxed/demir_demircan_a...  major   D  129.199219   \n",
       "398     Dataset25/MER_Extra25/relaxed/yalin_ki_sen.mp3  major   A   86.132812   \n",
       "399  Dataset25/MER_Extra25/relaxed/sarp_madem_ortad...  minor  G#   99.384014   \n",
       "\n",
       "     rms_mean   rms_var  chroma_mean  chroma_var  centroid_mean  centroid_var  \\\n",
       "0    0.203715  0.004113     0.355963    0.087506    2257.202789  3.633545e+05   \n",
       "1    0.134899  0.006532     0.337127    0.094463    1963.637200  2.299410e+05   \n",
       "2    0.202237  0.013383     0.377444    0.090938    2558.051976  1.368935e+06   \n",
       "3    0.077799  0.000701     0.482537    0.070852    3172.962069  2.144511e+05   \n",
       "4    0.134208  0.000974     0.376593    0.075194    2966.197823  2.636523e+05   \n",
       "..        ...       ...          ...         ...            ...           ...   \n",
       "395  0.142323  0.004840     0.339998    0.095134    1123.696387  5.788810e+05   \n",
       "396  0.152394  0.003350     0.326165    0.084352    2107.054510  6.873373e+05   \n",
       "397  0.161983  0.002465     0.300543    0.089547    1408.713888  5.549858e+05   \n",
       "398  0.182141  0.003011     0.302875    0.094239    1568.452968  1.262180e+06   \n",
       "399  0.078060  0.000220     0.344734    0.084124     375.472632  1.150694e+04   \n",
       "\n",
       "     ...  mfcc_var_16  mfcc_mean_17  mfcc_var_17  mfcc_mean_18  mfcc_var_18  \\\n",
       "0    ...    39.745495     -5.036633    51.429291     -3.394783    50.277916   \n",
       "1    ...    53.625290    -10.358211    58.315762     -0.972885    80.311607   \n",
       "2    ...    51.113770     -2.307574    66.429535      0.962885    38.927681   \n",
       "3    ...    33.953434    -12.987661    43.694424      3.631064    29.647392   \n",
       "4    ...    43.511322     -2.495036    33.745182      0.276652    27.181576   \n",
       "..   ...          ...           ...          ...           ...          ...   \n",
       "395  ...    38.897163      8.553452    82.796318      8.461837    67.124542   \n",
       "396  ...    42.552208     -2.485002    40.545063      3.581853    51.561790   \n",
       "397  ...    61.977123     -6.842248    49.472839     -4.599386    79.167732   \n",
       "398  ...    68.513573     -2.284982    64.041695     -6.485914    52.975601   \n",
       "399  ...    61.291878     -9.897033    66.397682     -6.967515    64.210945   \n",
       "\n",
       "     mfcc_mean_19  mfcc_var_19  mfcc_mean_20  mfcc_var_20     mood  \n",
       "0       -5.994911    51.078880     -2.758130    67.629799    happy  \n",
       "1       -4.383526   114.317406      6.196268   106.006302    happy  \n",
       "2       -3.003113    57.289982      1.009082    73.439346    happy  \n",
       "3      -10.251694    40.205898     -4.413622    40.858986    happy  \n",
       "4       -0.029217    34.924503      3.552903    46.541862    happy  \n",
       "..            ...          ...           ...          ...      ...  \n",
       "395      1.787519    73.779991     -0.676014    55.244400  relaxed  \n",
       "396     -2.310211    46.843857     -4.614769    53.502258  relaxed  \n",
       "397     -6.601953    53.350906     -1.959686    81.524139  relaxed  \n",
       "398    -11.130312    54.331902     -3.631683    62.101868  relaxed  \n",
       "399     -5.476778    66.888565     -8.320798   102.913956  relaxed  \n",
       "\n",
       "[400 rows x 59 columns]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Read dataset2\n",
    "df2 = pd.read_excel(\"Features1D/Features/Features_extra.xlsx\")\n",
    "df2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a463b2c2",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 400 entries, 0 to 399\n",
      "Data columns (total 59 columns):\n",
      " #   Column         Non-Null Count  Dtype  \n",
      "---  ------         --------------  -----  \n",
      " 0   file           400 non-null    object \n",
      " 1   scale          400 non-null    object \n",
      " 2   key            400 non-null    object \n",
      " 3   tempo          400 non-null    float64\n",
      " 4   rms_mean       400 non-null    float64\n",
      " 5   rms_var        400 non-null    float64\n",
      " 6   chroma_mean    400 non-null    float64\n",
      " 7   chroma_var     400 non-null    float64\n",
      " 8   centroid_mean  400 non-null    float64\n",
      " 9   centroid_var   400 non-null    float64\n",
      " 10  rolloff_mean   400 non-null    float64\n",
      " 11  roll_off_var   400 non-null    float64\n",
      " 12  zcr_mean       400 non-null    float64\n",
      " 13  zcr_var        400 non-null    float64\n",
      " 14  tonnetz_mean   400 non-null    float64\n",
      " 15  tonnetz_var    400 non-null    float64\n",
      " 16  mel_mean       400 non-null    float64\n",
      " 17  mel_var        400 non-null    float64\n",
      " 18  mfcc_mean_1    400 non-null    float64\n",
      " 19  mfcc_var_1     400 non-null    float64\n",
      " 20  mfcc_mean_2    400 non-null    float64\n",
      " 21  mfcc_var_2     400 non-null    float64\n",
      " 22  mfcc_mean_3    400 non-null    float64\n",
      " 23  mfcc_var_3     400 non-null    float64\n",
      " 24  mfcc_mean_4    400 non-null    float64\n",
      " 25  mfcc_var_4     400 non-null    float64\n",
      " 26  mfcc_mean_5    400 non-null    float64\n",
      " 27  mfcc_var_5     400 non-null    float64\n",
      " 28  mfcc_mean_6    400 non-null    float64\n",
      " 29  mfcc_var_6     400 non-null    float64\n",
      " 30  mfcc_mean_7    400 non-null    float64\n",
      " 31  mfcc_var_7     400 non-null    float64\n",
      " 32  mfcc_mean_8    400 non-null    float64\n",
      " 33  mfcc_var_8     400 non-null    float64\n",
      " 34  mfcc_mean_9    400 non-null    float64\n",
      " 35  mfcc_var_9     400 non-null    float64\n",
      " 36  mfcc_mean_10   400 non-null    float64\n",
      " 37  mfcc_var_10    400 non-null    float64\n",
      " 38  mfcc_mean_11   400 non-null    float64\n",
      " 39  mfcc_var_11    400 non-null    float64\n",
      " 40  mfcc_mean_12   400 non-null    float64\n",
      " 41  mfcc_var_12    400 non-null    float64\n",
      " 42  mfcc_mean_13   400 non-null    float64\n",
      " 43  mfcc_var_13    400 non-null    float64\n",
      " 44  mfcc_mean_14   400 non-null    float64\n",
      " 45  mfcc_var_14    400 non-null    float64\n",
      " 46  mfcc_mean_15   400 non-null    float64\n",
      " 47  mfcc_var_15    400 non-null    float64\n",
      " 48  mfcc_mean_16   400 non-null    float64\n",
      " 49  mfcc_var_16    400 non-null    float64\n",
      " 50  mfcc_mean_17   400 non-null    float64\n",
      " 51  mfcc_var_17    400 non-null    float64\n",
      " 52  mfcc_mean_18   400 non-null    float64\n",
      " 53  mfcc_var_18    400 non-null    float64\n",
      " 54  mfcc_mean_19   400 non-null    float64\n",
      " 55  mfcc_var_19    400 non-null    float64\n",
      " 56  mfcc_mean_20   400 non-null    float64\n",
      " 57  mfcc_var_20    400 non-null    float64\n",
      " 58  mood           400 non-null    object \n",
      "dtypes: float64(55), object(4)\n",
      "memory usage: 184.5+ KB\n"
     ]
    }
   ],
   "source": [
    "df2.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9baff1a2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tempo</th>\n",
       "      <th>rms_mean</th>\n",
       "      <th>rms_var</th>\n",
       "      <th>chroma_mean</th>\n",
       "      <th>chroma_var</th>\n",
       "      <th>centroid_mean</th>\n",
       "      <th>centroid_var</th>\n",
       "      <th>rolloff_mean</th>\n",
       "      <th>roll_off_var</th>\n",
       "      <th>zcr_mean</th>\n",
       "      <th>...</th>\n",
       "      <th>mfcc_mean_16</th>\n",
       "      <th>mfcc_var_16</th>\n",
       "      <th>mfcc_mean_17</th>\n",
       "      <th>mfcc_var_17</th>\n",
       "      <th>mfcc_mean_18</th>\n",
       "      <th>mfcc_var_18</th>\n",
       "      <th>mfcc_mean_19</th>\n",
       "      <th>mfcc_var_19</th>\n",
       "      <th>mfcc_mean_20</th>\n",
       "      <th>mfcc_var_20</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>400.000000</td>\n",
       "      <td>400.000000</td>\n",
       "      <td>400.000000</td>\n",
       "      <td>400.000000</td>\n",
       "      <td>400.000000</td>\n",
       "      <td>400.000000</td>\n",
       "      <td>4.000000e+02</td>\n",
       "      <td>400.000000</td>\n",
       "      <td>4.000000e+02</td>\n",
       "      <td>400.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>400.000000</td>\n",
       "      <td>400.000000</td>\n",
       "      <td>400.000000</td>\n",
       "      <td>400.000000</td>\n",
       "      <td>400.000000</td>\n",
       "      <td>400.000000</td>\n",
       "      <td>400.000000</td>\n",
       "      <td>400.000000</td>\n",
       "      <td>400.000000</td>\n",
       "      <td>400.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>121.153331</td>\n",
       "      <td>0.118770</td>\n",
       "      <td>0.002119</td>\n",
       "      <td>0.331975</td>\n",
       "      <td>0.085352</td>\n",
       "      <td>1802.486037</td>\n",
       "      <td>3.180519e+05</td>\n",
       "      <td>3680.355592</td>\n",
       "      <td>1.672630e+06</td>\n",
       "      <td>0.080345</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.010624</td>\n",
       "      <td>59.615112</td>\n",
       "      <td>-4.744398</td>\n",
       "      <td>59.401602</td>\n",
       "      <td>-0.608768</td>\n",
       "      <td>62.063296</td>\n",
       "      <td>-4.442099</td>\n",
       "      <td>66.349466</td>\n",
       "      <td>-0.848953</td>\n",
       "      <td>73.378861</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>21.744658</td>\n",
       "      <td>0.059437</td>\n",
       "      <td>0.002130</td>\n",
       "      <td>0.087221</td>\n",
       "      <td>0.007522</td>\n",
       "      <td>651.203680</td>\n",
       "      <td>2.810984e+05</td>\n",
       "      <td>1501.206045</td>\n",
       "      <td>1.395001e+06</td>\n",
       "      <td>0.041138</td>\n",
       "      <td>...</td>\n",
       "      <td>4.559469</td>\n",
       "      <td>40.272152</td>\n",
       "      <td>4.768623</td>\n",
       "      <td>37.590001</td>\n",
       "      <td>4.756607</td>\n",
       "      <td>38.923017</td>\n",
       "      <td>4.446831</td>\n",
       "      <td>42.345726</td>\n",
       "      <td>4.790249</td>\n",
       "      <td>50.602605</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>33.999794</td>\n",
       "      <td>0.006620</td>\n",
       "      <td>0.000022</td>\n",
       "      <td>0.160259</td>\n",
       "      <td>0.050952</td>\n",
       "      <td>375.472632</td>\n",
       "      <td>1.150694e+04</td>\n",
       "      <td>535.694759</td>\n",
       "      <td>3.266810e+04</td>\n",
       "      <td>0.012583</td>\n",
       "      <td>...</td>\n",
       "      <td>-15.610699</td>\n",
       "      <td>14.551353</td>\n",
       "      <td>-24.893364</td>\n",
       "      <td>14.379240</td>\n",
       "      <td>-16.672720</td>\n",
       "      <td>13.698275</td>\n",
       "      <td>-19.804493</td>\n",
       "      <td>14.141619</td>\n",
       "      <td>-18.626137</td>\n",
       "      <td>13.516327</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>103.359375</td>\n",
       "      <td>0.074349</td>\n",
       "      <td>0.000723</td>\n",
       "      <td>0.268281</td>\n",
       "      <td>0.081917</td>\n",
       "      <td>1300.376902</td>\n",
       "      <td>1.370374e+05</td>\n",
       "      <td>2575.757575</td>\n",
       "      <td>7.025397e+05</td>\n",
       "      <td>0.048085</td>\n",
       "      <td>...</td>\n",
       "      <td>-3.842349</td>\n",
       "      <td>34.335782</td>\n",
       "      <td>-7.630008</td>\n",
       "      <td>34.245143</td>\n",
       "      <td>-3.382443</td>\n",
       "      <td>36.080644</td>\n",
       "      <td>-6.791193</td>\n",
       "      <td>38.651546</td>\n",
       "      <td>-3.680080</td>\n",
       "      <td>40.332462</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>117.453835</td>\n",
       "      <td>0.108431</td>\n",
       "      <td>0.001499</td>\n",
       "      <td>0.319929</td>\n",
       "      <td>0.086349</td>\n",
       "      <td>1772.615561</td>\n",
       "      <td>2.283629e+05</td>\n",
       "      <td>3595.035859</td>\n",
       "      <td>1.224653e+06</td>\n",
       "      <td>0.075321</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.508271</td>\n",
       "      <td>50.099451</td>\n",
       "      <td>-4.506119</td>\n",
       "      <td>51.089298</td>\n",
       "      <td>-0.276665</td>\n",
       "      <td>53.090158</td>\n",
       "      <td>-4.065011</td>\n",
       "      <td>56.254971</td>\n",
       "      <td>-0.693477</td>\n",
       "      <td>62.006828</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>135.999178</td>\n",
       "      <td>0.156254</td>\n",
       "      <td>0.002770</td>\n",
       "      <td>0.381731</td>\n",
       "      <td>0.090105</td>\n",
       "      <td>2248.942922</td>\n",
       "      <td>4.188546e+05</td>\n",
       "      <td>4785.132302</td>\n",
       "      <td>2.189253e+06</td>\n",
       "      <td>0.105276</td>\n",
       "      <td>...</td>\n",
       "      <td>1.939811</td>\n",
       "      <td>71.015478</td>\n",
       "      <td>-1.810183</td>\n",
       "      <td>70.942482</td>\n",
       "      <td>2.786151</td>\n",
       "      <td>75.734247</td>\n",
       "      <td>-1.440908</td>\n",
       "      <td>80.194420</td>\n",
       "      <td>1.858971</td>\n",
       "      <td>92.351814</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>198.768029</td>\n",
       "      <td>0.381500</td>\n",
       "      <td>0.017487</td>\n",
       "      <td>0.635405</td>\n",
       "      <td>0.104724</td>\n",
       "      <td>3498.680882</td>\n",
       "      <td>2.039381e+06</td>\n",
       "      <td>6959.760493</td>\n",
       "      <td>8.731942e+06</td>\n",
       "      <td>0.246214</td>\n",
       "      <td>...</td>\n",
       "      <td>13.872192</td>\n",
       "      <td>358.093811</td>\n",
       "      <td>12.957845</td>\n",
       "      <td>302.761414</td>\n",
       "      <td>13.124251</td>\n",
       "      <td>303.582184</td>\n",
       "      <td>6.750900</td>\n",
       "      <td>280.381073</td>\n",
       "      <td>13.298891</td>\n",
       "      <td>442.129852</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>8 rows × 55 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            tempo    rms_mean     rms_var  chroma_mean  chroma_var  \\\n",
       "count  400.000000  400.000000  400.000000   400.000000  400.000000   \n",
       "mean   121.153331    0.118770    0.002119     0.331975    0.085352   \n",
       "std     21.744658    0.059437    0.002130     0.087221    0.007522   \n",
       "min     33.999794    0.006620    0.000022     0.160259    0.050952   \n",
       "25%    103.359375    0.074349    0.000723     0.268281    0.081917   \n",
       "50%    117.453835    0.108431    0.001499     0.319929    0.086349   \n",
       "75%    135.999178    0.156254    0.002770     0.381731    0.090105   \n",
       "max    198.768029    0.381500    0.017487     0.635405    0.104724   \n",
       "\n",
       "       centroid_mean  centroid_var  rolloff_mean  roll_off_var    zcr_mean  \\\n",
       "count     400.000000  4.000000e+02    400.000000  4.000000e+02  400.000000   \n",
       "mean     1802.486037  3.180519e+05   3680.355592  1.672630e+06    0.080345   \n",
       "std       651.203680  2.810984e+05   1501.206045  1.395001e+06    0.041138   \n",
       "min       375.472632  1.150694e+04    535.694759  3.266810e+04    0.012583   \n",
       "25%      1300.376902  1.370374e+05   2575.757575  7.025397e+05    0.048085   \n",
       "50%      1772.615561  2.283629e+05   3595.035859  1.224653e+06    0.075321   \n",
       "75%      2248.942922  4.188546e+05   4785.132302  2.189253e+06    0.105276   \n",
       "max      3498.680882  2.039381e+06   6959.760493  8.731942e+06    0.246214   \n",
       "\n",
       "       ...  mfcc_mean_16  mfcc_var_16  mfcc_mean_17  mfcc_var_17  \\\n",
       "count  ...    400.000000   400.000000    400.000000   400.000000   \n",
       "mean   ...     -1.010624    59.615112     -4.744398    59.401602   \n",
       "std    ...      4.559469    40.272152      4.768623    37.590001   \n",
       "min    ...    -15.610699    14.551353    -24.893364    14.379240   \n",
       "25%    ...     -3.842349    34.335782     -7.630008    34.245143   \n",
       "50%    ...     -0.508271    50.099451     -4.506119    51.089298   \n",
       "75%    ...      1.939811    71.015478     -1.810183    70.942482   \n",
       "max    ...     13.872192   358.093811     12.957845   302.761414   \n",
       "\n",
       "       mfcc_mean_18  mfcc_var_18  mfcc_mean_19  mfcc_var_19  mfcc_mean_20  \\\n",
       "count    400.000000   400.000000    400.000000   400.000000    400.000000   \n",
       "mean      -0.608768    62.063296     -4.442099    66.349466     -0.848953   \n",
       "std        4.756607    38.923017      4.446831    42.345726      4.790249   \n",
       "min      -16.672720    13.698275    -19.804493    14.141619    -18.626137   \n",
       "25%       -3.382443    36.080644     -6.791193    38.651546     -3.680080   \n",
       "50%       -0.276665    53.090158     -4.065011    56.254971     -0.693477   \n",
       "75%        2.786151    75.734247     -1.440908    80.194420      1.858971   \n",
       "max       13.124251   303.582184      6.750900   280.381073     13.298891   \n",
       "\n",
       "       mfcc_var_20  \n",
       "count   400.000000  \n",
       "mean     73.378861  \n",
       "std      50.602605  \n",
       "min      13.516327  \n",
       "25%      40.332462  \n",
       "50%      62.006828  \n",
       "75%      92.351814  \n",
       "max     442.129852  \n",
       "\n",
       "[8 rows x 55 columns]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df2.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "425f82d8",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tempo</th>\n",
       "      <th>rms_mean</th>\n",
       "      <th>rms_var</th>\n",
       "      <th>chroma_mean</th>\n",
       "      <th>chroma_var</th>\n",
       "      <th>centroid_mean</th>\n",
       "      <th>centroid_var</th>\n",
       "      <th>rolloff_mean</th>\n",
       "      <th>roll_off_var</th>\n",
       "      <th>zcr_mean</th>\n",
       "      <th>...</th>\n",
       "      <th>mfcc_mean_16</th>\n",
       "      <th>mfcc_var_16</th>\n",
       "      <th>mfcc_mean_17</th>\n",
       "      <th>mfcc_var_17</th>\n",
       "      <th>mfcc_mean_18</th>\n",
       "      <th>mfcc_var_18</th>\n",
       "      <th>mfcc_mean_19</th>\n",
       "      <th>mfcc_var_19</th>\n",
       "      <th>mfcc_mean_20</th>\n",
       "      <th>mfcc_var_20</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mood</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>angry</th>\n",
       "      <td>119.791301</td>\n",
       "      <td>0.143684</td>\n",
       "      <td>0.002207</td>\n",
       "      <td>0.411701</td>\n",
       "      <td>0.078807</td>\n",
       "      <td>1682.188780</td>\n",
       "      <td>246726.109565</td>\n",
       "      <td>3599.542549</td>\n",
       "      <td>1.383809e+06</td>\n",
       "      <td>0.063626</td>\n",
       "      <td>...</td>\n",
       "      <td>1.716340</td>\n",
       "      <td>29.765322</td>\n",
       "      <td>-2.571759</td>\n",
       "      <td>30.967275</td>\n",
       "      <td>1.447512</td>\n",
       "      <td>31.125205</td>\n",
       "      <td>-2.521920</td>\n",
       "      <td>32.986997</td>\n",
       "      <td>0.481562</td>\n",
       "      <td>35.662359</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>happy</th>\n",
       "      <td>119.642832</td>\n",
       "      <td>0.133122</td>\n",
       "      <td>0.002388</td>\n",
       "      <td>0.370830</td>\n",
       "      <td>0.084728</td>\n",
       "      <td>2493.997373</td>\n",
       "      <td>294069.478889</td>\n",
       "      <td>5204.505149</td>\n",
       "      <td>1.297738e+06</td>\n",
       "      <td>0.126250</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.320654</td>\n",
       "      <td>51.620961</td>\n",
       "      <td>-5.706858</td>\n",
       "      <td>53.342652</td>\n",
       "      <td>0.194193</td>\n",
       "      <td>54.841584</td>\n",
       "      <td>-4.663004</td>\n",
       "      <td>61.447341</td>\n",
       "      <td>0.325686</td>\n",
       "      <td>64.573383</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>relaxed</th>\n",
       "      <td>121.566391</td>\n",
       "      <td>0.098366</td>\n",
       "      <td>0.002285</td>\n",
       "      <td>0.282407</td>\n",
       "      <td>0.089781</td>\n",
       "      <td>1330.463239</td>\n",
       "      <td>493941.118676</td>\n",
       "      <td>2562.743408</td>\n",
       "      <td>2.661889e+06</td>\n",
       "      <td>0.053082</td>\n",
       "      <td>...</td>\n",
       "      <td>-2.757295</td>\n",
       "      <td>80.247555</td>\n",
       "      <td>-4.782255</td>\n",
       "      <td>79.223248</td>\n",
       "      <td>-2.434461</td>\n",
       "      <td>79.815612</td>\n",
       "      <td>-5.031546</td>\n",
       "      <td>82.772726</td>\n",
       "      <td>-2.542924</td>\n",
       "      <td>91.384488</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>sad</th>\n",
       "      <td>123.612802</td>\n",
       "      <td>0.099908</td>\n",
       "      <td>0.001595</td>\n",
       "      <td>0.262961</td>\n",
       "      <td>0.088090</td>\n",
       "      <td>1703.294757</td>\n",
       "      <td>237471.037870</td>\n",
       "      <td>3354.631264</td>\n",
       "      <td>1.347084e+06</td>\n",
       "      <td>0.078424</td>\n",
       "      <td>...</td>\n",
       "      <td>-2.680886</td>\n",
       "      <td>76.826611</td>\n",
       "      <td>-5.916719</td>\n",
       "      <td>74.073230</td>\n",
       "      <td>-1.642316</td>\n",
       "      <td>82.470784</td>\n",
       "      <td>-5.551928</td>\n",
       "      <td>88.190799</td>\n",
       "      <td>-1.660135</td>\n",
       "      <td>101.895213</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>4 rows × 55 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "              tempo  rms_mean   rms_var  chroma_mean  chroma_var  \\\n",
       "mood                                                               \n",
       "angry    119.791301  0.143684  0.002207     0.411701    0.078807   \n",
       "happy    119.642832  0.133122  0.002388     0.370830    0.084728   \n",
       "relaxed  121.566391  0.098366  0.002285     0.282407    0.089781   \n",
       "sad      123.612802  0.099908  0.001595     0.262961    0.088090   \n",
       "\n",
       "         centroid_mean   centroid_var  rolloff_mean  roll_off_var  zcr_mean  \\\n",
       "mood                                                                          \n",
       "angry      1682.188780  246726.109565   3599.542549  1.383809e+06  0.063626   \n",
       "happy      2493.997373  294069.478889   5204.505149  1.297738e+06  0.126250   \n",
       "relaxed    1330.463239  493941.118676   2562.743408  2.661889e+06  0.053082   \n",
       "sad        1703.294757  237471.037870   3354.631264  1.347084e+06  0.078424   \n",
       "\n",
       "         ...  mfcc_mean_16  mfcc_var_16  mfcc_mean_17  mfcc_var_17  \\\n",
       "mood     ...                                                         \n",
       "angry    ...      1.716340    29.765322     -2.571759    30.967275   \n",
       "happy    ...     -0.320654    51.620961     -5.706858    53.342652   \n",
       "relaxed  ...     -2.757295    80.247555     -4.782255    79.223248   \n",
       "sad      ...     -2.680886    76.826611     -5.916719    74.073230   \n",
       "\n",
       "         mfcc_mean_18  mfcc_var_18  mfcc_mean_19  mfcc_var_19  mfcc_mean_20  \\\n",
       "mood                                                                          \n",
       "angry        1.447512    31.125205     -2.521920    32.986997      0.481562   \n",
       "happy        0.194193    54.841584     -4.663004    61.447341      0.325686   \n",
       "relaxed     -2.434461    79.815612     -5.031546    82.772726     -2.542924   \n",
       "sad         -1.642316    82.470784     -5.551928    88.190799     -1.660135   \n",
       "\n",
       "         mfcc_var_20  \n",
       "mood                  \n",
       "angry      35.662359  \n",
       "happy      64.573383  \n",
       "relaxed    91.384488  \n",
       "sad       101.895213  \n",
       "\n",
       "[4 rows x 55 columns]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df2.groupby('mood').mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0eca9703",
   "metadata": {},
   "source": [
    "## 2. Data preprocessing\n",
    "### 2.1 Define Features and Labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6be28fc1",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['angry', 'happy', 'relaxed', 'sad'], dtype=object)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X1 = df1.drop(['mood'],axis=1)\n",
    "y1 = df1['mood']\n",
    "np.unique(y1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "99328a60",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['angry', 'happy', 'relaxed', 'sad'], dtype=object)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X2 = df2.drop(['mood'],axis=1)\n",
    "y2 = df2['mood']\n",
    "np.unique(y2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf5d5d1f",
   "metadata": {},
   "source": [
    "### 2.2 Encode the non-numerical features labels\n",
    "\n",
    "**Encode the labels in numerical way:**\n",
    "- Happy: 1 \n",
    "- Angry: 0 \n",
    "- Relaxed: 2 \n",
    "- Sad: 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "0e1d89f0",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 1, 2, 3])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "le = LabelEncoder()\n",
    "le.fit(y1)\n",
    "y1 = le.transform(y1)\n",
    "np.unique(y1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f32e6a91",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 1, 2, 3])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "le.fit(y2)\n",
    "y2 = le.transform(y2)\n",
    "np.unique(y2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e15a79f5",
   "metadata": {},
   "source": [
    "**Then encode the non-numeric variable scale:**\n",
    "- major: 0\n",
    "- minor: 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "8e64ba99",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['major', 'minor'], dtype=object)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X1['scale'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "53bcb6c9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 1])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "le.fit(X1['scale'])\n",
    "X1['scale'] = le.transform(X1['scale'])\n",
    "X1['scale'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "e96d6209",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['minor', 'major'], dtype=object)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X2['scale'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "0532f958",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 0])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "le.fit(X2['scale'])\n",
    "X2['scale'] = le.transform(X2['scale'])\n",
    "X2['scale'].unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6282266b",
   "metadata": {},
   "source": [
    "**Finally, encode the non-numeric variable key:**\n",
    "- A: 0\n",
    "- A#: 1\n",
    "- B: 2\n",
    "- C: 3\n",
    "- C#: 4\n",
    "- D: 5\n",
    "- D#: 6\n",
    "- E: 7\n",
    "- F: 8\n",
    "- F#: 9\n",
    "- G: 10\n",
    "- G#: 11"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "cbc4c682",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['C', 'A', 'E', 'G', 'F', 'A#', 'G#', 'D', 'F#', 'B', 'C#', 'D#'],\n",
       "      dtype=object)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X1['key'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "7324da64",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 3,  0,  7, 10,  8,  1, 11,  5,  9,  2,  4,  6])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "le.fit(X1['key'])\n",
    "X1['key'] = le.transform(X1['key'])\n",
    "X1['key'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "42fbeede",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['C#', 'D', 'G', 'F', 'G#', 'B', 'C', 'E', 'A', 'D#', 'F#', 'A#'],\n",
       "      dtype=object)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X2['key'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "8d88fb77",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 4,  5, 10,  8, 11,  2,  3,  7,  0,  6,  9,  1])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "le.fit(X2['key'])\n",
    "X2['key'] = le.transform(X2['key'])\n",
    "X2['key'].unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1c55b16",
   "metadata": {},
   "source": [
    "### 2.3 Train Test Validation Split\n",
    "Finally, split the dataset into train and test set, 80% of data are used as train set, 10% of data are used as test set and the remaining ones are used for validation set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "7cfd54b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "X1_train, X1_test, y1_train, y1_test = train_test_split(X1, y1, \n",
    "                                                        train_size = 0.8, \n",
    "                                                        random_state = 13,  \n",
    "                                                        stratify = y1)\n",
    "\n",
    "X1_train, X1_val, y1_train, y1_val = train_test_split(X1_train, y1_train, \n",
    "                                                      train_size = 0.8, \n",
    "                                                      random_state = 13,  \n",
    "                                                      stratify = y1_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "4900c2c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "X2_train, X2_test, y2_train, y2_test = train_test_split(X2, y2, \n",
    "                                                    train_size = 0.8, \n",
    "                                                    random_state = 13, \n",
    "                                                    shuffle = True, \n",
    "                                                    stratify = y2)\n",
    "X2_train, X2_val, y2_train, y2_val = train_test_split(X2_train, y2_train, \n",
    "                                                train_size = 0.8, \n",
    "                                                random_state = 13, \n",
    "                                                shuffle = True, \n",
    "                                                stratify = y2_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6a1c809",
   "metadata": {},
   "source": [
    "### 2.4 Feature Scaling\n",
    "As the range of the variales varies distinctly, in order to make the learning process better, the features need to be scaled into similar ranges. Here the Min-Max Scaler is used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "38e763e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = MinMaxScaler()\n",
    "X1_train_scaled = scaler.fit_transform(X1_train.drop('file',axis=1))\n",
    "X1_val_scaled = scaler.fit_transform(X1_val.drop('file',axis=1))\n",
    "X1_test_scaled = scaler.fit_transform(X1_test.drop('file',axis=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "ed445136",
   "metadata": {},
   "outputs": [],
   "source": [
    "X2_train_scaled = scaler.fit_transform(X2_train.drop('file',axis=1))\n",
    "X2_val_scaled = scaler.fit_transform(X2_val.drop('file',axis=1))\n",
    "X2_test_scaled = scaler.fit_transform(X2_test.drop('file',axis=1))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "571f2ff7",
   "metadata": {},
   "source": [
    "## 3. Train the model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "963df061",
   "metadata": {},
   "source": [
    "### 3.1 Overfitting problems and comparison study for improving the validation accuracy\n",
    "Using data in dataset 1 as training data, the model always suffers from the overfitting problems regardless of the\n",
    "adjustment of the parameters and number of input features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "5153ff50",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Define Model Builder of 3 conv1D layers and 2 fully connected layers\n",
    "# Input data and paras of layers are changable\n",
    "\n",
    "def modelBuilder3L(X_train,\n",
    "                  f1,k1,a1,\n",
    "                  f2,k2,a2,\n",
    "                  f3,k3,a3,\n",
    "                  d1,dr1,da1,r1,\n",
    "                  d2,dr2,da2,r2,\n",
    "                  num):\n",
    "    '''\n",
    "    args:\n",
    "    \n",
    "    X_train: training data\n",
    "    f1,k1,a1: num of filters, filter size and activation func of 1st conv1D layer\n",
    "    f2,k2,a2: num of filters, filter size and activation func of 2nd conv1D layer\n",
    "    f3,k3,a3: num of filters, filter size and activation func of 3rd conv1D layer\n",
    "    d1,dr1,da1, r1: num of units, dropout, activation func and regularizer para of 1st fully connected layer\n",
    "    d2,dr2,da2, r2: num of units, dropout, activation func and regularizer para of 2nd fully connected layer\n",
    "    num: integer for distinguishing different model\n",
    "    \n",
    "    return:\n",
    "    model\n",
    "    \n",
    "    '''\n",
    "    model = Sequential(name=\"Conv1D_\"+str(num))\n",
    "\n",
    "    m,n = X_train.shape\n",
    "\n",
    "    #layer 1\n",
    "    model.add(Conv1D(filters = f1, kernel_size = k1, input_shape = (n,1), padding = 'same', activation = a1, name ='Conv1D_1'))\n",
    "    model.add(BatchNormalization(name = \"BN1\"))\n",
    "    model.add(MaxPooling1D(name = \"MaxPooling1\"))\n",
    "    #layer 2\n",
    "    model.add(Conv1D(filters = f2, kernel_size = k2, activation = a2, padding='same', name = \"Conv1D_2\"))\n",
    "    model.add(BatchNormalization(name = \"BN2\"))\n",
    "    model.add(MaxPooling1D(name = \"MaxPooling2\"))\n",
    "    #layer 3\n",
    "    model.add(Conv1D(filters = f3, kernel_size = k3, activation = a3, padding='same', name = \"Conv1D_3\"))\n",
    "    model.add(BatchNormalization(name = \"BN3\"))\n",
    "    model.add(MaxPooling1D(name = \"MaxPooling3\"))\n",
    "    #Flatten output\n",
    "    model.add(Flatten(name = \"Flatten\"))\n",
    "\n",
    "    #Fully connected layer 4\n",
    "    model.add(Dense(d1, activation = da1, name = \"Dense_1\"))\n",
    "    #Prevent overfitting\n",
    "    model.add(Dropout(dr1, name = \"Dropout_1\"))\n",
    "\n",
    "    #Fully connected layer 6\n",
    "    model.add(Dense(d2, activation = da2, name = \"Dense_2\"))\n",
    "    #Prevent overfitting\n",
    "    model.add(Dropout(dr2, name = \"Dropout_2\"))\n",
    "\n",
    "    #Output layer\n",
    "    model.add(Dense(4, activation = 'softmax', name = \"Softmax\"))\n",
    "\n",
    "    #model compiling\n",
    "    optimizer = Adam(learning_rate=0.0001)\n",
    "    model.compile(loss = 'sparse_categorical_crossentropy', optimizer = optimizer, metrics=['accuracy'])\n",
    "    model.summary()\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "aca4bb89",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"Conv1D_1\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " Conv1D_1 (Conv1D)           (None, 57, 16)            96        \n",
      "                                                                 \n",
      " BN1 (BatchNormalization)    (None, 57, 16)            64        \n",
      "                                                                 \n",
      " MaxPooling1 (MaxPooling1D)  (None, 28, 16)            0         \n",
      "                                                                 \n",
      " Conv1D_2 (Conv1D)           (None, 28, 32)            3616      \n",
      "                                                                 \n",
      " BN2 (BatchNormalization)    (None, 28, 32)            128       \n",
      "                                                                 \n",
      " MaxPooling2 (MaxPooling1D)  (None, 14, 32)            0         \n",
      "                                                                 \n",
      " Conv1D_3 (Conv1D)           (None, 14, 32)            7200      \n",
      "                                                                 \n",
      " BN3 (BatchNormalization)    (None, 14, 32)            128       \n",
      "                                                                 \n",
      " MaxPooling3 (MaxPooling1D)  (None, 7, 32)             0         \n",
      "                                                                 \n",
      " Flatten (Flatten)           (None, 224)               0         \n",
      "                                                                 \n",
      " Dense_1 (Dense)             (None, 128)               28800     \n",
      "                                                                 \n",
      " Dropout_1 (Dropout)         (None, 128)               0         \n",
      "                                                                 \n",
      " Dense_2 (Dense)             (None, 64)                8256      \n",
      "                                                                 \n",
      " Dropout_2 (Dropout)         (None, 64)                0         \n",
      "                                                                 \n",
      " Softmax (Dense)             (None, 4)                 260       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 48,548\n",
      "Trainable params: 48,388\n",
      "Non-trainable params: 160\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "f1,k1,a1 = 16,5,'relu'\n",
    "f2,k2,a2 = 32,7,'relu'\n",
    "f3,k3,a3 = 32,7,'relu'\n",
    "d1,dr1,da1,r1 = 128,0.2,'relu',0\n",
    "d2,dr2,da2,r2 = 64,0.2,'relu',0\n",
    "num = 1\n",
    "\n",
    "model1 = modelBuilder3L(X1_train_scaled,\n",
    "                        f1,k1,a1,\n",
    "                        f2,k2,a2,\n",
    "                        f3,k3,a3,\n",
    "                        d1,dr1,da1,r1,\n",
    "                        d2,dr2,da2,r2,\n",
    "                        num)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "357e957b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/80\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "36/36 [==============================] - 3s 16ms/step - loss: 1.7021 - accuracy: 0.2979 - val_loss: 1.3830 - val_accuracy: 0.2500\n",
      "Epoch 2/80\n",
      "36/36 [==============================] - 0s 6ms/step - loss: 1.3754 - accuracy: 0.4024 - val_loss: 1.3851 - val_accuracy: 0.2500\n",
      "Epoch 3/80\n",
      "36/36 [==============================] - 0s 6ms/step - loss: 1.2827 - accuracy: 0.4443 - val_loss: 1.3907 - val_accuracy: 0.2500\n",
      "Epoch 4/80\n",
      "36/36 [==============================] - 0s 6ms/step - loss: 1.1984 - accuracy: 0.4373 - val_loss: 1.3954 - val_accuracy: 0.2500\n",
      "Epoch 5/80\n",
      "36/36 [==============================] - 0s 6ms/step - loss: 1.0811 - accuracy: 0.5314 - val_loss: 1.3939 - val_accuracy: 0.2500\n",
      "Epoch 6/80\n",
      "36/36 [==============================] - 0s 6ms/step - loss: 1.0664 - accuracy: 0.5105 - val_loss: 1.3856 - val_accuracy: 0.2431\n",
      "Epoch 7/80\n",
      "36/36 [==============================] - 0s 8ms/step - loss: 0.9979 - accuracy: 0.5314 - val_loss: 1.3787 - val_accuracy: 0.2500\n",
      "Epoch 8/80\n",
      "36/36 [==============================] - 0s 6ms/step - loss: 0.9885 - accuracy: 0.5488 - val_loss: 1.3638 - val_accuracy: 0.2431\n",
      "Epoch 9/80\n",
      "36/36 [==============================] - 0s 6ms/step - loss: 0.9500 - accuracy: 0.5836 - val_loss: 1.3373 - val_accuracy: 0.2500\n",
      "Epoch 10/80\n",
      "36/36 [==============================] - 0s 6ms/step - loss: 0.9431 - accuracy: 0.5610 - val_loss: 1.3228 - val_accuracy: 0.2847\n",
      "Epoch 11/80\n",
      "36/36 [==============================] - 0s 6ms/step - loss: 0.8647 - accuracy: 0.6359 - val_loss: 1.2623 - val_accuracy: 0.3403\n",
      "Epoch 12/80\n",
      "36/36 [==============================] - 0s 9ms/step - loss: 0.8587 - accuracy: 0.6359 - val_loss: 1.2162 - val_accuracy: 0.4444\n",
      "Epoch 13/80\n",
      "36/36 [==============================] - 0s 6ms/step - loss: 0.8534 - accuracy: 0.6063 - val_loss: 1.1757 - val_accuracy: 0.4931\n",
      "Epoch 14/80\n",
      "36/36 [==============================] - 0s 6ms/step - loss: 0.7780 - accuracy: 0.6690 - val_loss: 1.1625 - val_accuracy: 0.4861\n",
      "Epoch 15/80\n",
      "36/36 [==============================] - 0s 8ms/step - loss: 0.7895 - accuracy: 0.6655 - val_loss: 1.1337 - val_accuracy: 0.5069\n",
      "Epoch 16/80\n",
      "36/36 [==============================] - 0s 6ms/step - loss: 0.7666 - accuracy: 0.6829 - val_loss: 1.1235 - val_accuracy: 0.4861\n",
      "Epoch 17/80\n",
      "36/36 [==============================] - 0s 6ms/step - loss: 0.7402 - accuracy: 0.6951 - val_loss: 1.1252 - val_accuracy: 0.4792\n",
      "Epoch 18/80\n",
      "36/36 [==============================] - 1s 16ms/step - loss: 0.7344 - accuracy: 0.7003 - val_loss: 1.1159 - val_accuracy: 0.4861\n",
      "Epoch 19/80\n",
      "36/36 [==============================] - 0s 12ms/step - loss: 0.6959 - accuracy: 0.7178 - val_loss: 1.1260 - val_accuracy: 0.4792\n",
      "Epoch 20/80\n",
      "36/36 [==============================] - 0s 11ms/step - loss: 0.6943 - accuracy: 0.7178 - val_loss: 1.1353 - val_accuracy: 0.4722\n",
      "Epoch 21/80\n",
      "36/36 [==============================] - 0s 10ms/step - loss: 0.6741 - accuracy: 0.7195 - val_loss: 1.1404 - val_accuracy: 0.4792\n",
      "Epoch 22/80\n",
      "36/36 [==============================] - 0s 9ms/step - loss: 0.6471 - accuracy: 0.7456 - val_loss: 1.1521 - val_accuracy: 0.4722\n",
      "Epoch 23/80\n",
      "36/36 [==============================] - 0s 8ms/step - loss: 0.6529 - accuracy: 0.7247 - val_loss: 1.1535 - val_accuracy: 0.4931\n",
      "Epoch 24/80\n",
      "36/36 [==============================] - 0s 10ms/step - loss: 0.6130 - accuracy: 0.7770 - val_loss: 1.1601 - val_accuracy: 0.4722\n",
      "Epoch 25/80\n",
      "36/36 [==============================] - 0s 6ms/step - loss: 0.5715 - accuracy: 0.7909 - val_loss: 1.1687 - val_accuracy: 0.4722\n",
      "Epoch 26/80\n",
      "36/36 [==============================] - 0s 6ms/step - loss: 0.5821 - accuracy: 0.7805 - val_loss: 1.1770 - val_accuracy: 0.4653\n",
      "Epoch 27/80\n",
      "36/36 [==============================] - 0s 6ms/step - loss: 0.5928 - accuracy: 0.7509 - val_loss: 1.1745 - val_accuracy: 0.4861\n",
      "Epoch 28/80\n",
      "36/36 [==============================] - 0s 6ms/step - loss: 0.5168 - accuracy: 0.8223 - val_loss: 1.2046 - val_accuracy: 0.4722\n",
      "Epoch 29/80\n",
      "36/36 [==============================] - 0s 6ms/step - loss: 0.5292 - accuracy: 0.7979 - val_loss: 1.1935 - val_accuracy: 0.4792\n",
      "Epoch 30/80\n",
      "36/36 [==============================] - 0s 6ms/step - loss: 0.5129 - accuracy: 0.8031 - val_loss: 1.1925 - val_accuracy: 0.4792\n",
      "Epoch 31/80\n",
      "36/36 [==============================] - 0s 7ms/step - loss: 0.4822 - accuracy: 0.8362 - val_loss: 1.1949 - val_accuracy: 0.4861\n",
      "Epoch 32/80\n",
      "36/36 [==============================] - 0s 9ms/step - loss: 0.4938 - accuracy: 0.8258 - val_loss: 1.2304 - val_accuracy: 0.4653\n",
      "Epoch 33/80\n",
      "36/36 [==============================] - 0s 11ms/step - loss: 0.4622 - accuracy: 0.8415 - val_loss: 1.2493 - val_accuracy: 0.4583\n",
      "Epoch 34/80\n",
      "36/36 [==============================] - 0s 6ms/step - loss: 0.4332 - accuracy: 0.8519 - val_loss: 1.2595 - val_accuracy: 0.4583\n",
      "Epoch 35/80\n",
      "36/36 [==============================] - 0s 6ms/step - loss: 0.4239 - accuracy: 0.8641 - val_loss: 1.2640 - val_accuracy: 0.4653\n",
      "Epoch 36/80\n",
      "36/36 [==============================] - 0s 8ms/step - loss: 0.4486 - accuracy: 0.8397 - val_loss: 1.3127 - val_accuracy: 0.4514\n",
      "Epoch 37/80\n",
      "36/36 [==============================] - 0s 7ms/step - loss: 0.3934 - accuracy: 0.8624 - val_loss: 1.2788 - val_accuracy: 0.4653\n",
      "Epoch 38/80\n",
      "36/36 [==============================] - 0s 6ms/step - loss: 0.4171 - accuracy: 0.8537 - val_loss: 1.2716 - val_accuracy: 0.4653\n",
      "Epoch 39/80\n",
      "36/36 [==============================] - 0s 7ms/step - loss: 0.3773 - accuracy: 0.8693 - val_loss: 1.3130 - val_accuracy: 0.4722\n",
      "Epoch 40/80\n",
      "36/36 [==============================] - 0s 9ms/step - loss: 0.3781 - accuracy: 0.8763 - val_loss: 1.3109 - val_accuracy: 0.4583\n",
      "Epoch 41/80\n",
      "36/36 [==============================] - 0s 11ms/step - loss: 0.3564 - accuracy: 0.8902 - val_loss: 1.3154 - val_accuracy: 0.4583\n",
      "Epoch 42/80\n",
      "36/36 [==============================] - 0s 9ms/step - loss: 0.3232 - accuracy: 0.9077 - val_loss: 1.3445 - val_accuracy: 0.4792\n",
      "Epoch 43/80\n",
      "36/36 [==============================] - 0s 9ms/step - loss: 0.3501 - accuracy: 0.8868 - val_loss: 1.3652 - val_accuracy: 0.4861\n",
      "Epoch 44/80\n",
      "36/36 [==============================] - 0s 10ms/step - loss: 0.3312 - accuracy: 0.8833 - val_loss: 1.3698 - val_accuracy: 0.5069\n",
      "Epoch 45/80\n",
      "36/36 [==============================] - 0s 11ms/step - loss: 0.3238 - accuracy: 0.8780 - val_loss: 1.3801 - val_accuracy: 0.4931\n",
      "Epoch 46/80\n",
      "36/36 [==============================] - 0s 7ms/step - loss: 0.2790 - accuracy: 0.9164 - val_loss: 1.3937 - val_accuracy: 0.4792\n",
      "Epoch 47/80\n",
      "36/36 [==============================] - 0s 6ms/step - loss: 0.2746 - accuracy: 0.9129 - val_loss: 1.3960 - val_accuracy: 0.4931\n",
      "Epoch 48/80\n",
      "36/36 [==============================] - 0s 6ms/step - loss: 0.2327 - accuracy: 0.9286 - val_loss: 1.4240 - val_accuracy: 0.5000\n",
      "Epoch 49/80\n",
      "36/36 [==============================] - 0s 6ms/step - loss: 0.2487 - accuracy: 0.9251 - val_loss: 1.4252 - val_accuracy: 0.4861\n",
      "Epoch 50/80\n",
      "36/36 [==============================] - 0s 9ms/step - loss: 0.2241 - accuracy: 0.9355 - val_loss: 1.4695 - val_accuracy: 0.4861\n",
      "Epoch 51/80\n",
      "36/36 [==============================] - 0s 6ms/step - loss: 0.2445 - accuracy: 0.9164 - val_loss: 1.5058 - val_accuracy: 0.4861\n",
      "Epoch 52/80\n",
      "36/36 [==============================] - 0s 8ms/step - loss: 0.2294 - accuracy: 0.9321 - val_loss: 1.5016 - val_accuracy: 0.4931\n",
      "Epoch 53/80\n",
      "36/36 [==============================] - 0s 6ms/step - loss: 0.2240 - accuracy: 0.9477 - val_loss: 1.5478 - val_accuracy: 0.4931\n",
      "Epoch 54/80\n",
      "36/36 [==============================] - 0s 7ms/step - loss: 0.2195 - accuracy: 0.9460 - val_loss: 1.5306 - val_accuracy: 0.4792\n",
      "Epoch 55/80\n",
      "36/36 [==============================] - 0s 7ms/step - loss: 0.2135 - accuracy: 0.9460 - val_loss: 1.5822 - val_accuracy: 0.5069\n",
      "Epoch 56/80\n",
      "36/36 [==============================] - 0s 6ms/step - loss: 0.1930 - accuracy: 0.9495 - val_loss: 1.5836 - val_accuracy: 0.5069\n",
      "Epoch 57/80\n",
      "36/36 [==============================] - 0s 6ms/step - loss: 0.1909 - accuracy: 0.9547 - val_loss: 1.6114 - val_accuracy: 0.4931\n",
      "Epoch 58/80\n",
      "36/36 [==============================] - 0s 6ms/step - loss: 0.1830 - accuracy: 0.9477 - val_loss: 1.6215 - val_accuracy: 0.4792\n",
      "Epoch 59/80\n",
      "36/36 [==============================] - 0s 7ms/step - loss: 0.1663 - accuracy: 0.9495 - val_loss: 1.6539 - val_accuracy: 0.4792\n",
      "Epoch 60/80\n",
      "36/36 [==============================] - 0s 9ms/step - loss: 0.1731 - accuracy: 0.9390 - val_loss: 1.6565 - val_accuracy: 0.4931\n",
      "Epoch 61/80\n",
      "36/36 [==============================] - 0s 7ms/step - loss: 0.1524 - accuracy: 0.9582 - val_loss: 1.6529 - val_accuracy: 0.4931\n",
      "Epoch 62/80\n",
      "36/36 [==============================] - 0s 7ms/step - loss: 0.1539 - accuracy: 0.9739 - val_loss: 1.6748 - val_accuracy: 0.4792\n",
      "Epoch 63/80\n",
      "36/36 [==============================] - 0s 7ms/step - loss: 0.1316 - accuracy: 0.9721 - val_loss: 1.7050 - val_accuracy: 0.4931\n",
      "Epoch 64/80\n",
      "36/36 [==============================] - 0s 6ms/step - loss: 0.1375 - accuracy: 0.9652 - val_loss: 1.6921 - val_accuracy: 0.4792\n",
      "Epoch 65/80\n",
      "36/36 [==============================] - 0s 7ms/step - loss: 0.1376 - accuracy: 0.9756 - val_loss: 1.7198 - val_accuracy: 0.5000\n",
      "Epoch 66/80\n",
      "36/36 [==============================] - 0s 7ms/step - loss: 0.1063 - accuracy: 0.9843 - val_loss: 1.7544 - val_accuracy: 0.5139\n",
      "Epoch 67/80\n",
      "36/36 [==============================] - 0s 6ms/step - loss: 0.1033 - accuracy: 0.9826 - val_loss: 1.7953 - val_accuracy: 0.5000\n",
      "Epoch 68/80\n",
      "36/36 [==============================] - 0s 10ms/step - loss: 0.1218 - accuracy: 0.9721 - val_loss: 1.7698 - val_accuracy: 0.4931\n",
      "Epoch 69/80\n",
      "36/36 [==============================] - 0s 7ms/step - loss: 0.1123 - accuracy: 0.9791 - val_loss: 1.7730 - val_accuracy: 0.4931\n",
      "Epoch 70/80\n",
      "36/36 [==============================] - 0s 6ms/step - loss: 0.1140 - accuracy: 0.9739 - val_loss: 1.8244 - val_accuracy: 0.5000\n",
      "Epoch 71/80\n",
      "36/36 [==============================] - 0s 6ms/step - loss: 0.1144 - accuracy: 0.9721 - val_loss: 1.8546 - val_accuracy: 0.4931\n",
      "Epoch 72/80\n",
      "36/36 [==============================] - 0s 6ms/step - loss: 0.1110 - accuracy: 0.9808 - val_loss: 1.8819 - val_accuracy: 0.4722\n",
      "Epoch 73/80\n",
      "36/36 [==============================] - 0s 6ms/step - loss: 0.1005 - accuracy: 0.9774 - val_loss: 1.9143 - val_accuracy: 0.4861\n",
      "Epoch 74/80\n",
      "36/36 [==============================] - 0s 6ms/step - loss: 0.0879 - accuracy: 0.9808 - val_loss: 1.9103 - val_accuracy: 0.5000\n",
      "Epoch 75/80\n",
      "36/36 [==============================] - 0s 6ms/step - loss: 0.0917 - accuracy: 0.9739 - val_loss: 1.9162 - val_accuracy: 0.4861\n",
      "Epoch 76/80\n",
      "36/36 [==============================] - 0s 6ms/step - loss: 0.0910 - accuracy: 0.9739 - val_loss: 1.9473 - val_accuracy: 0.4653\n",
      "Epoch 77/80\n",
      "36/36 [==============================] - 0s 6ms/step - loss: 0.0775 - accuracy: 0.9895 - val_loss: 1.9623 - val_accuracy: 0.4792\n",
      "Epoch 78/80\n",
      "36/36 [==============================] - 0s 6ms/step - loss: 0.0953 - accuracy: 0.9791 - val_loss: 1.9195 - val_accuracy: 0.4792\n",
      "Epoch 79/80\n",
      "36/36 [==============================] - 0s 7ms/step - loss: 0.0846 - accuracy: 0.9791 - val_loss: 1.9602 - val_accuracy: 0.4861\n",
      "Epoch 80/80\n",
      "36/36 [==============================] - 0s 9ms/step - loss: 0.0792 - accuracy: 0.9808 - val_loss: 1.9636 - val_accuracy: 0.4861\n"
     ]
    }
   ],
   "source": [
    "history1 = model1.fit(X1_train_scaled, y1_train, \n",
    "                      validation_data=(X1_val_scaled, y1_val), \n",
    "                      batch_size=16, \n",
    "                      epochs=80)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd40c7df",
   "metadata": {},
   "source": [
    "**Add regularization**</br>\n",
    "The training accuarcy is high but validation accuracy is low, which means the model is overfitting. Therefore, add the regularization parameters to the dense layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "256f299d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"Conv1D_2\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " Conv1D_1 (Conv1D)           (None, 57, 16)            96        \n",
      "                                                                 \n",
      " BN1 (BatchNormalization)    (None, 57, 16)            64        \n",
      "                                                                 \n",
      " MaxPooling1 (MaxPooling1D)  (None, 28, 16)            0         \n",
      "                                                                 \n",
      " Conv1D_2 (Conv1D)           (None, 28, 32)            3616      \n",
      "                                                                 \n",
      " BN2 (BatchNormalization)    (None, 28, 32)            128       \n",
      "                                                                 \n",
      " MaxPooling2 (MaxPooling1D)  (None, 14, 32)            0         \n",
      "                                                                 \n",
      " Conv1D_3 (Conv1D)           (None, 14, 32)            7200      \n",
      "                                                                 \n",
      " BN3 (BatchNormalization)    (None, 14, 32)            128       \n",
      "                                                                 \n",
      " MaxPooling3 (MaxPooling1D)  (None, 7, 32)             0         \n",
      "                                                                 \n",
      " Flatten (Flatten)           (None, 224)               0         \n",
      "                                                                 \n",
      " Dense_1 (Dense)             (None, 128)               28800     \n",
      "                                                                 \n",
      " Dropout_1 (Dropout)         (None, 128)               0         \n",
      "                                                                 \n",
      " Dense_2 (Dense)             (None, 64)                8256      \n",
      "                                                                 \n",
      " Dropout_2 (Dropout)         (None, 64)                0         \n",
      "                                                                 \n",
      " Softmax (Dense)             (None, 4)                 260       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 48,548\n",
      "Trainable params: 48,388\n",
      "Non-trainable params: 160\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "f1,k1,a1 = 16,5,'relu'\n",
    "f2,k2,a2 = 32,7,'relu'\n",
    "f3,k3,a3 = 32,7,'relu'\n",
    "d1,dr1,da1,r1 = 128,0.2,'relu',0.15\n",
    "d2,dr2,da2,r2 = 64,0.2,'relu',0.15\n",
    "num = 2\n",
    "\n",
    "model2 = modelBuilder3L(X1_train_scaled,\n",
    "                        f1,k1,a1,\n",
    "                        f2,k2,a2,\n",
    "                        f3,k3,a3,\n",
    "                        d1,dr1,da1,r1,\n",
    "                        d2,dr2,da2,r2,\n",
    "                        num)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "5a3822fd",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/80\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "36/36 [==============================] - 3s 15ms/step - loss: 1.7591 - accuracy: 0.2700 - val_loss: 1.3873 - val_accuracy: 0.2431\n",
      "Epoch 2/80\n",
      "36/36 [==============================] - 0s 6ms/step - loss: 1.4318 - accuracy: 0.3432 - val_loss: 1.3900 - val_accuracy: 0.2500\n",
      "Epoch 3/80\n",
      "36/36 [==============================] - 0s 8ms/step - loss: 1.2872 - accuracy: 0.4233 - val_loss: 1.3893 - val_accuracy: 0.2500\n",
      "Epoch 4/80\n",
      "36/36 [==============================] - 0s 7ms/step - loss: 1.2267 - accuracy: 0.4286 - val_loss: 1.3871 - val_accuracy: 0.2500\n",
      "Epoch 5/80\n",
      "36/36 [==============================] - 0s 6ms/step - loss: 1.1407 - accuracy: 0.4826 - val_loss: 1.3816 - val_accuracy: 0.2639\n",
      "Epoch 6/80\n",
      "36/36 [==============================] - 0s 6ms/step - loss: 1.0178 - accuracy: 0.5366 - val_loss: 1.3712 - val_accuracy: 0.3194\n",
      "Epoch 7/80\n",
      "36/36 [==============================] - 0s 6ms/step - loss: 1.0485 - accuracy: 0.5401 - val_loss: 1.3577 - val_accuracy: 0.3542\n",
      "Epoch 8/80\n",
      "36/36 [==============================] - 0s 7ms/step - loss: 0.9938 - accuracy: 0.5575 - val_loss: 1.3437 - val_accuracy: 0.3333\n",
      "Epoch 9/80\n",
      "36/36 [==============================] - 0s 7ms/step - loss: 0.9678 - accuracy: 0.5679 - val_loss: 1.3187 - val_accuracy: 0.3472\n",
      "Epoch 10/80\n",
      "36/36 [==============================] - 0s 7ms/step - loss: 0.8997 - accuracy: 0.5941 - val_loss: 1.2845 - val_accuracy: 0.3681\n",
      "Epoch 11/80\n",
      "36/36 [==============================] - 0s 9ms/step - loss: 0.8920 - accuracy: 0.6220 - val_loss: 1.2285 - val_accuracy: 0.4306\n",
      "Epoch 12/80\n",
      "36/36 [==============================] - 0s 7ms/step - loss: 0.8656 - accuracy: 0.6498 - val_loss: 1.1748 - val_accuracy: 0.4722\n",
      "Epoch 13/80\n",
      "36/36 [==============================] - 0s 7ms/step - loss: 0.8293 - accuracy: 0.6655 - val_loss: 1.1172 - val_accuracy: 0.5347\n",
      "Epoch 14/80\n",
      "36/36 [==============================] - 0s 7ms/step - loss: 0.8320 - accuracy: 0.6707 - val_loss: 1.0789 - val_accuracy: 0.5486\n",
      "Epoch 15/80\n",
      "36/36 [==============================] - 0s 6ms/step - loss: 0.8256 - accuracy: 0.6498 - val_loss: 1.0525 - val_accuracy: 0.5625\n",
      "Epoch 16/80\n",
      "36/36 [==============================] - 0s 7ms/step - loss: 0.7602 - accuracy: 0.6916 - val_loss: 1.0461 - val_accuracy: 0.5278\n",
      "Epoch 17/80\n",
      "36/36 [==============================] - 0s 7ms/step - loss: 0.7662 - accuracy: 0.6742 - val_loss: 1.0357 - val_accuracy: 0.5347\n",
      "Epoch 18/80\n",
      "36/36 [==============================] - 0s 6ms/step - loss: 0.7180 - accuracy: 0.7091 - val_loss: 1.0322 - val_accuracy: 0.5278\n",
      "Epoch 19/80\n",
      "36/36 [==============================] - 0s 11ms/step - loss: 0.6864 - accuracy: 0.7265 - val_loss: 1.0307 - val_accuracy: 0.5417\n",
      "Epoch 20/80\n",
      "36/36 [==============================] - 0s 9ms/step - loss: 0.6827 - accuracy: 0.7317 - val_loss: 1.0331 - val_accuracy: 0.5556\n",
      "Epoch 21/80\n",
      "36/36 [==============================] - 0s 6ms/step - loss: 0.6440 - accuracy: 0.7578 - val_loss: 1.0430 - val_accuracy: 0.5278\n",
      "Epoch 22/80\n",
      "36/36 [==============================] - 0s 8ms/step - loss: 0.6585 - accuracy: 0.7456 - val_loss: 1.0414 - val_accuracy: 0.5417\n",
      "Epoch 23/80\n",
      "36/36 [==============================] - 0s 7ms/step - loss: 0.6038 - accuracy: 0.7787 - val_loss: 1.0516 - val_accuracy: 0.5347\n",
      "Epoch 24/80\n",
      "36/36 [==============================] - 0s 9ms/step - loss: 0.5793 - accuracy: 0.7997 - val_loss: 1.0588 - val_accuracy: 0.5278\n",
      "Epoch 25/80\n",
      "36/36 [==============================] - 0s 8ms/step - loss: 0.5969 - accuracy: 0.7875 - val_loss: 1.0605 - val_accuracy: 0.5347\n",
      "Epoch 26/80\n",
      "36/36 [==============================] - 0s 8ms/step - loss: 0.5788 - accuracy: 0.7718 - val_loss: 1.0658 - val_accuracy: 0.5278\n",
      "Epoch 27/80\n",
      "36/36 [==============================] - 0s 6ms/step - loss: 0.5236 - accuracy: 0.8118 - val_loss: 1.0756 - val_accuracy: 0.5208\n",
      "Epoch 28/80\n",
      "36/36 [==============================] - 0s 7ms/step - loss: 0.5317 - accuracy: 0.7979 - val_loss: 1.0802 - val_accuracy: 0.5208\n",
      "Epoch 29/80\n",
      "36/36 [==============================] - 0s 6ms/step - loss: 0.4973 - accuracy: 0.8240 - val_loss: 1.0827 - val_accuracy: 0.5139\n",
      "Epoch 30/80\n",
      "36/36 [==============================] - 0s 8ms/step - loss: 0.4929 - accuracy: 0.8031 - val_loss: 1.0983 - val_accuracy: 0.5208\n",
      "Epoch 31/80\n",
      "36/36 [==============================] - 0s 6ms/step - loss: 0.5062 - accuracy: 0.8188 - val_loss: 1.0946 - val_accuracy: 0.5139\n",
      "Epoch 32/80\n",
      "36/36 [==============================] - 0s 6ms/step - loss: 0.4318 - accuracy: 0.8415 - val_loss: 1.1030 - val_accuracy: 0.5139\n",
      "Epoch 33/80\n",
      "36/36 [==============================] - 0s 7ms/step - loss: 0.4327 - accuracy: 0.8449 - val_loss: 1.1143 - val_accuracy: 0.5208\n",
      "Epoch 34/80\n",
      "36/36 [==============================] - 0s 7ms/step - loss: 0.4707 - accuracy: 0.8328 - val_loss: 1.1413 - val_accuracy: 0.5139\n",
      "Epoch 35/80\n",
      "36/36 [==============================] - 0s 7ms/step - loss: 0.4187 - accuracy: 0.8554 - val_loss: 1.1367 - val_accuracy: 0.5139\n",
      "Epoch 36/80\n",
      "36/36 [==============================] - 0s 6ms/step - loss: 0.4156 - accuracy: 0.8711 - val_loss: 1.1599 - val_accuracy: 0.5139\n",
      "Epoch 37/80\n",
      "36/36 [==============================] - 0s 6ms/step - loss: 0.3959 - accuracy: 0.8432 - val_loss: 1.1525 - val_accuracy: 0.5139\n",
      "Epoch 38/80\n",
      "36/36 [==============================] - 0s 7ms/step - loss: 0.3580 - accuracy: 0.8850 - val_loss: 1.1687 - val_accuracy: 0.5278\n",
      "Epoch 39/80\n",
      "36/36 [==============================] - 0s 7ms/step - loss: 0.3710 - accuracy: 0.8693 - val_loss: 1.1828 - val_accuracy: 0.5139\n",
      "Epoch 40/80\n",
      "36/36 [==============================] - 0s 10ms/step - loss: 0.3558 - accuracy: 0.8798 - val_loss: 1.2053 - val_accuracy: 0.5208\n",
      "Epoch 41/80\n",
      "36/36 [==============================] - 0s 7ms/step - loss: 0.3187 - accuracy: 0.8972 - val_loss: 1.2223 - val_accuracy: 0.5347\n",
      "Epoch 42/80\n",
      "36/36 [==============================] - 0s 7ms/step - loss: 0.3303 - accuracy: 0.8885 - val_loss: 1.2451 - val_accuracy: 0.5139\n",
      "Epoch 43/80\n",
      "36/36 [==============================] - 0s 6ms/step - loss: 0.3167 - accuracy: 0.8850 - val_loss: 1.2102 - val_accuracy: 0.5417\n",
      "Epoch 44/80\n",
      "36/36 [==============================] - 0s 6ms/step - loss: 0.2986 - accuracy: 0.9129 - val_loss: 1.2317 - val_accuracy: 0.5139\n",
      "Epoch 45/80\n",
      "36/36 [==============================] - 0s 6ms/step - loss: 0.2956 - accuracy: 0.8972 - val_loss: 1.2535 - val_accuracy: 0.5139\n",
      "Epoch 46/80\n",
      "36/36 [==============================] - 0s 8ms/step - loss: 0.2733 - accuracy: 0.9251 - val_loss: 1.2610 - val_accuracy: 0.5208\n",
      "Epoch 47/80\n",
      "36/36 [==============================] - 0s 7ms/step - loss: 0.2665 - accuracy: 0.9181 - val_loss: 1.2686 - val_accuracy: 0.5208\n",
      "Epoch 48/80\n",
      "36/36 [==============================] - 0s 7ms/step - loss: 0.2617 - accuracy: 0.9233 - val_loss: 1.2881 - val_accuracy: 0.5278\n",
      "Epoch 49/80\n",
      "36/36 [==============================] - 0s 10ms/step - loss: 0.2565 - accuracy: 0.9181 - val_loss: 1.2827 - val_accuracy: 0.5347\n",
      "Epoch 50/80\n",
      "36/36 [==============================] - 0s 7ms/step - loss: 0.2493 - accuracy: 0.9303 - val_loss: 1.3188 - val_accuracy: 0.5417\n",
      "Epoch 51/80\n",
      "36/36 [==============================] - 0s 5ms/step - loss: 0.2259 - accuracy: 0.9199 - val_loss: 1.3386 - val_accuracy: 0.5347\n",
      "Epoch 52/80\n",
      "36/36 [==============================] - 0s 6ms/step - loss: 0.2155 - accuracy: 0.9338 - val_loss: 1.3453 - val_accuracy: 0.5139\n",
      "Epoch 53/80\n",
      "36/36 [==============================] - 0s 6ms/step - loss: 0.2085 - accuracy: 0.9338 - val_loss: 1.3747 - val_accuracy: 0.5069\n",
      "Epoch 54/80\n",
      "36/36 [==============================] - 0s 7ms/step - loss: 0.2231 - accuracy: 0.9286 - val_loss: 1.3655 - val_accuracy: 0.5347\n",
      "Epoch 55/80\n",
      "36/36 [==============================] - 0s 6ms/step - loss: 0.1831 - accuracy: 0.9599 - val_loss: 1.3685 - val_accuracy: 0.5556\n",
      "Epoch 56/80\n",
      "36/36 [==============================] - 0s 6ms/step - loss: 0.1974 - accuracy: 0.9408 - val_loss: 1.4047 - val_accuracy: 0.5417\n",
      "Epoch 57/80\n",
      "36/36 [==============================] - 0s 6ms/step - loss: 0.1867 - accuracy: 0.9512 - val_loss: 1.3948 - val_accuracy: 0.5278\n",
      "Epoch 58/80\n",
      "36/36 [==============================] - 0s 6ms/step - loss: 0.1664 - accuracy: 0.9564 - val_loss: 1.4001 - val_accuracy: 0.5486\n",
      "Epoch 59/80\n",
      "36/36 [==============================] - 0s 7ms/step - loss: 0.1682 - accuracy: 0.9443 - val_loss: 1.4305 - val_accuracy: 0.5417\n",
      "Epoch 60/80\n",
      "36/36 [==============================] - 0s 9ms/step - loss: 0.1701 - accuracy: 0.9582 - val_loss: 1.4625 - val_accuracy: 0.5556\n",
      "Epoch 61/80\n",
      "36/36 [==============================] - 0s 7ms/step - loss: 0.1572 - accuracy: 0.9547 - val_loss: 1.4644 - val_accuracy: 0.5486\n",
      "Epoch 62/80\n",
      "36/36 [==============================] - 0s 6ms/step - loss: 0.1625 - accuracy: 0.9547 - val_loss: 1.4976 - val_accuracy: 0.5417\n",
      "Epoch 63/80\n",
      "36/36 [==============================] - 0s 7ms/step - loss: 0.1364 - accuracy: 0.9652 - val_loss: 1.5238 - val_accuracy: 0.5417\n",
      "Epoch 64/80\n",
      "36/36 [==============================] - 0s 6ms/step - loss: 0.1237 - accuracy: 0.9721 - val_loss: 1.5271 - val_accuracy: 0.5625\n",
      "Epoch 65/80\n",
      "36/36 [==============================] - 0s 6ms/step - loss: 0.1444 - accuracy: 0.9547 - val_loss: 1.5499 - val_accuracy: 0.5556\n",
      "Epoch 66/80\n",
      "36/36 [==============================] - 0s 7ms/step - loss: 0.1509 - accuracy: 0.9530 - val_loss: 1.5459 - val_accuracy: 0.5486\n",
      "Epoch 67/80\n",
      "36/36 [==============================] - 0s 6ms/step - loss: 0.1313 - accuracy: 0.9634 - val_loss: 1.5744 - val_accuracy: 0.5347\n",
      "Epoch 68/80\n",
      "36/36 [==============================] - 0s 6ms/step - loss: 0.1068 - accuracy: 0.9843 - val_loss: 1.6171 - val_accuracy: 0.5417\n",
      "Epoch 69/80\n",
      "36/36 [==============================] - 0s 7ms/step - loss: 0.0936 - accuracy: 0.9948 - val_loss: 1.6379 - val_accuracy: 0.5486\n",
      "Epoch 70/80\n",
      "36/36 [==============================] - 0s 9ms/step - loss: 0.0980 - accuracy: 0.9808 - val_loss: 1.6417 - val_accuracy: 0.5764\n",
      "Epoch 71/80\n",
      "36/36 [==============================] - 0s 7ms/step - loss: 0.1035 - accuracy: 0.9791 - val_loss: 1.6606 - val_accuracy: 0.5694\n",
      "Epoch 72/80\n",
      "36/36 [==============================] - 0s 6ms/step - loss: 0.1018 - accuracy: 0.9774 - val_loss: 1.6918 - val_accuracy: 0.5486\n",
      "Epoch 73/80\n",
      "36/36 [==============================] - 0s 5ms/step - loss: 0.1102 - accuracy: 0.9652 - val_loss: 1.7446 - val_accuracy: 0.5417\n",
      "Epoch 74/80\n",
      "36/36 [==============================] - 0s 6ms/step - loss: 0.1123 - accuracy: 0.9686 - val_loss: 1.7033 - val_accuracy: 0.5556\n",
      "Epoch 75/80\n",
      "36/36 [==============================] - 0s 7ms/step - loss: 0.0902 - accuracy: 0.9808 - val_loss: 1.7317 - val_accuracy: 0.5486\n",
      "Epoch 76/80\n",
      "36/36 [==============================] - 0s 6ms/step - loss: 0.0746 - accuracy: 0.9843 - val_loss: 1.7515 - val_accuracy: 0.5486\n",
      "Epoch 77/80\n",
      "36/36 [==============================] - 0s 6ms/step - loss: 0.0857 - accuracy: 0.9861 - val_loss: 1.7750 - val_accuracy: 0.5486\n",
      "Epoch 78/80\n",
      "36/36 [==============================] - 0s 6ms/step - loss: 0.0735 - accuracy: 0.9861 - val_loss: 1.7414 - val_accuracy: 0.5764\n",
      "Epoch 79/80\n",
      "36/36 [==============================] - 0s 10ms/step - loss: 0.0785 - accuracy: 0.9826 - val_loss: 1.7619 - val_accuracy: 0.5556\n",
      "Epoch 80/80\n",
      "36/36 [==============================] - 0s 6ms/step - loss: 0.0725 - accuracy: 0.9808 - val_loss: 1.7817 - val_accuracy: 0.5625\n"
     ]
    }
   ],
   "source": [
    "history2 = model2.fit(X1_train_scaled, y1_train, \n",
    "                      validation_data=(X1_val_scaled, y1_val), \n",
    "                      batch_size=16, \n",
    "                      epochs=80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "0d347faf",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"Conv1D_3\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " Conv1D_1 (Conv1D)           (None, 57, 16)            96        \n",
      "                                                                 \n",
      " BN1 (BatchNormalization)    (None, 57, 16)            64        \n",
      "                                                                 \n",
      " MaxPooling1 (MaxPooling1D)  (None, 28, 16)            0         \n",
      "                                                                 \n",
      " Conv1D_2 (Conv1D)           (None, 28, 32)            3616      \n",
      "                                                                 \n",
      " BN2 (BatchNormalization)    (None, 28, 32)            128       \n",
      "                                                                 \n",
      " MaxPooling2 (MaxPooling1D)  (None, 14, 32)            0         \n",
      "                                                                 \n",
      " Conv1D_3 (Conv1D)           (None, 14, 32)            7200      \n",
      "                                                                 \n",
      " BN3 (BatchNormalization)    (None, 14, 32)            128       \n",
      "                                                                 \n",
      " MaxPooling3 (MaxPooling1D)  (None, 7, 32)             0         \n",
      "                                                                 \n",
      " Flatten (Flatten)           (None, 224)               0         \n",
      "                                                                 \n",
      " Dense_1 (Dense)             (None, 128)               28800     \n",
      "                                                                 \n",
      " Dropout_1 (Dropout)         (None, 128)               0         \n",
      "                                                                 \n",
      " Dense_2 (Dense)             (None, 64)                8256      \n",
      "                                                                 \n",
      " Dropout_2 (Dropout)         (None, 64)                0         \n",
      "                                                                 \n",
      " Softmax (Dense)             (None, 4)                 260       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 48,548\n",
      "Trainable params: 48,388\n",
      "Non-trainable params: 160\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "f1,k1,a1 = 16,5,'relu'\n",
    "f2,k2,a2 = 32,7,'relu'\n",
    "f3,k3,a3 = 32,7,'relu'\n",
    "d1,dr1,da1,r1 = 128,0.2,'relu',0.2\n",
    "d2,dr2,da2,r2 = 64,0.2,'relu',0.2\n",
    "num = 3\n",
    "\n",
    "model3 = modelBuilder3L(X1_train_scaled,\n",
    "                        f1,k1,a1,\n",
    "                        f2,k2,a2,\n",
    "                        f3,k3,a3,\n",
    "                        d1,dr1,da1,r1,\n",
    "                        d2,dr2,da2,r2,\n",
    "                        num)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "e7d27032",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/80\n",
      "36/36 [==============================] - 3s 16ms/step - loss: 1.8055 - accuracy: 0.2596 - val_loss: 1.3851 - val_accuracy: 0.3333\n",
      "Epoch 2/80\n",
      "36/36 [==============================] - 0s 6ms/step - loss: 1.3655 - accuracy: 0.3746 - val_loss: 1.3846 - val_accuracy: 0.2014\n",
      "Epoch 3/80\n",
      "36/36 [==============================] - 0s 6ms/step - loss: 1.1497 - accuracy: 0.4652 - val_loss: 1.3803 - val_accuracy: 0.2222\n",
      "Epoch 4/80\n",
      "36/36 [==============================] - 0s 7ms/step - loss: 1.1551 - accuracy: 0.4652 - val_loss: 1.3721 - val_accuracy: 0.2083\n",
      "Epoch 5/80\n",
      "36/36 [==============================] - 0s 6ms/step - loss: 1.0796 - accuracy: 0.5105 - val_loss: 1.3636 - val_accuracy: 0.2153\n",
      "Epoch 6/80\n",
      "36/36 [==============================] - 0s 7ms/step - loss: 1.0131 - accuracy: 0.5383 - val_loss: 1.3508 - val_accuracy: 0.2222\n",
      "Epoch 7/80\n",
      "36/36 [==============================] - 0s 6ms/step - loss: 0.9589 - accuracy: 0.5679 - val_loss: 1.3353 - val_accuracy: 0.2500\n",
      "Epoch 8/80\n",
      "36/36 [==============================] - 0s 6ms/step - loss: 0.9247 - accuracy: 0.5976 - val_loss: 1.3118 - val_accuracy: 0.3056\n",
      "Epoch 9/80\n",
      "36/36 [==============================] - 0s 6ms/step - loss: 0.9384 - accuracy: 0.5906 - val_loss: 1.2828 - val_accuracy: 0.3333\n",
      "Epoch 10/80\n",
      "36/36 [==============================] - 0s 6ms/step - loss: 0.8920 - accuracy: 0.6185 - val_loss: 1.2448 - val_accuracy: 0.3889\n",
      "Epoch 11/80\n",
      "36/36 [==============================] - 0s 6ms/step - loss: 0.8610 - accuracy: 0.6289 - val_loss: 1.2040 - val_accuracy: 0.4375\n",
      "Epoch 12/80\n",
      "36/36 [==============================] - 0s 11ms/step - loss: 0.8072 - accuracy: 0.6603 - val_loss: 1.1647 - val_accuracy: 0.5000\n",
      "Epoch 13/80\n",
      "36/36 [==============================] - 0s 6ms/step - loss: 0.7950 - accuracy: 0.6551 - val_loss: 1.1194 - val_accuracy: 0.4861\n",
      "Epoch 14/80\n",
      "36/36 [==============================] - 0s 6ms/step - loss: 0.8113 - accuracy: 0.6498 - val_loss: 1.0765 - val_accuracy: 0.5417\n",
      "Epoch 15/80\n",
      "36/36 [==============================] - 0s 7ms/step - loss: 0.7851 - accuracy: 0.6847 - val_loss: 1.0290 - val_accuracy: 0.5556\n",
      "Epoch 16/80\n",
      "36/36 [==============================] - 0s 6ms/step - loss: 0.7366 - accuracy: 0.7003 - val_loss: 0.9917 - val_accuracy: 0.5764\n",
      "Epoch 17/80\n",
      "36/36 [==============================] - 0s 7ms/step - loss: 0.7222 - accuracy: 0.7195 - val_loss: 0.9758 - val_accuracy: 0.5903\n",
      "Epoch 18/80\n",
      "36/36 [==============================] - 0s 6ms/step - loss: 0.7145 - accuracy: 0.6951 - val_loss: 0.9555 - val_accuracy: 0.5833\n",
      "Epoch 19/80\n",
      "36/36 [==============================] - 0s 6ms/step - loss: 0.6435 - accuracy: 0.7439 - val_loss: 0.9396 - val_accuracy: 0.5972\n",
      "Epoch 20/80\n",
      "36/36 [==============================] - 0s 6ms/step - loss: 0.6251 - accuracy: 0.7474 - val_loss: 0.9347 - val_accuracy: 0.6042\n",
      "Epoch 21/80\n",
      "36/36 [==============================] - 0s 6ms/step - loss: 0.6565 - accuracy: 0.7213 - val_loss: 0.9316 - val_accuracy: 0.5972\n",
      "Epoch 22/80\n",
      "36/36 [==============================] - 0s 6ms/step - loss: 0.6285 - accuracy: 0.7282 - val_loss: 0.9369 - val_accuracy: 0.5903\n",
      "Epoch 23/80\n",
      "36/36 [==============================] - 0s 6ms/step - loss: 0.5850 - accuracy: 0.7683 - val_loss: 0.9414 - val_accuracy: 0.5972\n",
      "Epoch 24/80\n",
      "36/36 [==============================] - 0s 6ms/step - loss: 0.6001 - accuracy: 0.7787 - val_loss: 0.9306 - val_accuracy: 0.5972\n",
      "Epoch 25/80\n",
      "36/36 [==============================] - 0s 6ms/step - loss: 0.5812 - accuracy: 0.7648 - val_loss: 0.9503 - val_accuracy: 0.5972\n",
      "Epoch 26/80\n",
      "36/36 [==============================] - 0s 6ms/step - loss: 0.5612 - accuracy: 0.7805 - val_loss: 0.9558 - val_accuracy: 0.5903\n",
      "Epoch 27/80\n",
      "36/36 [==============================] - 0s 9ms/step - loss: 0.5367 - accuracy: 0.8014 - val_loss: 0.9617 - val_accuracy: 0.5972\n",
      "Epoch 28/80\n",
      "36/36 [==============================] - 0s 6ms/step - loss: 0.5256 - accuracy: 0.8171 - val_loss: 0.9671 - val_accuracy: 0.5833\n",
      "Epoch 29/80\n",
      "36/36 [==============================] - 0s 6ms/step - loss: 0.5353 - accuracy: 0.8118 - val_loss: 0.9826 - val_accuracy: 0.6111\n",
      "Epoch 30/80\n",
      "36/36 [==============================] - 0s 6ms/step - loss: 0.4815 - accuracy: 0.8345 - val_loss: 1.0010 - val_accuracy: 0.5972\n",
      "Epoch 31/80\n",
      "36/36 [==============================] - 0s 6ms/step - loss: 0.4692 - accuracy: 0.8397 - val_loss: 0.9945 - val_accuracy: 0.6042\n",
      "Epoch 32/80\n",
      "36/36 [==============================] - 0s 6ms/step - loss: 0.4543 - accuracy: 0.8502 - val_loss: 1.0112 - val_accuracy: 0.5903\n",
      "Epoch 33/80\n",
      "36/36 [==============================] - 0s 7ms/step - loss: 0.4762 - accuracy: 0.8345 - val_loss: 1.0253 - val_accuracy: 0.5694\n",
      "Epoch 34/80\n",
      "36/36 [==============================] - 0s 6ms/step - loss: 0.4281 - accuracy: 0.8519 - val_loss: 1.0252 - val_accuracy: 0.5694\n",
      "Epoch 35/80\n",
      "36/36 [==============================] - 0s 6ms/step - loss: 0.4214 - accuracy: 0.8380 - val_loss: 1.0380 - val_accuracy: 0.5694\n",
      "Epoch 36/80\n",
      "36/36 [==============================] - 0s 6ms/step - loss: 0.4092 - accuracy: 0.8624 - val_loss: 1.0478 - val_accuracy: 0.5625\n",
      "Epoch 37/80\n",
      "36/36 [==============================] - 0s 8ms/step - loss: 0.3714 - accuracy: 0.8868 - val_loss: 1.0589 - val_accuracy: 0.5486\n",
      "Epoch 38/80\n",
      "36/36 [==============================] - 0s 6ms/step - loss: 0.3858 - accuracy: 0.8780 - val_loss: 1.0795 - val_accuracy: 0.5486\n",
      "Epoch 39/80\n",
      "36/36 [==============================] - 0s 6ms/step - loss: 0.3575 - accuracy: 0.8868 - val_loss: 1.0883 - val_accuracy: 0.5694\n",
      "Epoch 40/80\n",
      "36/36 [==============================] - 0s 7ms/step - loss: 0.3533 - accuracy: 0.8763 - val_loss: 1.1045 - val_accuracy: 0.5556\n",
      "Epoch 41/80\n",
      "36/36 [==============================] - 0s 11ms/step - loss: 0.3276 - accuracy: 0.8955 - val_loss: 1.1091 - val_accuracy: 0.5556\n",
      "Epoch 42/80\n",
      "36/36 [==============================] - 0s 6ms/step - loss: 0.3338 - accuracy: 0.8746 - val_loss: 1.1207 - val_accuracy: 0.5694\n",
      "Epoch 43/80\n",
      "36/36 [==============================] - 0s 6ms/step - loss: 0.3243 - accuracy: 0.8798 - val_loss: 1.1235 - val_accuracy: 0.5486\n",
      "Epoch 44/80\n",
      "36/36 [==============================] - 0s 6ms/step - loss: 0.2930 - accuracy: 0.9059 - val_loss: 1.1455 - val_accuracy: 0.5486\n",
      "Epoch 45/80\n",
      "36/36 [==============================] - 0s 7ms/step - loss: 0.2990 - accuracy: 0.9024 - val_loss: 1.1411 - val_accuracy: 0.5556\n",
      "Epoch 46/80\n",
      "36/36 [==============================] - 0s 7ms/step - loss: 0.2888 - accuracy: 0.9007 - val_loss: 1.1574 - val_accuracy: 0.5347\n",
      "Epoch 47/80\n",
      "36/36 [==============================] - 0s 6ms/step - loss: 0.2881 - accuracy: 0.8990 - val_loss: 1.1605 - val_accuracy: 0.5486\n",
      "Epoch 48/80\n",
      "36/36 [==============================] - 0s 7ms/step - loss: 0.2726 - accuracy: 0.9129 - val_loss: 1.1902 - val_accuracy: 0.5417\n",
      "Epoch 49/80\n",
      "36/36 [==============================] - 0s 7ms/step - loss: 0.2495 - accuracy: 0.9216 - val_loss: 1.1970 - val_accuracy: 0.5347\n",
      "Epoch 50/80\n",
      "36/36 [==============================] - 0s 7ms/step - loss: 0.2699 - accuracy: 0.9094 - val_loss: 1.2167 - val_accuracy: 0.5347\n",
      "Epoch 51/80\n",
      "36/36 [==============================] - 0s 6ms/step - loss: 0.2585 - accuracy: 0.9233 - val_loss: 1.2339 - val_accuracy: 0.5486\n",
      "Epoch 52/80\n",
      "36/36 [==============================] - 0s 11ms/step - loss: 0.2264 - accuracy: 0.9233 - val_loss: 1.2559 - val_accuracy: 0.5347\n",
      "Epoch 53/80\n",
      "36/36 [==============================] - 0s 6ms/step - loss: 0.2122 - accuracy: 0.9390 - val_loss: 1.2729 - val_accuracy: 0.5347\n",
      "Epoch 54/80\n",
      "36/36 [==============================] - 0s 6ms/step - loss: 0.1827 - accuracy: 0.9512 - val_loss: 1.2995 - val_accuracy: 0.5347\n",
      "Epoch 55/80\n",
      "36/36 [==============================] - 0s 7ms/step - loss: 0.1891 - accuracy: 0.9355 - val_loss: 1.3077 - val_accuracy: 0.5347\n",
      "Epoch 56/80\n",
      "36/36 [==============================] - 0s 6ms/step - loss: 0.1988 - accuracy: 0.9390 - val_loss: 1.3324 - val_accuracy: 0.5069\n",
      "Epoch 57/80\n",
      "36/36 [==============================] - 0s 6ms/step - loss: 0.1920 - accuracy: 0.9390 - val_loss: 1.3649 - val_accuracy: 0.5278\n",
      "Epoch 58/80\n",
      "36/36 [==============================] - 0s 6ms/step - loss: 0.1900 - accuracy: 0.9477 - val_loss: 1.3905 - val_accuracy: 0.5347\n",
      "Epoch 59/80\n",
      "36/36 [==============================] - 0s 6ms/step - loss: 0.1865 - accuracy: 0.9443 - val_loss: 1.3966 - val_accuracy: 0.5347\n",
      "Epoch 60/80\n",
      "36/36 [==============================] - 0s 6ms/step - loss: 0.1568 - accuracy: 0.9599 - val_loss: 1.4169 - val_accuracy: 0.5417\n",
      "Epoch 61/80\n",
      "36/36 [==============================] - 0s 6ms/step - loss: 0.1601 - accuracy: 0.9617 - val_loss: 1.4173 - val_accuracy: 0.5486\n",
      "Epoch 62/80\n",
      "36/36 [==============================] - 0s 6ms/step - loss: 0.1564 - accuracy: 0.9530 - val_loss: 1.4456 - val_accuracy: 0.5417\n",
      "Epoch 63/80\n",
      "36/36 [==============================] - 0s 9ms/step - loss: 0.1417 - accuracy: 0.9652 - val_loss: 1.4465 - val_accuracy: 0.5139\n",
      "Epoch 64/80\n",
      "36/36 [==============================] - 0s 6ms/step - loss: 0.1308 - accuracy: 0.9704 - val_loss: 1.4691 - val_accuracy: 0.5278\n",
      "Epoch 65/80\n",
      "36/36 [==============================] - 0s 6ms/step - loss: 0.1316 - accuracy: 0.9721 - val_loss: 1.4894 - val_accuracy: 0.5208\n",
      "Epoch 66/80\n",
      "36/36 [==============================] - 0s 6ms/step - loss: 0.1167 - accuracy: 0.9843 - val_loss: 1.5017 - val_accuracy: 0.5139\n",
      "Epoch 67/80\n",
      "36/36 [==============================] - 0s 6ms/step - loss: 0.1178 - accuracy: 0.9739 - val_loss: 1.5107 - val_accuracy: 0.5278\n",
      "Epoch 68/80\n",
      "36/36 [==============================] - 0s 6ms/step - loss: 0.1216 - accuracy: 0.9686 - val_loss: 1.5585 - val_accuracy: 0.5278\n",
      "Epoch 69/80\n",
      "36/36 [==============================] - 0s 7ms/step - loss: 0.1083 - accuracy: 0.9774 - val_loss: 1.5560 - val_accuracy: 0.5139\n",
      "Epoch 70/80\n",
      "36/36 [==============================] - 0s 7ms/step - loss: 0.1084 - accuracy: 0.9669 - val_loss: 1.5931 - val_accuracy: 0.5139\n",
      "Epoch 71/80\n",
      "36/36 [==============================] - 0s 6ms/step - loss: 0.1107 - accuracy: 0.9669 - val_loss: 1.6182 - val_accuracy: 0.5278\n",
      "Epoch 72/80\n",
      "36/36 [==============================] - 0s 8ms/step - loss: 0.1124 - accuracy: 0.9721 - val_loss: 1.6515 - val_accuracy: 0.5069\n",
      "Epoch 73/80\n",
      "36/36 [==============================] - 0s 7ms/step - loss: 0.0930 - accuracy: 0.9843 - val_loss: 1.6351 - val_accuracy: 0.5069\n",
      "Epoch 74/80\n",
      "36/36 [==============================] - 0s 7ms/step - loss: 0.0976 - accuracy: 0.9686 - val_loss: 1.6804 - val_accuracy: 0.5069\n",
      "Epoch 75/80\n",
      "36/36 [==============================] - 0s 5ms/step - loss: 0.0898 - accuracy: 0.9774 - val_loss: 1.6778 - val_accuracy: 0.5486\n",
      "Epoch 76/80\n",
      "36/36 [==============================] - 0s 6ms/step - loss: 0.0771 - accuracy: 0.9861 - val_loss: 1.7574 - val_accuracy: 0.5208\n",
      "Epoch 77/80\n",
      "36/36 [==============================] - 0s 7ms/step - loss: 0.0955 - accuracy: 0.9686 - val_loss: 1.7343 - val_accuracy: 0.5000\n",
      "Epoch 78/80\n",
      "36/36 [==============================] - 0s 6ms/step - loss: 0.0986 - accuracy: 0.9756 - val_loss: 1.7712 - val_accuracy: 0.5347\n",
      "Epoch 79/80\n",
      "36/36 [==============================] - 0s 6ms/step - loss: 0.0805 - accuracy: 0.9843 - val_loss: 1.7531 - val_accuracy: 0.5347\n",
      "Epoch 80/80\n",
      "36/36 [==============================] - 0s 6ms/step - loss: 0.0856 - accuracy: 0.9791 - val_loss: 1.7715 - val_accuracy: 0.5208\n"
     ]
    }
   ],
   "source": [
    "history3 = model3.fit(X1_train_scaled, y1_train, \n",
    "                      validation_data=(X1_val_scaled, y1_val), \n",
    "                      batch_size=16, \n",
    "                      epochs=80)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4568ba75",
   "metadata": {},
   "source": [
    "Adding regularization parameters almost does not improve the validation accuracy very well."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f991714c",
   "metadata": {},
   "source": [
    "**Increase the dropout parameter value**<br>\n",
    "This only slightly improve the validation accuracy and damage the training accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "e7deb22f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"Conv1D_4\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " Conv1D_1 (Conv1D)           (None, 57, 16)            96        \n",
      "                                                                 \n",
      " BN1 (BatchNormalization)    (None, 57, 16)            64        \n",
      "                                                                 \n",
      " MaxPooling1 (MaxPooling1D)  (None, 28, 16)            0         \n",
      "                                                                 \n",
      " Conv1D_2 (Conv1D)           (None, 28, 32)            3616      \n",
      "                                                                 \n",
      " BN2 (BatchNormalization)    (None, 28, 32)            128       \n",
      "                                                                 \n",
      " MaxPooling2 (MaxPooling1D)  (None, 14, 32)            0         \n",
      "                                                                 \n",
      " Conv1D_3 (Conv1D)           (None, 14, 32)            7200      \n",
      "                                                                 \n",
      " BN3 (BatchNormalization)    (None, 14, 32)            128       \n",
      "                                                                 \n",
      " MaxPooling3 (MaxPooling1D)  (None, 7, 32)             0         \n",
      "                                                                 \n",
      " Flatten (Flatten)           (None, 224)               0         \n",
      "                                                                 \n",
      " Dense_1 (Dense)             (None, 128)               28800     \n",
      "                                                                 \n",
      " Dropout_1 (Dropout)         (None, 128)               0         \n",
      "                                                                 \n",
      " Dense_2 (Dense)             (None, 64)                8256      \n",
      "                                                                 \n",
      " Dropout_2 (Dropout)         (None, 64)                0         \n",
      "                                                                 \n",
      " Softmax (Dense)             (None, 4)                 260       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 48,548\n",
      "Trainable params: 48,388\n",
      "Non-trainable params: 160\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "f1,k1,a1 = 16,5,'relu'\n",
    "f2,k2,a2 = 32,7,'relu'\n",
    "f3,k3,a3 = 32,7,'relu'\n",
    "d1,dr1,da1,r1 = 128,0.3,'relu',0.15\n",
    "d2,dr2,da2,r2 = 64,0.3,'relu',0.15\n",
    "num = 4\n",
    "\n",
    "model4 = modelBuilder3L(X1_train_scaled,\n",
    "                        f1,k1,a1,\n",
    "                        f2,k2,a2,\n",
    "                        f3,k3,a3,\n",
    "                        d1,dr1,da1,r1,\n",
    "                        d2,dr2,da2,r2,\n",
    "                        num)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "a7435da3",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/80\n",
      "36/36 [==============================] - 4s 16ms/step - loss: 2.0268 - accuracy: 0.2561 - val_loss: 1.3901 - val_accuracy: 0.2986\n",
      "Epoch 2/80\n",
      "36/36 [==============================] - 0s 7ms/step - loss: 1.5235 - accuracy: 0.3728 - val_loss: 1.3923 - val_accuracy: 0.2500\n",
      "Epoch 3/80\n",
      "36/36 [==============================] - 0s 6ms/step - loss: 1.3968 - accuracy: 0.3571 - val_loss: 1.4080 - val_accuracy: 0.2500\n",
      "Epoch 4/80\n",
      "36/36 [==============================] - 0s 7ms/step - loss: 1.3187 - accuracy: 0.4303 - val_loss: 1.4344 - val_accuracy: 0.2500\n",
      "Epoch 5/80\n",
      "36/36 [==============================] - 0s 8ms/step - loss: 1.1663 - accuracy: 0.4843 - val_loss: 1.4730 - val_accuracy: 0.2500\n",
      "Epoch 6/80\n",
      "36/36 [==============================] - 0s 6ms/step - loss: 1.2173 - accuracy: 0.4582 - val_loss: 1.5131 - val_accuracy: 0.2500\n",
      "Epoch 7/80\n",
      "36/36 [==============================] - 0s 7ms/step - loss: 1.1272 - accuracy: 0.5192 - val_loss: 1.5368 - val_accuracy: 0.2569\n",
      "Epoch 8/80\n",
      "36/36 [==============================] - 0s 6ms/step - loss: 1.0961 - accuracy: 0.5226 - val_loss: 1.5386 - val_accuracy: 0.2708\n",
      "Epoch 9/80\n",
      "36/36 [==============================] - 0s 6ms/step - loss: 1.0737 - accuracy: 0.5540 - val_loss: 1.5080 - val_accuracy: 0.2847\n",
      "Epoch 10/80\n",
      "36/36 [==============================] - 0s 7ms/step - loss: 0.9630 - accuracy: 0.5732 - val_loss: 1.4513 - val_accuracy: 0.3056\n",
      "Epoch 11/80\n",
      "36/36 [==============================] - 0s 6ms/step - loss: 0.9782 - accuracy: 0.5976 - val_loss: 1.3800 - val_accuracy: 0.3264\n",
      "Epoch 12/80\n",
      "36/36 [==============================] - 0s 7ms/step - loss: 0.9621 - accuracy: 0.5627 - val_loss: 1.3077 - val_accuracy: 0.3681\n",
      "Epoch 13/80\n",
      "36/36 [==============================] - 0s 6ms/step - loss: 0.9278 - accuracy: 0.6010 - val_loss: 1.2494 - val_accuracy: 0.4028\n",
      "Epoch 14/80\n",
      "36/36 [==============================] - 0s 9ms/step - loss: 0.8818 - accuracy: 0.6167 - val_loss: 1.1782 - val_accuracy: 0.4514\n",
      "Epoch 15/80\n",
      "36/36 [==============================] - 0s 7ms/step - loss: 0.8930 - accuracy: 0.6185 - val_loss: 1.1356 - val_accuracy: 0.5069\n",
      "Epoch 16/80\n",
      "36/36 [==============================] - 0s 6ms/step - loss: 0.8864 - accuracy: 0.6150 - val_loss: 1.1067 - val_accuracy: 0.5139\n",
      "Epoch 17/80\n",
      "36/36 [==============================] - 0s 7ms/step - loss: 0.8702 - accuracy: 0.6080 - val_loss: 1.0855 - val_accuracy: 0.5139\n",
      "Epoch 18/80\n",
      "36/36 [==============================] - 0s 6ms/step - loss: 0.8346 - accuracy: 0.6446 - val_loss: 1.0712 - val_accuracy: 0.5139\n",
      "Epoch 19/80\n",
      "36/36 [==============================] - 0s 9ms/step - loss: 0.8060 - accuracy: 0.6516 - val_loss: 1.0551 - val_accuracy: 0.5208\n",
      "Epoch 20/80\n",
      "36/36 [==============================] - 0s 7ms/step - loss: 0.8054 - accuracy: 0.6341 - val_loss: 1.0419 - val_accuracy: 0.5069\n",
      "Epoch 21/80\n",
      "36/36 [==============================] - 0s 6ms/step - loss: 0.8071 - accuracy: 0.6429 - val_loss: 1.0438 - val_accuracy: 0.5139\n",
      "Epoch 22/80\n",
      "36/36 [==============================] - 0s 6ms/step - loss: 0.7239 - accuracy: 0.6864 - val_loss: 1.0402 - val_accuracy: 0.5139\n",
      "Epoch 23/80\n",
      "36/36 [==============================] - 0s 9ms/step - loss: 0.7317 - accuracy: 0.6829 - val_loss: 1.0464 - val_accuracy: 0.5278\n",
      "Epoch 24/80\n",
      "36/36 [==============================] - 0s 7ms/step - loss: 0.7233 - accuracy: 0.6829 - val_loss: 1.0457 - val_accuracy: 0.5417\n",
      "Epoch 25/80\n",
      "36/36 [==============================] - 0s 6ms/step - loss: 0.6896 - accuracy: 0.6986 - val_loss: 1.0423 - val_accuracy: 0.5625\n",
      "Epoch 26/80\n",
      "36/36 [==============================] - 0s 10ms/step - loss: 0.6964 - accuracy: 0.7282 - val_loss: 1.0485 - val_accuracy: 0.5556\n",
      "Epoch 27/80\n",
      "36/36 [==============================] - 0s 7ms/step - loss: 0.6451 - accuracy: 0.7387 - val_loss: 1.0580 - val_accuracy: 0.5486\n",
      "Epoch 28/80\n",
      "36/36 [==============================] - 0s 6ms/step - loss: 0.6579 - accuracy: 0.7230 - val_loss: 1.0580 - val_accuracy: 0.5486\n",
      "Epoch 29/80\n",
      "36/36 [==============================] - 0s 7ms/step - loss: 0.6553 - accuracy: 0.7178 - val_loss: 1.0750 - val_accuracy: 0.5417\n",
      "Epoch 30/80\n",
      "36/36 [==============================] - 0s 6ms/step - loss: 0.6275 - accuracy: 0.7456 - val_loss: 1.0934 - val_accuracy: 0.5833\n",
      "Epoch 31/80\n",
      "36/36 [==============================] - 0s 6ms/step - loss: 0.6289 - accuracy: 0.7561 - val_loss: 1.0808 - val_accuracy: 0.5625\n",
      "Epoch 32/80\n",
      "36/36 [==============================] - 0s 7ms/step - loss: 0.5870 - accuracy: 0.7857 - val_loss: 1.0740 - val_accuracy: 0.5764\n",
      "Epoch 33/80\n",
      "36/36 [==============================] - 0s 6ms/step - loss: 0.6025 - accuracy: 0.7456 - val_loss: 1.1011 - val_accuracy: 0.5764\n",
      "Epoch 34/80\n",
      "36/36 [==============================] - 0s 7ms/step - loss: 0.5791 - accuracy: 0.7700 - val_loss: 1.0948 - val_accuracy: 0.5972\n",
      "Epoch 35/80\n",
      "36/36 [==============================] - 0s 6ms/step - loss: 0.5434 - accuracy: 0.7979 - val_loss: 1.0879 - val_accuracy: 0.5764\n",
      "Epoch 36/80\n",
      "36/36 [==============================] - 0s 6ms/step - loss: 0.5911 - accuracy: 0.7578 - val_loss: 1.1047 - val_accuracy: 0.5903\n",
      "Epoch 37/80\n",
      "36/36 [==============================] - 0s 6ms/step - loss: 0.5283 - accuracy: 0.8031 - val_loss: 1.1042 - val_accuracy: 0.6111\n",
      "Epoch 38/80\n",
      "36/36 [==============================] - 0s 6ms/step - loss: 0.5154 - accuracy: 0.7997 - val_loss: 1.1175 - val_accuracy: 0.5903\n",
      "Epoch 39/80\n",
      "36/36 [==============================] - 0s 7ms/step - loss: 0.4828 - accuracy: 0.8328 - val_loss: 1.1216 - val_accuracy: 0.5903\n",
      "Epoch 40/80\n",
      "36/36 [==============================] - 0s 8ms/step - loss: 0.4733 - accuracy: 0.8066 - val_loss: 1.1859 - val_accuracy: 0.5694\n",
      "Epoch 41/80\n",
      "36/36 [==============================] - 0s 6ms/step - loss: 0.4685 - accuracy: 0.8101 - val_loss: 1.1520 - val_accuracy: 0.5972\n",
      "Epoch 42/80\n",
      "36/36 [==============================] - 0s 7ms/step - loss: 0.4763 - accuracy: 0.8240 - val_loss: 1.1626 - val_accuracy: 0.5972\n",
      "Epoch 43/80\n",
      "36/36 [==============================] - 0s 6ms/step - loss: 0.4646 - accuracy: 0.8293 - val_loss: 1.1695 - val_accuracy: 0.5903\n",
      "Epoch 44/80\n",
      "36/36 [==============================] - 0s 6ms/step - loss: 0.4539 - accuracy: 0.8240 - val_loss: 1.1823 - val_accuracy: 0.5764\n",
      "Epoch 45/80\n",
      "36/36 [==============================] - 0s 8ms/step - loss: 0.4393 - accuracy: 0.8345 - val_loss: 1.2152 - val_accuracy: 0.5833\n",
      "Epoch 46/80\n",
      "36/36 [==============================] - 0s 7ms/step - loss: 0.4269 - accuracy: 0.8519 - val_loss: 1.2425 - val_accuracy: 0.5625\n",
      "Epoch 47/80\n",
      "36/36 [==============================] - 0s 6ms/step - loss: 0.4224 - accuracy: 0.8397 - val_loss: 1.2586 - val_accuracy: 0.5694\n",
      "Epoch 48/80\n",
      "36/36 [==============================] - 0s 8ms/step - loss: 0.4089 - accuracy: 0.8502 - val_loss: 1.2547 - val_accuracy: 0.5556\n",
      "Epoch 49/80\n",
      "36/36 [==============================] - 0s 7ms/step - loss: 0.3990 - accuracy: 0.8449 - val_loss: 1.2552 - val_accuracy: 0.5833\n",
      "Epoch 50/80\n",
      "36/36 [==============================] - 0s 6ms/step - loss: 0.3944 - accuracy: 0.8554 - val_loss: 1.2635 - val_accuracy: 0.5833\n",
      "Epoch 51/80\n",
      "36/36 [==============================] - 0s 8ms/step - loss: 0.3530 - accuracy: 0.8937 - val_loss: 1.2736 - val_accuracy: 0.5833\n",
      "Epoch 52/80\n",
      "36/36 [==============================] - 0s 8ms/step - loss: 0.3773 - accuracy: 0.8624 - val_loss: 1.2523 - val_accuracy: 0.5625\n",
      "Epoch 53/80\n",
      "36/36 [==============================] - 0s 6ms/step - loss: 0.3551 - accuracy: 0.8763 - val_loss: 1.2804 - val_accuracy: 0.5625\n",
      "Epoch 54/80\n",
      "36/36 [==============================] - 0s 6ms/step - loss: 0.3323 - accuracy: 0.8920 - val_loss: 1.2960 - val_accuracy: 0.5625\n",
      "Epoch 55/80\n",
      "36/36 [==============================] - 0s 7ms/step - loss: 0.3349 - accuracy: 0.8798 - val_loss: 1.3035 - val_accuracy: 0.5486\n",
      "Epoch 56/80\n",
      "36/36 [==============================] - 0s 7ms/step - loss: 0.3250 - accuracy: 0.8798 - val_loss: 1.3165 - val_accuracy: 0.5417\n",
      "Epoch 57/80\n",
      "36/36 [==============================] - 0s 6ms/step - loss: 0.3595 - accuracy: 0.8589 - val_loss: 1.3172 - val_accuracy: 0.5486\n",
      "Epoch 58/80\n",
      "36/36 [==============================] - 0s 6ms/step - loss: 0.3205 - accuracy: 0.8815 - val_loss: 1.3299 - val_accuracy: 0.5556\n",
      "Epoch 59/80\n",
      "36/36 [==============================] - 0s 7ms/step - loss: 0.3122 - accuracy: 0.8815 - val_loss: 1.3438 - val_accuracy: 0.5417\n",
      "Epoch 60/80\n",
      "36/36 [==============================] - 0s 7ms/step - loss: 0.2725 - accuracy: 0.8990 - val_loss: 1.3676 - val_accuracy: 0.5625\n",
      "Epoch 61/80\n",
      "36/36 [==============================] - 0s 6ms/step - loss: 0.2839 - accuracy: 0.9129 - val_loss: 1.3702 - val_accuracy: 0.5556\n",
      "Epoch 62/80\n",
      "36/36 [==============================] - 0s 10ms/step - loss: 0.3014 - accuracy: 0.8920 - val_loss: 1.3949 - val_accuracy: 0.5694\n",
      "Epoch 63/80\n",
      "36/36 [==============================] - 0s 6ms/step - loss: 0.2598 - accuracy: 0.9059 - val_loss: 1.3986 - val_accuracy: 0.5625\n",
      "Epoch 64/80\n",
      "36/36 [==============================] - 0s 6ms/step - loss: 0.2920 - accuracy: 0.8972 - val_loss: 1.4264 - val_accuracy: 0.5556\n",
      "Epoch 65/80\n",
      "36/36 [==============================] - 0s 6ms/step - loss: 0.2488 - accuracy: 0.9129 - val_loss: 1.4370 - val_accuracy: 0.5556\n",
      "Epoch 66/80\n",
      "36/36 [==============================] - 0s 7ms/step - loss: 0.2513 - accuracy: 0.9129 - val_loss: 1.5158 - val_accuracy: 0.5556\n",
      "Epoch 67/80\n",
      "36/36 [==============================] - 0s 6ms/step - loss: 0.2530 - accuracy: 0.9146 - val_loss: 1.5216 - val_accuracy: 0.5417\n",
      "Epoch 68/80\n",
      "36/36 [==============================] - 0s 6ms/step - loss: 0.2462 - accuracy: 0.9286 - val_loss: 1.5259 - val_accuracy: 0.5625\n",
      "Epoch 69/80\n",
      "36/36 [==============================] - 0s 6ms/step - loss: 0.2443 - accuracy: 0.9129 - val_loss: 1.5087 - val_accuracy: 0.5556\n",
      "Epoch 70/80\n",
      "36/36 [==============================] - 0s 6ms/step - loss: 0.2263 - accuracy: 0.9199 - val_loss: 1.5292 - val_accuracy: 0.5347\n",
      "Epoch 71/80\n",
      "36/36 [==============================] - 0s 6ms/step - loss: 0.2159 - accuracy: 0.9286 - val_loss: 1.5715 - val_accuracy: 0.5347\n",
      "Epoch 72/80\n",
      "36/36 [==============================] - 0s 6ms/step - loss: 0.2133 - accuracy: 0.9355 - val_loss: 1.5834 - val_accuracy: 0.5417\n",
      "Epoch 73/80\n",
      "36/36 [==============================] - 0s 9ms/step - loss: 0.2068 - accuracy: 0.9355 - val_loss: 1.5613 - val_accuracy: 0.5556\n",
      "Epoch 74/80\n",
      "36/36 [==============================] - 0s 6ms/step - loss: 0.1936 - accuracy: 0.9390 - val_loss: 1.5673 - val_accuracy: 0.5486\n",
      "Epoch 75/80\n",
      "36/36 [==============================] - 0s 7ms/step - loss: 0.2062 - accuracy: 0.9338 - val_loss: 1.5967 - val_accuracy: 0.5278\n",
      "Epoch 76/80\n",
      "36/36 [==============================] - 0s 7ms/step - loss: 0.1959 - accuracy: 0.9233 - val_loss: 1.5897 - val_accuracy: 0.5417\n",
      "Epoch 77/80\n",
      "36/36 [==============================] - 0s 7ms/step - loss: 0.1749 - accuracy: 0.9390 - val_loss: 1.6246 - val_accuracy: 0.5417\n",
      "Epoch 78/80\n",
      "36/36 [==============================] - 0s 6ms/step - loss: 0.1881 - accuracy: 0.9338 - val_loss: 1.6766 - val_accuracy: 0.5139\n",
      "Epoch 79/80\n",
      "36/36 [==============================] - 0s 6ms/step - loss: 0.1790 - accuracy: 0.9355 - val_loss: 1.6880 - val_accuracy: 0.5278\n",
      "Epoch 80/80\n",
      "36/36 [==============================] - 0s 6ms/step - loss: 0.1480 - accuracy: 0.9617 - val_loss: 1.7048 - val_accuracy: 0.5208\n"
     ]
    }
   ],
   "source": [
    "history4 = model4.fit(X1_train_scaled, y1_train, \n",
    "                      validation_data=(X1_val_scaled, y1_val), \n",
    "                      batch_size=16, \n",
    "                      epochs=80)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f52d1fd",
   "metadata": {},
   "source": [
    "#### Use different parameters\n",
    "\n",
    "**1. Less unit num in Dense layer**<br>\n",
    "Unit number in Dense layer: 128 -> 64, 64 -> 32\n",
    "\n",
    "Lower complexity of the model improves the validation accuracy a bit and decreases the training validation a bit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "8b9cf91a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"Conv1D_5\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " Conv1D_1 (Conv1D)           (None, 57, 16)            96        \n",
      "                                                                 \n",
      " BN1 (BatchNormalization)    (None, 57, 16)            64        \n",
      "                                                                 \n",
      " MaxPooling1 (MaxPooling1D)  (None, 28, 16)            0         \n",
      "                                                                 \n",
      " Conv1D_2 (Conv1D)           (None, 28, 32)            3616      \n",
      "                                                                 \n",
      " BN2 (BatchNormalization)    (None, 28, 32)            128       \n",
      "                                                                 \n",
      " MaxPooling2 (MaxPooling1D)  (None, 14, 32)            0         \n",
      "                                                                 \n",
      " Conv1D_3 (Conv1D)           (None, 14, 32)            7200      \n",
      "                                                                 \n",
      " BN3 (BatchNormalization)    (None, 14, 32)            128       \n",
      "                                                                 \n",
      " MaxPooling3 (MaxPooling1D)  (None, 7, 32)             0         \n",
      "                                                                 \n",
      " Flatten (Flatten)           (None, 224)               0         \n",
      "                                                                 \n",
      " Dense_1 (Dense)             (None, 64)                14400     \n",
      "                                                                 \n",
      " Dropout_1 (Dropout)         (None, 64)                0         \n",
      "                                                                 \n",
      " Dense_2 (Dense)             (None, 32)                2080      \n",
      "                                                                 \n",
      " Dropout_2 (Dropout)         (None, 32)                0         \n",
      "                                                                 \n",
      " Softmax (Dense)             (None, 4)                 132       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 27,844\n",
      "Trainable params: 27,684\n",
      "Non-trainable params: 160\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "f1,k1,a1 = 16,5,'relu'\n",
    "f2,k2,a2 = 32,7,'relu'\n",
    "f3,k3,a3 = 32,7,'relu'\n",
    "d1,dr1,da1,r1 = 64,0.2,'relu',0.15\n",
    "d2,dr2,da2,r2 = 32,0.2,'relu',0.15\n",
    "num = 5\n",
    "\n",
    "model5 = modelBuilder3L(X1_train_scaled,\n",
    "                        f1,k1,a1,\n",
    "                        f2,k2,a2,\n",
    "                        f3,k3,a3,\n",
    "                        d1,dr1,da1,r1,\n",
    "                        d2,dr2,da2,r2,\n",
    "                        num)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "dabc0eb3",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/80\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "36/36 [==============================] - 3s 17ms/step - loss: 1.5885 - accuracy: 0.3118 - val_loss: 1.3873 - val_accuracy: 0.3194\n",
      "Epoch 2/80\n",
      "36/36 [==============================] - 0s 7ms/step - loss: 1.3589 - accuracy: 0.3432 - val_loss: 1.3873 - val_accuracy: 0.2778\n",
      "Epoch 3/80\n",
      "36/36 [==============================] - 0s 6ms/step - loss: 1.2298 - accuracy: 0.4181 - val_loss: 1.3885 - val_accuracy: 0.3264\n",
      "Epoch 4/80\n",
      "36/36 [==============================] - 0s 7ms/step - loss: 1.1685 - accuracy: 0.4930 - val_loss: 1.3937 - val_accuracy: 0.3125\n",
      "Epoch 5/80\n",
      "36/36 [==============================] - 0s 8ms/step - loss: 1.1265 - accuracy: 0.4843 - val_loss: 1.3990 - val_accuracy: 0.3194\n",
      "Epoch 6/80\n",
      "36/36 [==============================] - 0s 6ms/step - loss: 1.0892 - accuracy: 0.5192 - val_loss: 1.4044 - val_accuracy: 0.3542\n",
      "Epoch 7/80\n",
      "36/36 [==============================] - 0s 6ms/step - loss: 1.0092 - accuracy: 0.5732 - val_loss: 1.4036 - val_accuracy: 0.3542\n",
      "Epoch 8/80\n",
      "36/36 [==============================] - 0s 6ms/step - loss: 0.9959 - accuracy: 0.5679 - val_loss: 1.3982 - val_accuracy: 0.3750\n",
      "Epoch 9/80\n",
      "36/36 [==============================] - 0s 6ms/step - loss: 0.9474 - accuracy: 0.6063 - val_loss: 1.3868 - val_accuracy: 0.3819\n",
      "Epoch 10/80\n",
      "36/36 [==============================] - 0s 6ms/step - loss: 0.9247 - accuracy: 0.6080 - val_loss: 1.3645 - val_accuracy: 0.3750\n",
      "Epoch 11/80\n",
      "36/36 [==============================] - 0s 6ms/step - loss: 0.9303 - accuracy: 0.5958 - val_loss: 1.3298 - val_accuracy: 0.3889\n",
      "Epoch 12/80\n",
      "36/36 [==============================] - 0s 7ms/step - loss: 0.8956 - accuracy: 0.6220 - val_loss: 1.2682 - val_accuracy: 0.4514\n",
      "Epoch 13/80\n",
      "36/36 [==============================] - 0s 6ms/step - loss: 0.8921 - accuracy: 0.6150 - val_loss: 1.2060 - val_accuracy: 0.4931\n",
      "Epoch 14/80\n",
      "36/36 [==============================] - 0s 6ms/step - loss: 0.8574 - accuracy: 0.6063 - val_loss: 1.1346 - val_accuracy: 0.5069\n",
      "Epoch 15/80\n",
      "36/36 [==============================] - 0s 6ms/step - loss: 0.8654 - accuracy: 0.6568 - val_loss: 1.0924 - val_accuracy: 0.5208\n",
      "Epoch 16/80\n",
      "36/36 [==============================] - 0s 7ms/step - loss: 0.8209 - accuracy: 0.6568 - val_loss: 1.0745 - val_accuracy: 0.5139\n",
      "Epoch 17/80\n",
      "36/36 [==============================] - 0s 10ms/step - loss: 0.7845 - accuracy: 0.6847 - val_loss: 1.0476 - val_accuracy: 0.5208\n",
      "Epoch 18/80\n",
      "36/36 [==============================] - 0s 7ms/step - loss: 0.8081 - accuracy: 0.6742 - val_loss: 1.0368 - val_accuracy: 0.5139\n",
      "Epoch 19/80\n",
      "36/36 [==============================] - 0s 6ms/step - loss: 0.7575 - accuracy: 0.7091 - val_loss: 1.0274 - val_accuracy: 0.5069\n",
      "Epoch 20/80\n",
      "36/36 [==============================] - 0s 6ms/step - loss: 0.7883 - accuracy: 0.6585 - val_loss: 1.0426 - val_accuracy: 0.5069\n",
      "Epoch 21/80\n",
      "36/36 [==============================] - 0s 6ms/step - loss: 0.7446 - accuracy: 0.7056 - val_loss: 1.0333 - val_accuracy: 0.4931\n",
      "Epoch 22/80\n",
      "36/36 [==============================] - 0s 5ms/step - loss: 0.7402 - accuracy: 0.6760 - val_loss: 1.0252 - val_accuracy: 0.5139\n",
      "Epoch 23/80\n",
      "36/36 [==============================] - 0s 6ms/step - loss: 0.7033 - accuracy: 0.7265 - val_loss: 1.0220 - val_accuracy: 0.4861\n",
      "Epoch 24/80\n",
      "36/36 [==============================] - 0s 6ms/step - loss: 0.7220 - accuracy: 0.7038 - val_loss: 1.0217 - val_accuracy: 0.4931\n",
      "Epoch 25/80\n",
      "36/36 [==============================] - 0s 6ms/step - loss: 0.6806 - accuracy: 0.7369 - val_loss: 1.0216 - val_accuracy: 0.5139\n",
      "Epoch 26/80\n",
      "36/36 [==============================] - 0s 7ms/step - loss: 0.6806 - accuracy: 0.7352 - val_loss: 1.0271 - val_accuracy: 0.5000\n",
      "Epoch 27/80\n",
      "36/36 [==============================] - 0s 5ms/step - loss: 0.6595 - accuracy: 0.7474 - val_loss: 1.0198 - val_accuracy: 0.5000\n",
      "Epoch 28/80\n",
      "36/36 [==============================] - 0s 9ms/step - loss: 0.6563 - accuracy: 0.7491 - val_loss: 1.0216 - val_accuracy: 0.5000\n",
      "Epoch 29/80\n",
      "36/36 [==============================] - 0s 6ms/step - loss: 0.6325 - accuracy: 0.7561 - val_loss: 1.0200 - val_accuracy: 0.5139\n",
      "Epoch 30/80\n",
      "36/36 [==============================] - 0s 6ms/step - loss: 0.6107 - accuracy: 0.7666 - val_loss: 1.0202 - val_accuracy: 0.5000\n",
      "Epoch 31/80\n",
      "36/36 [==============================] - 0s 6ms/step - loss: 0.6326 - accuracy: 0.7509 - val_loss: 1.0262 - val_accuracy: 0.4931\n",
      "Epoch 32/80\n",
      "36/36 [==============================] - 0s 6ms/step - loss: 0.5933 - accuracy: 0.7753 - val_loss: 1.0192 - val_accuracy: 0.4722\n",
      "Epoch 33/80\n",
      "36/36 [==============================] - 0s 6ms/step - loss: 0.6083 - accuracy: 0.7474 - val_loss: 1.0096 - val_accuracy: 0.4653\n",
      "Epoch 34/80\n",
      "36/36 [==============================] - 0s 6ms/step - loss: 0.5622 - accuracy: 0.7648 - val_loss: 1.0212 - val_accuracy: 0.4931\n",
      "Epoch 35/80\n",
      "36/36 [==============================] - 0s 5ms/step - loss: 0.5671 - accuracy: 0.7875 - val_loss: 1.0440 - val_accuracy: 0.4722\n",
      "Epoch 36/80\n",
      "36/36 [==============================] - 0s 6ms/step - loss: 0.5359 - accuracy: 0.8101 - val_loss: 1.0467 - val_accuracy: 0.4861\n",
      "Epoch 37/80\n",
      "36/36 [==============================] - 0s 5ms/step - loss: 0.5202 - accuracy: 0.8171 - val_loss: 1.0458 - val_accuracy: 0.4722\n",
      "Epoch 38/80\n",
      "36/36 [==============================] - 0s 6ms/step - loss: 0.4985 - accuracy: 0.8031 - val_loss: 1.0433 - val_accuracy: 0.4931\n",
      "Epoch 39/80\n",
      "36/36 [==============================] - 0s 6ms/step - loss: 0.5585 - accuracy: 0.7787 - val_loss: 1.0648 - val_accuracy: 0.4792\n",
      "Epoch 40/80\n",
      "36/36 [==============================] - 0s 6ms/step - loss: 0.4877 - accuracy: 0.8258 - val_loss: 1.0622 - val_accuracy: 0.4861\n",
      "Epoch 41/80\n",
      "36/36 [==============================] - 0s 5ms/step - loss: 0.4948 - accuracy: 0.8171 - val_loss: 1.0666 - val_accuracy: 0.4722\n",
      "Epoch 42/80\n",
      "36/36 [==============================] - 0s 9ms/step - loss: 0.4917 - accuracy: 0.8467 - val_loss: 1.0748 - val_accuracy: 0.4931\n",
      "Epoch 43/80\n",
      "36/36 [==============================] - 0s 6ms/step - loss: 0.4540 - accuracy: 0.8328 - val_loss: 1.0739 - val_accuracy: 0.4792\n",
      "Epoch 44/80\n",
      "36/36 [==============================] - 0s 6ms/step - loss: 0.4954 - accuracy: 0.8380 - val_loss: 1.0971 - val_accuracy: 0.4792\n",
      "Epoch 45/80\n",
      "36/36 [==============================] - 0s 6ms/step - loss: 0.4486 - accuracy: 0.8258 - val_loss: 1.0920 - val_accuracy: 0.4931\n",
      "Epoch 46/80\n",
      "36/36 [==============================] - 0s 7ms/step - loss: 0.4303 - accuracy: 0.8449 - val_loss: 1.0785 - val_accuracy: 0.4931\n",
      "Epoch 47/80\n",
      "36/36 [==============================] - 0s 7ms/step - loss: 0.4704 - accuracy: 0.8310 - val_loss: 1.0873 - val_accuracy: 0.4931\n",
      "Epoch 48/80\n",
      "36/36 [==============================] - 0s 10ms/step - loss: 0.4345 - accuracy: 0.8328 - val_loss: 1.1141 - val_accuracy: 0.4792\n",
      "Epoch 49/80\n",
      "36/36 [==============================] - 0s 8ms/step - loss: 0.4059 - accuracy: 0.8606 - val_loss: 1.1299 - val_accuracy: 0.4861\n",
      "Epoch 50/80\n",
      "36/36 [==============================] - 0s 8ms/step - loss: 0.4036 - accuracy: 0.8571 - val_loss: 1.1442 - val_accuracy: 0.5000\n",
      "Epoch 51/80\n",
      "36/36 [==============================] - 0s 12ms/step - loss: 0.3953 - accuracy: 0.8676 - val_loss: 1.1726 - val_accuracy: 0.4861\n",
      "Epoch 52/80\n",
      "36/36 [==============================] - 0s 6ms/step - loss: 0.3857 - accuracy: 0.8537 - val_loss: 1.1757 - val_accuracy: 0.4931\n",
      "Epoch 53/80\n",
      "36/36 [==============================] - 0s 9ms/step - loss: 0.3900 - accuracy: 0.8693 - val_loss: 1.1736 - val_accuracy: 0.4792\n",
      "Epoch 54/80\n",
      "36/36 [==============================] - 0s 9ms/step - loss: 0.3507 - accuracy: 0.8902 - val_loss: 1.1666 - val_accuracy: 0.5069\n",
      "Epoch 55/80\n",
      "36/36 [==============================] - 0s 6ms/step - loss: 0.3290 - accuracy: 0.8868 - val_loss: 1.1835 - val_accuracy: 0.5069\n",
      "Epoch 56/80\n",
      "36/36 [==============================] - 0s 6ms/step - loss: 0.3718 - accuracy: 0.8659 - val_loss: 1.1791 - val_accuracy: 0.4931\n",
      "Epoch 57/80\n",
      "36/36 [==============================] - 0s 8ms/step - loss: 0.3391 - accuracy: 0.8868 - val_loss: 1.1904 - val_accuracy: 0.5069\n",
      "Epoch 58/80\n",
      "36/36 [==============================] - 0s 6ms/step - loss: 0.3215 - accuracy: 0.8885 - val_loss: 1.1919 - val_accuracy: 0.4931\n",
      "Epoch 59/80\n",
      "36/36 [==============================] - 0s 5ms/step - loss: 0.3091 - accuracy: 0.8990 - val_loss: 1.2403 - val_accuracy: 0.5000\n",
      "Epoch 60/80\n",
      "36/36 [==============================] - 0s 6ms/step - loss: 0.3279 - accuracy: 0.8746 - val_loss: 1.2623 - val_accuracy: 0.4931\n",
      "Epoch 61/80\n",
      "36/36 [==============================] - 0s 6ms/step - loss: 0.3286 - accuracy: 0.8902 - val_loss: 1.2623 - val_accuracy: 0.5208\n",
      "Epoch 62/80\n",
      "36/36 [==============================] - 0s 7ms/step - loss: 0.2697 - accuracy: 0.9164 - val_loss: 1.2635 - val_accuracy: 0.5278\n",
      "Epoch 63/80\n",
      "36/36 [==============================] - 0s 6ms/step - loss: 0.3134 - accuracy: 0.8990 - val_loss: 1.2562 - val_accuracy: 0.5278\n",
      "Epoch 64/80\n",
      "36/36 [==============================] - 0s 6ms/step - loss: 0.2862 - accuracy: 0.8885 - val_loss: 1.2831 - val_accuracy: 0.5069\n",
      "Epoch 65/80\n",
      "36/36 [==============================] - 0s 10ms/step - loss: 0.2688 - accuracy: 0.9181 - val_loss: 1.2552 - val_accuracy: 0.5208\n",
      "Epoch 66/80\n",
      "36/36 [==============================] - 0s 6ms/step - loss: 0.2935 - accuracy: 0.8885 - val_loss: 1.3102 - val_accuracy: 0.4861\n",
      "Epoch 67/80\n",
      "36/36 [==============================] - 0s 11ms/step - loss: 0.2627 - accuracy: 0.9216 - val_loss: 1.3040 - val_accuracy: 0.5139\n",
      "Epoch 68/80\n",
      "36/36 [==============================] - 0s 9ms/step - loss: 0.2737 - accuracy: 0.9042 - val_loss: 1.2966 - val_accuracy: 0.5000\n",
      "Epoch 69/80\n",
      "36/36 [==============================] - 0s 6ms/step - loss: 0.2678 - accuracy: 0.9129 - val_loss: 1.3411 - val_accuracy: 0.5000\n",
      "Epoch 70/80\n",
      "36/36 [==============================] - 0s 10ms/step - loss: 0.2341 - accuracy: 0.9233 - val_loss: 1.3109 - val_accuracy: 0.5069\n",
      "Epoch 71/80\n",
      "36/36 [==============================] - 0s 10ms/step - loss: 0.2364 - accuracy: 0.9321 - val_loss: 1.3553 - val_accuracy: 0.4792\n",
      "Epoch 72/80\n",
      "36/36 [==============================] - 0s 8ms/step - loss: 0.2271 - accuracy: 0.9338 - val_loss: 1.3390 - val_accuracy: 0.4861\n",
      "Epoch 73/80\n",
      "36/36 [==============================] - 0s 7ms/step - loss: 0.2307 - accuracy: 0.9216 - val_loss: 1.3625 - val_accuracy: 0.4931\n",
      "Epoch 74/80\n",
      "36/36 [==============================] - 0s 7ms/step - loss: 0.2186 - accuracy: 0.9251 - val_loss: 1.3900 - val_accuracy: 0.4792\n",
      "Epoch 75/80\n",
      "36/36 [==============================] - 0s 6ms/step - loss: 0.2081 - accuracy: 0.9408 - val_loss: 1.4159 - val_accuracy: 0.4792\n",
      "Epoch 76/80\n",
      "36/36 [==============================] - 0s 6ms/step - loss: 0.2090 - accuracy: 0.9390 - val_loss: 1.3995 - val_accuracy: 0.5000\n",
      "Epoch 77/80\n",
      "36/36 [==============================] - 0s 6ms/step - loss: 0.1854 - accuracy: 0.9390 - val_loss: 1.4059 - val_accuracy: 0.5139\n",
      "Epoch 78/80\n",
      "36/36 [==============================] - 0s 6ms/step - loss: 0.1845 - accuracy: 0.9512 - val_loss: 1.4597 - val_accuracy: 0.5000\n",
      "Epoch 79/80\n",
      "36/36 [==============================] - 0s 6ms/step - loss: 0.1967 - accuracy: 0.9408 - val_loss: 1.4866 - val_accuracy: 0.4792\n",
      "Epoch 80/80\n",
      "36/36 [==============================] - 0s 6ms/step - loss: 0.1791 - accuracy: 0.9530 - val_loss: 1.4907 - val_accuracy: 0.4931\n"
     ]
    }
   ],
   "source": [
    "history5 = model5.fit(X1_train_scaled, y1_train, \n",
    "                      validation_data=(X1_val_scaled, y1_val), \n",
    "                      batch_size=16, \n",
    "                      epochs=80)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87a1efe6",
   "metadata": {},
   "source": [
    "Then, try: <br>\n",
    "Unit number of Dense layers: 128 -> 32, 64 -> 16\n",
    "\n",
    "\n",
    "But this way is unable to improve the model validation accuracy prominently but sacrifices the training accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "d78cb1e7",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"Conv1D_5\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " Conv1D_1 (Conv1D)           (None, 57, 16)            96        \n",
      "                                                                 \n",
      " BN1 (BatchNormalization)    (None, 57, 16)            64        \n",
      "                                                                 \n",
      " MaxPooling1 (MaxPooling1D)  (None, 28, 16)            0         \n",
      "                                                                 \n",
      " Conv1D_2 (Conv1D)           (None, 28, 32)            3616      \n",
      "                                                                 \n",
      " BN2 (BatchNormalization)    (None, 28, 32)            128       \n",
      "                                                                 \n",
      " MaxPooling2 (MaxPooling1D)  (None, 14, 32)            0         \n",
      "                                                                 \n",
      " Conv1D_3 (Conv1D)           (None, 14, 32)            7200      \n",
      "                                                                 \n",
      " BN3 (BatchNormalization)    (None, 14, 32)            128       \n",
      "                                                                 \n",
      " MaxPooling3 (MaxPooling1D)  (None, 7, 32)             0         \n",
      "                                                                 \n",
      " Flatten (Flatten)           (None, 224)               0         \n",
      "                                                                 \n",
      " Dense_1 (Dense)             (None, 32)                7200      \n",
      "                                                                 \n",
      " Dropout_1 (Dropout)         (None, 32)                0         \n",
      "                                                                 \n",
      " Dense_2 (Dense)             (None, 16)                528       \n",
      "                                                                 \n",
      " Dropout_2 (Dropout)         (None, 16)                0         \n",
      "                                                                 \n",
      " Softmax (Dense)             (None, 4)                 68        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 19,028\n",
      "Trainable params: 18,868\n",
      "Non-trainable params: 160\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "f1,k1,a1 = 16,5,'relu'\n",
    "f2,k2,a2 = 32,7,'relu'\n",
    "f3,k3,a3 = 32,7,'relu'\n",
    "d1,dr1,da1,r1 = 32,0.2,'relu',0.15\n",
    "d2,dr2,da2,r2 = 16,0.2,'relu',0.15\n",
    "num = 5\n",
    "\n",
    "model5 = modelBuilder3L(X1_train_scaled,\n",
    "                        f1,k1,a1,                            \n",
    "                        f2,k2,a2,\n",
    "                        f3,k3,a3,                               \n",
    "                        d1,dr1,da1,r1,                        \n",
    "                        d2,dr2,da2,r2,\n",
    "                        num)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "2c642b5c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/80\n",
      "36/36 [==============================] - 3s 14ms/step - loss: 1.7834 - accuracy: 0.2875 - val_loss: 1.3888 - val_accuracy: 0.2500\n",
      "Epoch 2/80\n",
      "36/36 [==============================] - 0s 7ms/step - loss: 1.4573 - accuracy: 0.3258 - val_loss: 1.3939 - val_accuracy: 0.2500\n",
      "Epoch 3/80\n",
      "36/36 [==============================] - 0s 6ms/step - loss: 1.3332 - accuracy: 0.3589 - val_loss: 1.3911 - val_accuracy: 0.2500\n",
      "Epoch 4/80\n",
      "36/36 [==============================] - 0s 6ms/step - loss: 1.2714 - accuracy: 0.3746 - val_loss: 1.3858 - val_accuracy: 0.2500\n",
      "Epoch 5/80\n",
      "36/36 [==============================] - 0s 5ms/step - loss: 1.2259 - accuracy: 0.4024 - val_loss: 1.3766 - val_accuracy: 0.2569\n",
      "Epoch 6/80\n",
      "36/36 [==============================] - 0s 6ms/step - loss: 1.1934 - accuracy: 0.4251 - val_loss: 1.3597 - val_accuracy: 0.2708\n",
      "Epoch 7/80\n",
      "36/36 [==============================] - 0s 8ms/step - loss: 1.1688 - accuracy: 0.4477 - val_loss: 1.3390 - val_accuracy: 0.3194\n",
      "Epoch 8/80\n",
      "36/36 [==============================] - 0s 6ms/step - loss: 1.1518 - accuracy: 0.4564 - val_loss: 1.3157 - val_accuracy: 0.3542\n",
      "Epoch 9/80\n",
      "36/36 [==============================] - 0s 5ms/step - loss: 1.1068 - accuracy: 0.4861 - val_loss: 1.2842 - val_accuracy: 0.3542\n",
      "Epoch 10/80\n",
      "36/36 [==============================] - 0s 5ms/step - loss: 1.0453 - accuracy: 0.5157 - val_loss: 1.2556 - val_accuracy: 0.4306\n",
      "Epoch 11/80\n",
      "36/36 [==============================] - 0s 5ms/step - loss: 1.0393 - accuracy: 0.5401 - val_loss: 1.2276 - val_accuracy: 0.4722\n",
      "Epoch 12/80\n",
      "36/36 [==============================] - 0s 5ms/step - loss: 1.0111 - accuracy: 0.5627 - val_loss: 1.2054 - val_accuracy: 0.5000\n",
      "Epoch 13/80\n",
      "36/36 [==============================] - 0s 5ms/step - loss: 0.9891 - accuracy: 0.5505 - val_loss: 1.1736 - val_accuracy: 0.5000\n",
      "Epoch 14/80\n",
      "36/36 [==============================] - 0s 5ms/step - loss: 0.9487 - accuracy: 0.5801 - val_loss: 1.1509 - val_accuracy: 0.5069\n",
      "Epoch 15/80\n",
      "36/36 [==============================] - 0s 7ms/step - loss: 0.9651 - accuracy: 0.5836 - val_loss: 1.1382 - val_accuracy: 0.5208\n",
      "Epoch 16/80\n",
      "36/36 [==============================] - 0s 6ms/step - loss: 0.9200 - accuracy: 0.6115 - val_loss: 1.1277 - val_accuracy: 0.5069\n",
      "Epoch 17/80\n",
      "36/36 [==============================] - 0s 5ms/step - loss: 0.9114 - accuracy: 0.5784 - val_loss: 1.1121 - val_accuracy: 0.5139\n",
      "Epoch 18/80\n",
      "36/36 [==============================] - 0s 5ms/step - loss: 0.9115 - accuracy: 0.6185 - val_loss: 1.1165 - val_accuracy: 0.5208\n",
      "Epoch 19/80\n",
      "36/36 [==============================] - 0s 7ms/step - loss: 0.8797 - accuracy: 0.6220 - val_loss: 1.1287 - val_accuracy: 0.5208\n",
      "Epoch 20/80\n",
      "36/36 [==============================] - 0s 6ms/step - loss: 0.8471 - accuracy: 0.6446 - val_loss: 1.1353 - val_accuracy: 0.5208\n",
      "Epoch 21/80\n",
      "36/36 [==============================] - 0s 5ms/step - loss: 0.8493 - accuracy: 0.6237 - val_loss: 1.1329 - val_accuracy: 0.5139\n",
      "Epoch 22/80\n",
      "36/36 [==============================] - 0s 5ms/step - loss: 0.8057 - accuracy: 0.6516 - val_loss: 1.1378 - val_accuracy: 0.5208\n",
      "Epoch 23/80\n",
      "36/36 [==============================] - 0s 5ms/step - loss: 0.8069 - accuracy: 0.6603 - val_loss: 1.1415 - val_accuracy: 0.5208\n",
      "Epoch 24/80\n",
      "36/36 [==============================] - 0s 8ms/step - loss: 0.8212 - accuracy: 0.6516 - val_loss: 1.1384 - val_accuracy: 0.5278\n",
      "Epoch 25/80\n",
      "36/36 [==============================] - 0s 7ms/step - loss: 0.7591 - accuracy: 0.6742 - val_loss: 1.1357 - val_accuracy: 0.5208\n",
      "Epoch 26/80\n",
      "36/36 [==============================] - 0s 7ms/step - loss: 0.7446 - accuracy: 0.7091 - val_loss: 1.1408 - val_accuracy: 0.5208\n",
      "Epoch 27/80\n",
      "36/36 [==============================] - 0s 6ms/step - loss: 0.7470 - accuracy: 0.7021 - val_loss: 1.1381 - val_accuracy: 0.5417\n",
      "Epoch 28/80\n",
      "36/36 [==============================] - 0s 5ms/step - loss: 0.7548 - accuracy: 0.7091 - val_loss: 1.1554 - val_accuracy: 0.5278\n",
      "Epoch 29/80\n",
      "36/36 [==============================] - 0s 5ms/step - loss: 0.7094 - accuracy: 0.7108 - val_loss: 1.1509 - val_accuracy: 0.5486\n",
      "Epoch 30/80\n",
      "36/36 [==============================] - 0s 6ms/step - loss: 0.6870 - accuracy: 0.7213 - val_loss: 1.1493 - val_accuracy: 0.5347\n",
      "Epoch 31/80\n",
      "36/36 [==============================] - 0s 6ms/step - loss: 0.7176 - accuracy: 0.7091 - val_loss: 1.1668 - val_accuracy: 0.4931\n",
      "Epoch 32/80\n",
      "36/36 [==============================] - 0s 6ms/step - loss: 0.6899 - accuracy: 0.7178 - val_loss: 1.1673 - val_accuracy: 0.5139\n",
      "Epoch 33/80\n",
      "36/36 [==============================] - 0s 5ms/step - loss: 0.6798 - accuracy: 0.7526 - val_loss: 1.1522 - val_accuracy: 0.5208\n",
      "Epoch 34/80\n",
      "36/36 [==============================] - 0s 5ms/step - loss: 0.7026 - accuracy: 0.7125 - val_loss: 1.1499 - val_accuracy: 0.5139\n",
      "Epoch 35/80\n",
      "36/36 [==============================] - 0s 5ms/step - loss: 0.6872 - accuracy: 0.7282 - val_loss: 1.1666 - val_accuracy: 0.5000\n",
      "Epoch 36/80\n",
      "36/36 [==============================] - 0s 5ms/step - loss: 0.6405 - accuracy: 0.7526 - val_loss: 1.1592 - val_accuracy: 0.5278\n",
      "Epoch 37/80\n",
      "36/36 [==============================] - 0s 5ms/step - loss: 0.5963 - accuracy: 0.7735 - val_loss: 1.1356 - val_accuracy: 0.5347\n",
      "Epoch 38/80\n",
      "36/36 [==============================] - 0s 5ms/step - loss: 0.6138 - accuracy: 0.7648 - val_loss: 1.1276 - val_accuracy: 0.5556\n",
      "Epoch 39/80\n",
      "36/36 [==============================] - 0s 9ms/step - loss: 0.6206 - accuracy: 0.7456 - val_loss: 1.1589 - val_accuracy: 0.5278\n",
      "Epoch 40/80\n",
      "36/36 [==============================] - 0s 5ms/step - loss: 0.6229 - accuracy: 0.7404 - val_loss: 1.1582 - val_accuracy: 0.5417\n",
      "Epoch 41/80\n",
      "36/36 [==============================] - 0s 6ms/step - loss: 0.6060 - accuracy: 0.7700 - val_loss: 1.1369 - val_accuracy: 0.5625\n",
      "Epoch 42/80\n",
      "36/36 [==============================] - 0s 5ms/step - loss: 0.5865 - accuracy: 0.7770 - val_loss: 1.1518 - val_accuracy: 0.5625\n",
      "Epoch 43/80\n",
      "36/36 [==============================] - 0s 5ms/step - loss: 0.5748 - accuracy: 0.7718 - val_loss: 1.1493 - val_accuracy: 0.5556\n",
      "Epoch 44/80\n",
      "36/36 [==============================] - 0s 6ms/step - loss: 0.5611 - accuracy: 0.7962 - val_loss: 1.1679 - val_accuracy: 0.5347\n",
      "Epoch 45/80\n",
      "36/36 [==============================] - 0s 5ms/step - loss: 0.5371 - accuracy: 0.7944 - val_loss: 1.1866 - val_accuracy: 0.5139\n",
      "Epoch 46/80\n",
      "36/36 [==============================] - 0s 5ms/step - loss: 0.5602 - accuracy: 0.7770 - val_loss: 1.1921 - val_accuracy: 0.5208\n",
      "Epoch 47/80\n",
      "36/36 [==============================] - 0s 5ms/step - loss: 0.5556 - accuracy: 0.7875 - val_loss: 1.1810 - val_accuracy: 0.5556\n",
      "Epoch 48/80\n",
      "36/36 [==============================] - 0s 5ms/step - loss: 0.5579 - accuracy: 0.7857 - val_loss: 1.1989 - val_accuracy: 0.5486\n",
      "Epoch 49/80\n",
      "36/36 [==============================] - 0s 6ms/step - loss: 0.4949 - accuracy: 0.8049 - val_loss: 1.1958 - val_accuracy: 0.5694\n",
      "Epoch 50/80\n",
      "36/36 [==============================] - 0s 5ms/step - loss: 0.4782 - accuracy: 0.8362 - val_loss: 1.1806 - val_accuracy: 0.5694\n",
      "Epoch 51/80\n",
      "36/36 [==============================] - 0s 5ms/step - loss: 0.5234 - accuracy: 0.7805 - val_loss: 1.2131 - val_accuracy: 0.5208\n",
      "Epoch 52/80\n",
      "36/36 [==============================] - 0s 5ms/step - loss: 0.5103 - accuracy: 0.8171 - val_loss: 1.1824 - val_accuracy: 0.5694\n",
      "Epoch 53/80\n",
      "36/36 [==============================] - 0s 5ms/step - loss: 0.4389 - accuracy: 0.8432 - val_loss: 1.2174 - val_accuracy: 0.5764\n",
      "Epoch 54/80\n",
      "36/36 [==============================] - 0s 5ms/step - loss: 0.4581 - accuracy: 0.8449 - val_loss: 1.2050 - val_accuracy: 0.5625\n",
      "Epoch 55/80\n",
      "36/36 [==============================] - 0s 5ms/step - loss: 0.4214 - accuracy: 0.8432 - val_loss: 1.2008 - val_accuracy: 0.5833\n",
      "Epoch 56/80\n",
      "36/36 [==============================] - 0s 5ms/step - loss: 0.4311 - accuracy: 0.8415 - val_loss: 1.2063 - val_accuracy: 0.5764\n",
      "Epoch 57/80\n",
      "36/36 [==============================] - 0s 5ms/step - loss: 0.4493 - accuracy: 0.8223 - val_loss: 1.2232 - val_accuracy: 0.5903\n",
      "Epoch 58/80\n",
      "36/36 [==============================] - 0s 9ms/step - loss: 0.4246 - accuracy: 0.8345 - val_loss: 1.2366 - val_accuracy: 0.5625\n",
      "Epoch 59/80\n",
      "36/36 [==============================] - 0s 5ms/step - loss: 0.4480 - accuracy: 0.8345 - val_loss: 1.2309 - val_accuracy: 0.5833\n",
      "Epoch 60/80\n",
      "36/36 [==============================] - 0s 5ms/step - loss: 0.4345 - accuracy: 0.8432 - val_loss: 1.2225 - val_accuracy: 0.5833\n",
      "Epoch 61/80\n",
      "36/36 [==============================] - 0s 5ms/step - loss: 0.4648 - accuracy: 0.8310 - val_loss: 1.2017 - val_accuracy: 0.5833\n",
      "Epoch 62/80\n",
      "36/36 [==============================] - 0s 5ms/step - loss: 0.3986 - accuracy: 0.8432 - val_loss: 1.2399 - val_accuracy: 0.5764\n",
      "Epoch 63/80\n",
      "36/36 [==============================] - 0s 5ms/step - loss: 0.3960 - accuracy: 0.8624 - val_loss: 1.2362 - val_accuracy: 0.5764\n",
      "Epoch 64/80\n",
      "36/36 [==============================] - 0s 5ms/step - loss: 0.3945 - accuracy: 0.8537 - val_loss: 1.2367 - val_accuracy: 0.5694\n",
      "Epoch 65/80\n",
      "36/36 [==============================] - 0s 5ms/step - loss: 0.4168 - accuracy: 0.8502 - val_loss: 1.2776 - val_accuracy: 0.5347\n",
      "Epoch 66/80\n",
      "36/36 [==============================] - 0s 5ms/step - loss: 0.3931 - accuracy: 0.8606 - val_loss: 1.2662 - val_accuracy: 0.5417\n",
      "Epoch 67/80\n",
      "36/36 [==============================] - 0s 5ms/step - loss: 0.4068 - accuracy: 0.8659 - val_loss: 1.2844 - val_accuracy: 0.5347\n",
      "Epoch 68/80\n",
      "36/36 [==============================] - 0s 6ms/step - loss: 0.3635 - accuracy: 0.8746 - val_loss: 1.2685 - val_accuracy: 0.5625\n",
      "Epoch 69/80\n",
      "36/36 [==============================] - 0s 6ms/step - loss: 0.3815 - accuracy: 0.8711 - val_loss: 1.2949 - val_accuracy: 0.5417\n",
      "Epoch 70/80\n",
      "36/36 [==============================] - 0s 6ms/step - loss: 0.3855 - accuracy: 0.8432 - val_loss: 1.2997 - val_accuracy: 0.5417\n",
      "Epoch 71/80\n",
      "36/36 [==============================] - 0s 6ms/step - loss: 0.3585 - accuracy: 0.8815 - val_loss: 1.2741 - val_accuracy: 0.5764\n",
      "Epoch 72/80\n",
      "36/36 [==============================] - 0s 6ms/step - loss: 0.3544 - accuracy: 0.8728 - val_loss: 1.2831 - val_accuracy: 0.5347\n",
      "Epoch 73/80\n",
      "36/36 [==============================] - 0s 9ms/step - loss: 0.3418 - accuracy: 0.8815 - val_loss: 1.2960 - val_accuracy: 0.5694\n",
      "Epoch 74/80\n",
      "36/36 [==============================] - 0s 6ms/step - loss: 0.3410 - accuracy: 0.8728 - val_loss: 1.3301 - val_accuracy: 0.5278\n",
      "Epoch 75/80\n",
      "36/36 [==============================] - 0s 6ms/step - loss: 0.3368 - accuracy: 0.8885 - val_loss: 1.2985 - val_accuracy: 0.5417\n",
      "Epoch 76/80\n",
      "36/36 [==============================] - 0s 6ms/step - loss: 0.3082 - accuracy: 0.8990 - val_loss: 1.3166 - val_accuracy: 0.5278\n",
      "Epoch 77/80\n",
      "36/36 [==============================] - 0s 5ms/step - loss: 0.3042 - accuracy: 0.8885 - val_loss: 1.3108 - val_accuracy: 0.5347\n",
      "Epoch 78/80\n",
      "36/36 [==============================] - 0s 5ms/step - loss: 0.3057 - accuracy: 0.8902 - val_loss: 1.3268 - val_accuracy: 0.5278\n",
      "Epoch 79/80\n",
      "36/36 [==============================] - 0s 5ms/step - loss: 0.3108 - accuracy: 0.8920 - val_loss: 1.3160 - val_accuracy: 0.5556\n",
      "Epoch 80/80\n",
      "36/36 [==============================] - 0s 6ms/step - loss: 0.3056 - accuracy: 0.9007 - val_loss: 1.3356 - val_accuracy: 0.5625\n"
     ]
    }
   ],
   "source": [
    "history5 = model5.fit(X1_train_scaled, y1_train, \n",
    "                      validation_data=(X1_val_scaled, y1_val), \n",
    "                      batch_size=16, \n",
    "                      epochs=80)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c949242",
   "metadata": {},
   "source": [
    "**Different num of filters and kernel size**<br>\n",
    "- f1,k1: 16,5 -> 8,3\n",
    "- f2,k2: 32,7 -> 16,5\n",
    "- f3,k3: 32,7 -> 16,5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "0357b4fc",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"Conv1D_6\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " Conv1D_1 (Conv1D)           (None, 57, 8)             32        \n",
      "                                                                 \n",
      " BN1 (BatchNormalization)    (None, 57, 8)             32        \n",
      "                                                                 \n",
      " MaxPooling1 (MaxPooling1D)  (None, 28, 8)             0         \n",
      "                                                                 \n",
      " Conv1D_2 (Conv1D)           (None, 28, 16)            656       \n",
      "                                                                 \n",
      " BN2 (BatchNormalization)    (None, 28, 16)            64        \n",
      "                                                                 \n",
      " MaxPooling2 (MaxPooling1D)  (None, 14, 16)            0         \n",
      "                                                                 \n",
      " Conv1D_3 (Conv1D)           (None, 14, 16)            1296      \n",
      "                                                                 \n",
      " BN3 (BatchNormalization)    (None, 14, 16)            64        \n",
      "                                                                 \n",
      " MaxPooling3 (MaxPooling1D)  (None, 7, 16)             0         \n",
      "                                                                 \n",
      " Flatten (Flatten)           (None, 112)               0         \n",
      "                                                                 \n",
      " Dense_1 (Dense)             (None, 128)               14464     \n",
      "                                                                 \n",
      " Dropout_1 (Dropout)         (None, 128)               0         \n",
      "                                                                 \n",
      " Dense_2 (Dense)             (None, 64)                8256      \n",
      "                                                                 \n",
      " Dropout_2 (Dropout)         (None, 64)                0         \n",
      "                                                                 \n",
      " Softmax (Dense)             (None, 4)                 260       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 25,124\n",
      "Trainable params: 25,044\n",
      "Non-trainable params: 80\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "f1,k1,a1 = 8,3,'relu'\n",
    "f2,k2,a2 = 16,5,'relu'\n",
    "f3,k3,a3 = 16,5,'relu'\n",
    "d1,dr1,da1,r1 = 128,0.2,'relu',0.15\n",
    "d2,dr2,da2,r2 = 64,0.2,'relu',0.15\n",
    "num = 6\n",
    "\n",
    "model6 = modelBuilder3L(X1_train_scaled,\n",
    "                        f1,k1,a1,\n",
    "                        f2,k2,a2,\n",
    "                        f3,k3,a3,\n",
    "                        d1,dr1,da1,r1,\n",
    "                        d2,dr2,da2,r2,\n",
    "                        num)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "00148418",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/80\n",
      "36/36 [==============================] - 3s 14ms/step - loss: 1.8051 - accuracy: 0.2822 - val_loss: 1.3891 - val_accuracy: 0.2500\n",
      "Epoch 2/80\n",
      "36/36 [==============================] - 0s 6ms/step - loss: 1.4658 - accuracy: 0.3415 - val_loss: 1.3995 - val_accuracy: 0.2500\n",
      "Epoch 3/80\n",
      "36/36 [==============================] - 0s 5ms/step - loss: 1.3898 - accuracy: 0.3693 - val_loss: 1.4114 - val_accuracy: 0.2500\n",
      "Epoch 4/80\n",
      "36/36 [==============================] - 0s 5ms/step - loss: 1.2665 - accuracy: 0.4321 - val_loss: 1.4208 - val_accuracy: 0.2500\n",
      "Epoch 5/80\n",
      "36/36 [==============================] - 0s 5ms/step - loss: 1.2373 - accuracy: 0.4390 - val_loss: 1.4258 - val_accuracy: 0.2500\n",
      "Epoch 6/80\n",
      "36/36 [==============================] - 0s 6ms/step - loss: 1.2112 - accuracy: 0.4495 - val_loss: 1.4246 - val_accuracy: 0.2639\n",
      "Epoch 7/80\n",
      "36/36 [==============================] - 0s 6ms/step - loss: 1.1581 - accuracy: 0.4965 - val_loss: 1.4192 - val_accuracy: 0.3403\n",
      "Epoch 8/80\n",
      "36/36 [==============================] - 0s 5ms/step - loss: 1.1044 - accuracy: 0.5122 - val_loss: 1.4067 - val_accuracy: 0.3194\n",
      "Epoch 9/80\n",
      "36/36 [==============================] - 0s 6ms/step - loss: 1.1148 - accuracy: 0.4930 - val_loss: 1.3894 - val_accuracy: 0.3472\n",
      "Epoch 10/80\n",
      "36/36 [==============================] - 0s 6ms/step - loss: 1.0710 - accuracy: 0.5209 - val_loss: 1.3637 - val_accuracy: 0.3333\n",
      "Epoch 11/80\n",
      "36/36 [==============================] - 0s 5ms/step - loss: 1.0474 - accuracy: 0.5488 - val_loss: 1.3361 - val_accuracy: 0.3264\n",
      "Epoch 12/80\n",
      "36/36 [==============================] - 0s 7ms/step - loss: 1.0464 - accuracy: 0.5592 - val_loss: 1.2998 - val_accuracy: 0.3681\n",
      "Epoch 13/80\n",
      "36/36 [==============================] - 0s 6ms/step - loss: 0.9813 - accuracy: 0.5871 - val_loss: 1.2695 - val_accuracy: 0.3611\n",
      "Epoch 14/80\n",
      "36/36 [==============================] - 0s 6ms/step - loss: 0.9634 - accuracy: 0.5889 - val_loss: 1.2349 - val_accuracy: 0.3750\n",
      "Epoch 15/80\n",
      "36/36 [==============================] - 0s 6ms/step - loss: 0.9583 - accuracy: 0.5836 - val_loss: 1.1911 - val_accuracy: 0.4583\n",
      "Epoch 16/80\n",
      "36/36 [==============================] - 0s 5ms/step - loss: 0.9285 - accuracy: 0.5889 - val_loss: 1.1554 - val_accuracy: 0.4514\n",
      "Epoch 17/80\n",
      "36/36 [==============================] - 0s 5ms/step - loss: 0.9266 - accuracy: 0.5906 - val_loss: 1.1328 - val_accuracy: 0.4653\n",
      "Epoch 18/80\n",
      "36/36 [==============================] - 0s 5ms/step - loss: 0.9342 - accuracy: 0.5801 - val_loss: 1.1175 - val_accuracy: 0.4722\n",
      "Epoch 19/80\n",
      "36/36 [==============================] - 0s 6ms/step - loss: 0.9243 - accuracy: 0.5976 - val_loss: 1.1212 - val_accuracy: 0.4861\n",
      "Epoch 20/80\n",
      "36/36 [==============================] - 0s 5ms/step - loss: 0.8805 - accuracy: 0.6220 - val_loss: 1.1088 - val_accuracy: 0.4792\n",
      "Epoch 21/80\n",
      "36/36 [==============================] - 0s 5ms/step - loss: 0.8593 - accuracy: 0.6167 - val_loss: 1.0918 - val_accuracy: 0.5069\n",
      "Epoch 22/80\n",
      "36/36 [==============================] - 0s 6ms/step - loss: 0.8813 - accuracy: 0.6167 - val_loss: 1.1015 - val_accuracy: 0.5000\n",
      "Epoch 23/80\n",
      "36/36 [==============================] - 0s 6ms/step - loss: 0.8403 - accuracy: 0.6481 - val_loss: 1.0934 - val_accuracy: 0.5000\n",
      "Epoch 24/80\n",
      "36/36 [==============================] - 0s 6ms/step - loss: 0.8210 - accuracy: 0.6498 - val_loss: 1.1037 - val_accuracy: 0.5069\n",
      "Epoch 25/80\n",
      "36/36 [==============================] - 0s 6ms/step - loss: 0.8106 - accuracy: 0.6777 - val_loss: 1.1057 - val_accuracy: 0.5000\n",
      "Epoch 26/80\n",
      "36/36 [==============================] - 0s 9ms/step - loss: 0.7877 - accuracy: 0.6707 - val_loss: 1.1219 - val_accuracy: 0.4861\n",
      "Epoch 27/80\n",
      "36/36 [==============================] - 0s 6ms/step - loss: 0.8282 - accuracy: 0.6254 - val_loss: 1.1111 - val_accuracy: 0.5139\n",
      "Epoch 28/80\n",
      "36/36 [==============================] - 0s 6ms/step - loss: 0.8041 - accuracy: 0.6533 - val_loss: 1.1064 - val_accuracy: 0.5278\n",
      "Epoch 29/80\n",
      "36/36 [==============================] - 0s 6ms/step - loss: 0.7855 - accuracy: 0.6620 - val_loss: 1.1116 - val_accuracy: 0.5278\n",
      "Epoch 30/80\n",
      "36/36 [==============================] - 0s 6ms/step - loss: 0.7810 - accuracy: 0.6690 - val_loss: 1.1225 - val_accuracy: 0.5208\n",
      "Epoch 31/80\n",
      "36/36 [==============================] - 0s 6ms/step - loss: 0.7546 - accuracy: 0.6882 - val_loss: 1.1137 - val_accuracy: 0.5278\n",
      "Epoch 32/80\n",
      "36/36 [==============================] - 0s 6ms/step - loss: 0.7355 - accuracy: 0.6951 - val_loss: 1.1290 - val_accuracy: 0.5139\n",
      "Epoch 33/80\n",
      "36/36 [==============================] - 0s 6ms/step - loss: 0.7279 - accuracy: 0.6916 - val_loss: 1.1246 - val_accuracy: 0.5139\n",
      "Epoch 34/80\n",
      "36/36 [==============================] - 0s 7ms/step - loss: 0.7326 - accuracy: 0.7108 - val_loss: 1.1289 - val_accuracy: 0.5278\n",
      "Epoch 35/80\n",
      "36/36 [==============================] - 0s 5ms/step - loss: 0.7155 - accuracy: 0.7091 - val_loss: 1.1292 - val_accuracy: 0.5278\n",
      "Epoch 36/80\n",
      "36/36 [==============================] - 0s 6ms/step - loss: 0.6940 - accuracy: 0.7334 - val_loss: 1.1445 - val_accuracy: 0.5208\n",
      "Epoch 37/80\n",
      "36/36 [==============================] - 0s 5ms/step - loss: 0.7003 - accuracy: 0.7143 - val_loss: 1.1492 - val_accuracy: 0.5139\n",
      "Epoch 38/80\n",
      "36/36 [==============================] - 0s 5ms/step - loss: 0.6735 - accuracy: 0.7596 - val_loss: 1.1504 - val_accuracy: 0.5417\n",
      "Epoch 39/80\n",
      "36/36 [==============================] - 0s 5ms/step - loss: 0.7049 - accuracy: 0.6986 - val_loss: 1.1442 - val_accuracy: 0.5417\n",
      "Epoch 40/80\n",
      "36/36 [==============================] - 0s 5ms/step - loss: 0.6618 - accuracy: 0.7247 - val_loss: 1.1522 - val_accuracy: 0.5417\n",
      "Epoch 41/80\n",
      "36/36 [==============================] - 0s 8ms/step - loss: 0.6502 - accuracy: 0.7474 - val_loss: 1.1720 - val_accuracy: 0.5347\n",
      "Epoch 42/80\n",
      "36/36 [==============================] - 0s 6ms/step - loss: 0.6753 - accuracy: 0.7125 - val_loss: 1.1938 - val_accuracy: 0.5278\n",
      "Epoch 43/80\n",
      "36/36 [==============================] - 0s 5ms/step - loss: 0.5806 - accuracy: 0.7718 - val_loss: 1.1748 - val_accuracy: 0.5347\n",
      "Epoch 44/80\n",
      "36/36 [==============================] - 0s 6ms/step - loss: 0.6270 - accuracy: 0.7474 - val_loss: 1.1945 - val_accuracy: 0.5278\n",
      "Epoch 45/80\n",
      "36/36 [==============================] - 0s 5ms/step - loss: 0.5956 - accuracy: 0.7805 - val_loss: 1.1825 - val_accuracy: 0.5347\n",
      "Epoch 46/80\n",
      "36/36 [==============================] - 0s 6ms/step - loss: 0.5927 - accuracy: 0.7770 - val_loss: 1.1981 - val_accuracy: 0.5417\n",
      "Epoch 47/80\n",
      "36/36 [==============================] - 0s 5ms/step - loss: 0.5918 - accuracy: 0.7666 - val_loss: 1.2011 - val_accuracy: 0.5417\n",
      "Epoch 48/80\n",
      "36/36 [==============================] - 0s 5ms/step - loss: 0.5777 - accuracy: 0.7927 - val_loss: 1.2158 - val_accuracy: 0.5347\n",
      "Epoch 49/80\n",
      "36/36 [==============================] - 0s 6ms/step - loss: 0.5809 - accuracy: 0.7753 - val_loss: 1.2160 - val_accuracy: 0.5347\n",
      "Epoch 50/80\n",
      "36/36 [==============================] - 0s 6ms/step - loss: 0.5591 - accuracy: 0.7892 - val_loss: 1.2148 - val_accuracy: 0.5417\n",
      "Epoch 51/80\n",
      "36/36 [==============================] - 0s 5ms/step - loss: 0.5804 - accuracy: 0.7683 - val_loss: 1.1973 - val_accuracy: 0.5486\n",
      "Epoch 52/80\n",
      "36/36 [==============================] - 0s 5ms/step - loss: 0.5450 - accuracy: 0.7944 - val_loss: 1.2349 - val_accuracy: 0.5486\n",
      "Epoch 53/80\n",
      "36/36 [==============================] - 0s 6ms/step - loss: 0.5311 - accuracy: 0.8014 - val_loss: 1.2533 - val_accuracy: 0.5208\n",
      "Epoch 54/80\n",
      "36/36 [==============================] - 0s 8ms/step - loss: 0.5309 - accuracy: 0.7875 - val_loss: 1.2490 - val_accuracy: 0.5278\n",
      "Epoch 55/80\n",
      "36/36 [==============================] - 0s 5ms/step - loss: 0.5154 - accuracy: 0.8171 - val_loss: 1.2701 - val_accuracy: 0.5069\n",
      "Epoch 56/80\n",
      "36/36 [==============================] - 0s 6ms/step - loss: 0.5209 - accuracy: 0.8101 - val_loss: 1.2493 - val_accuracy: 0.5139\n",
      "Epoch 57/80\n",
      "36/36 [==============================] - 0s 5ms/step - loss: 0.5024 - accuracy: 0.8136 - val_loss: 1.2353 - val_accuracy: 0.5208\n",
      "Epoch 58/80\n",
      "36/36 [==============================] - 0s 6ms/step - loss: 0.4971 - accuracy: 0.8101 - val_loss: 1.2856 - val_accuracy: 0.5069\n",
      "Epoch 59/80\n",
      "36/36 [==============================] - 0s 5ms/step - loss: 0.5042 - accuracy: 0.7979 - val_loss: 1.2976 - val_accuracy: 0.5069\n",
      "Epoch 60/80\n",
      "36/36 [==============================] - 0s 5ms/step - loss: 0.4981 - accuracy: 0.7909 - val_loss: 1.3033 - val_accuracy: 0.5139\n",
      "Epoch 61/80\n",
      "36/36 [==============================] - 0s 6ms/step - loss: 0.4811 - accuracy: 0.8206 - val_loss: 1.3271 - val_accuracy: 0.5069\n",
      "Epoch 62/80\n",
      "36/36 [==============================] - 0s 5ms/step - loss: 0.4477 - accuracy: 0.8171 - val_loss: 1.3355 - val_accuracy: 0.5278\n",
      "Epoch 63/80\n",
      "36/36 [==============================] - 0s 5ms/step - loss: 0.4476 - accuracy: 0.8380 - val_loss: 1.3705 - val_accuracy: 0.5139\n",
      "Epoch 64/80\n",
      "36/36 [==============================] - 0s 5ms/step - loss: 0.4238 - accuracy: 0.8571 - val_loss: 1.3903 - val_accuracy: 0.5139\n",
      "Epoch 65/80\n",
      "36/36 [==============================] - 0s 6ms/step - loss: 0.4603 - accuracy: 0.8188 - val_loss: 1.3296 - val_accuracy: 0.5208\n",
      "Epoch 66/80\n",
      "36/36 [==============================] - 0s 6ms/step - loss: 0.4361 - accuracy: 0.8328 - val_loss: 1.3246 - val_accuracy: 0.5278\n",
      "Epoch 67/80\n",
      "36/36 [==============================] - 0s 8ms/step - loss: 0.4066 - accuracy: 0.8537 - val_loss: 1.3617 - val_accuracy: 0.5278\n",
      "Epoch 68/80\n",
      "36/36 [==============================] - 0s 6ms/step - loss: 0.4158 - accuracy: 0.8554 - val_loss: 1.3798 - val_accuracy: 0.5208\n",
      "Epoch 69/80\n",
      "36/36 [==============================] - 0s 5ms/step - loss: 0.4018 - accuracy: 0.8571 - val_loss: 1.3792 - val_accuracy: 0.5139\n",
      "Epoch 70/80\n",
      "36/36 [==============================] - 0s 6ms/step - loss: 0.3922 - accuracy: 0.8606 - val_loss: 1.3792 - val_accuracy: 0.5208\n",
      "Epoch 71/80\n",
      "36/36 [==============================] - 0s 6ms/step - loss: 0.3850 - accuracy: 0.8554 - val_loss: 1.4284 - val_accuracy: 0.5208\n",
      "Epoch 72/80\n",
      "36/36 [==============================] - 0s 5ms/step - loss: 0.3909 - accuracy: 0.8484 - val_loss: 1.4480 - val_accuracy: 0.5139\n",
      "Epoch 73/80\n",
      "36/36 [==============================] - 0s 5ms/step - loss: 0.3845 - accuracy: 0.8537 - val_loss: 1.4402 - val_accuracy: 0.5139\n",
      "Epoch 74/80\n",
      "36/36 [==============================] - 0s 5ms/step - loss: 0.3671 - accuracy: 0.8676 - val_loss: 1.4610 - val_accuracy: 0.5069\n",
      "Epoch 75/80\n",
      "36/36 [==============================] - 0s 5ms/step - loss: 0.3890 - accuracy: 0.8537 - val_loss: 1.4517 - val_accuracy: 0.5417\n",
      "Epoch 76/80\n",
      "36/36 [==============================] - 0s 6ms/step - loss: 0.3367 - accuracy: 0.8728 - val_loss: 1.4720 - val_accuracy: 0.5417\n",
      "Epoch 77/80\n",
      "36/36 [==============================] - 0s 5ms/step - loss: 0.3589 - accuracy: 0.8868 - val_loss: 1.5246 - val_accuracy: 0.5278\n",
      "Epoch 78/80\n",
      "36/36 [==============================] - 0s 6ms/step - loss: 0.3344 - accuracy: 0.8815 - val_loss: 1.5315 - val_accuracy: 0.5278\n",
      "Epoch 79/80\n",
      "36/36 [==============================] - 0s 5ms/step - loss: 0.3264 - accuracy: 0.8780 - val_loss: 1.5467 - val_accuracy: 0.5417\n",
      "Epoch 80/80\n",
      "36/36 [==============================] - 0s 5ms/step - loss: 0.3280 - accuracy: 0.8902 - val_loss: 1.5458 - val_accuracy: 0.5417\n"
     ]
    }
   ],
   "source": [
    "history6 = model6.fit(X1_train_scaled, y1_train, \n",
    "                      validation_data=(X1_val_scaled, y1_val), \n",
    "                      batch_size=16, \n",
    "                      epochs=80)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd33b1e6",
   "metadata": {},
   "source": [
    "- f1,k1: 16,5 -> 8,3\n",
    "- f2,k2: 32,7 -> 16,5\n",
    "- f3,k3: 32,7 -> 32,7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "bb3a20cc",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"Conv1D_7\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " Conv1D_1 (Conv1D)           (None, 57, 8)             32        \n",
      "                                                                 \n",
      " BN1 (BatchNormalization)    (None, 57, 8)             32        \n",
      "                                                                 \n",
      " MaxPooling1 (MaxPooling1D)  (None, 28, 8)             0         \n",
      "                                                                 \n",
      " Conv1D_2 (Conv1D)           (None, 28, 16)            656       \n",
      "                                                                 \n",
      " BN2 (BatchNormalization)    (None, 28, 16)            64        \n",
      "                                                                 \n",
      " MaxPooling2 (MaxPooling1D)  (None, 14, 16)            0         \n",
      "                                                                 \n",
      " Conv1D_3 (Conv1D)           (None, 14, 32)            3616      \n",
      "                                                                 \n",
      " BN3 (BatchNormalization)    (None, 14, 32)            128       \n",
      "                                                                 \n",
      " MaxPooling3 (MaxPooling1D)  (None, 7, 32)             0         \n",
      "                                                                 \n",
      " Flatten (Flatten)           (None, 224)               0         \n",
      "                                                                 \n",
      " Dense_1 (Dense)             (None, 128)               28800     \n",
      "                                                                 \n",
      " Dropout_1 (Dropout)         (None, 128)               0         \n",
      "                                                                 \n",
      " Dense_2 (Dense)             (None, 64)                8256      \n",
      "                                                                 \n",
      " Dropout_2 (Dropout)         (None, 64)                0         \n",
      "                                                                 \n",
      " Softmax (Dense)             (None, 4)                 260       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 41,844\n",
      "Trainable params: 41,732\n",
      "Non-trainable params: 112\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "f1,k1,a1 = 8,3,'relu'\n",
    "f2,k2,a2 = 16,5,'relu'\n",
    "f3,k3,a3 = 32,7,'relu'\n",
    "d1,dr1,da1,r1 = 128,0.2,'relu',0.15\n",
    "d2,dr2,da2,r2 = 64,0.2,'relu',0.15\n",
    "num = 7\n",
    "\n",
    "model7 = modelBuilder3L(X1_train_scaled,\n",
    "                        f1,k1,a1,\n",
    "                        f2,k2,a2,\n",
    "                        f3,k3,a3,\n",
    "                        d1,dr1,da1,r1,\n",
    "                        d2,dr2,da2,r2,\n",
    "                        num)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "fd93420f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/80\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "36/36 [==============================] - 3s 16ms/step - loss: 1.7490 - accuracy: 0.2805 - val_loss: 1.3868 - val_accuracy: 0.2500\n",
      "Epoch 2/80\n",
      "36/36 [==============================] - 0s 6ms/step - loss: 1.3401 - accuracy: 0.3990 - val_loss: 1.3882 - val_accuracy: 0.2500\n",
      "Epoch 3/80\n",
      "36/36 [==============================] - 0s 6ms/step - loss: 1.2311 - accuracy: 0.4233 - val_loss: 1.3862 - val_accuracy: 0.2500\n",
      "Epoch 4/80\n",
      "36/36 [==============================] - 0s 8ms/step - loss: 1.1598 - accuracy: 0.4669 - val_loss: 1.3808 - val_accuracy: 0.2500\n",
      "Epoch 5/80\n",
      "36/36 [==============================] - 0s 5ms/step - loss: 1.0728 - accuracy: 0.5174 - val_loss: 1.3709 - val_accuracy: 0.2569\n",
      "Epoch 6/80\n",
      "36/36 [==============================] - 0s 5ms/step - loss: 1.0453 - accuracy: 0.5261 - val_loss: 1.3528 - val_accuracy: 0.2778\n",
      "Epoch 7/80\n",
      "36/36 [==============================] - 0s 7ms/step - loss: 1.0319 - accuracy: 0.5453 - val_loss: 1.3254 - val_accuracy: 0.3264\n",
      "Epoch 8/80\n",
      "36/36 [==============================] - 0s 6ms/step - loss: 0.9706 - accuracy: 0.5714 - val_loss: 1.2934 - val_accuracy: 0.3750\n",
      "Epoch 9/80\n",
      "36/36 [==============================] - 0s 6ms/step - loss: 0.9553 - accuracy: 0.5836 - val_loss: 1.2505 - val_accuracy: 0.4444\n",
      "Epoch 10/80\n",
      "36/36 [==============================] - 0s 6ms/step - loss: 0.9171 - accuracy: 0.5749 - val_loss: 1.2008 - val_accuracy: 0.4722\n",
      "Epoch 11/80\n",
      "36/36 [==============================] - 0s 5ms/step - loss: 0.8856 - accuracy: 0.6132 - val_loss: 1.1609 - val_accuracy: 0.4444\n",
      "Epoch 12/80\n",
      "36/36 [==============================] - 0s 5ms/step - loss: 0.8749 - accuracy: 0.6150 - val_loss: 1.1225 - val_accuracy: 0.4722\n",
      "Epoch 13/80\n",
      "36/36 [==============================] - 0s 5ms/step - loss: 0.8589 - accuracy: 0.6220 - val_loss: 1.0926 - val_accuracy: 0.4444\n",
      "Epoch 14/80\n",
      "36/36 [==============================] - 0s 5ms/step - loss: 0.8406 - accuracy: 0.6132 - val_loss: 1.0715 - val_accuracy: 0.4861\n",
      "Epoch 15/80\n",
      "36/36 [==============================] - 0s 5ms/step - loss: 0.8242 - accuracy: 0.6533 - val_loss: 1.0620 - val_accuracy: 0.4861\n",
      "Epoch 16/80\n",
      "36/36 [==============================] - 0s 5ms/step - loss: 0.8090 - accuracy: 0.6672 - val_loss: 1.0629 - val_accuracy: 0.4792\n",
      "Epoch 17/80\n",
      "36/36 [==============================] - 0s 5ms/step - loss: 0.7928 - accuracy: 0.6411 - val_loss: 1.0623 - val_accuracy: 0.4722\n",
      "Epoch 18/80\n",
      "36/36 [==============================] - 0s 5ms/step - loss: 0.8034 - accuracy: 0.6603 - val_loss: 1.0642 - val_accuracy: 0.4861\n",
      "Epoch 19/80\n",
      "36/36 [==============================] - 0s 5ms/step - loss: 0.7571 - accuracy: 0.6707 - val_loss: 1.0584 - val_accuracy: 0.4861\n",
      "Epoch 20/80\n",
      "36/36 [==============================] - 0s 5ms/step - loss: 0.7463 - accuracy: 0.6777 - val_loss: 1.0668 - val_accuracy: 0.4861\n",
      "Epoch 21/80\n",
      "36/36 [==============================] - 0s 9ms/step - loss: 0.7037 - accuracy: 0.7160 - val_loss: 1.0770 - val_accuracy: 0.4931\n",
      "Epoch 22/80\n",
      "36/36 [==============================] - 0s 6ms/step - loss: 0.7503 - accuracy: 0.6742 - val_loss: 1.0774 - val_accuracy: 0.4861\n",
      "Epoch 23/80\n",
      "36/36 [==============================] - 0s 5ms/step - loss: 0.7006 - accuracy: 0.7213 - val_loss: 1.0871 - val_accuracy: 0.5000\n",
      "Epoch 24/80\n",
      "36/36 [==============================] - 0s 6ms/step - loss: 0.6725 - accuracy: 0.7317 - val_loss: 1.0857 - val_accuracy: 0.5000\n",
      "Epoch 25/80\n",
      "36/36 [==============================] - 0s 6ms/step - loss: 0.6847 - accuracy: 0.7213 - val_loss: 1.0991 - val_accuracy: 0.5139\n",
      "Epoch 26/80\n",
      "36/36 [==============================] - 0s 6ms/step - loss: 0.6359 - accuracy: 0.7613 - val_loss: 1.1142 - val_accuracy: 0.5139\n",
      "Epoch 27/80\n",
      "36/36 [==============================] - 0s 5ms/step - loss: 0.6221 - accuracy: 0.7631 - val_loss: 1.1056 - val_accuracy: 0.5069\n",
      "Epoch 28/80\n",
      "36/36 [==============================] - 0s 5ms/step - loss: 0.6116 - accuracy: 0.7683 - val_loss: 1.1277 - val_accuracy: 0.5069\n",
      "Epoch 29/80\n",
      "36/36 [==============================] - 0s 5ms/step - loss: 0.6166 - accuracy: 0.7596 - val_loss: 1.1516 - val_accuracy: 0.5069\n",
      "Epoch 30/80\n",
      "36/36 [==============================] - 0s 6ms/step - loss: 0.6003 - accuracy: 0.7387 - val_loss: 1.1323 - val_accuracy: 0.5139\n",
      "Epoch 31/80\n",
      "36/36 [==============================] - 0s 5ms/step - loss: 0.5694 - accuracy: 0.7735 - val_loss: 1.1483 - val_accuracy: 0.4931\n",
      "Epoch 32/80\n",
      "36/36 [==============================] - 0s 6ms/step - loss: 0.5609 - accuracy: 0.7683 - val_loss: 1.1691 - val_accuracy: 0.5000\n",
      "Epoch 33/80\n",
      "36/36 [==============================] - 0s 6ms/step - loss: 0.5674 - accuracy: 0.7718 - val_loss: 1.1677 - val_accuracy: 0.5000\n",
      "Epoch 34/80\n",
      "36/36 [==============================] - 0s 5ms/step - loss: 0.5478 - accuracy: 0.7962 - val_loss: 1.1948 - val_accuracy: 0.5069\n",
      "Epoch 35/80\n",
      "36/36 [==============================] - 0s 5ms/step - loss: 0.5184 - accuracy: 0.7840 - val_loss: 1.1752 - val_accuracy: 0.5000\n",
      "Epoch 36/80\n",
      "36/36 [==============================] - 0s 5ms/step - loss: 0.5177 - accuracy: 0.8153 - val_loss: 1.1801 - val_accuracy: 0.5000\n",
      "Epoch 37/80\n",
      "36/36 [==============================] - 0s 5ms/step - loss: 0.5272 - accuracy: 0.7909 - val_loss: 1.1937 - val_accuracy: 0.5000\n",
      "Epoch 38/80\n",
      "36/36 [==============================] - 0s 9ms/step - loss: 0.5064 - accuracy: 0.7979 - val_loss: 1.2237 - val_accuracy: 0.5000\n",
      "Epoch 39/80\n",
      "36/36 [==============================] - 0s 6ms/step - loss: 0.4736 - accuracy: 0.8328 - val_loss: 1.2202 - val_accuracy: 0.5069\n",
      "Epoch 40/80\n",
      "36/36 [==============================] - 0s 5ms/step - loss: 0.4998 - accuracy: 0.8258 - val_loss: 1.2253 - val_accuracy: 0.5069\n",
      "Epoch 41/80\n",
      "36/36 [==============================] - 0s 5ms/step - loss: 0.4873 - accuracy: 0.8275 - val_loss: 1.2232 - val_accuracy: 0.5139\n",
      "Epoch 42/80\n",
      "36/36 [==============================] - 0s 5ms/step - loss: 0.4705 - accuracy: 0.8380 - val_loss: 1.2352 - val_accuracy: 0.5069\n",
      "Epoch 43/80\n",
      "36/36 [==============================] - 0s 5ms/step - loss: 0.4442 - accuracy: 0.8328 - val_loss: 1.2602 - val_accuracy: 0.5000\n",
      "Epoch 44/80\n",
      "36/36 [==============================] - 0s 5ms/step - loss: 0.4485 - accuracy: 0.8397 - val_loss: 1.2395 - val_accuracy: 0.5139\n",
      "Epoch 45/80\n",
      "36/36 [==============================] - 0s 5ms/step - loss: 0.4447 - accuracy: 0.8362 - val_loss: 1.2546 - val_accuracy: 0.5069\n",
      "Epoch 46/80\n",
      "36/36 [==============================] - 0s 5ms/step - loss: 0.4181 - accuracy: 0.8484 - val_loss: 1.2653 - val_accuracy: 0.5069\n",
      "Epoch 47/80\n",
      "36/36 [==============================] - 0s 5ms/step - loss: 0.4154 - accuracy: 0.8397 - val_loss: 1.3194 - val_accuracy: 0.5069\n",
      "Epoch 48/80\n",
      "36/36 [==============================] - 0s 5ms/step - loss: 0.3888 - accuracy: 0.8589 - val_loss: 1.2907 - val_accuracy: 0.5000\n",
      "Epoch 49/80\n",
      "36/36 [==============================] - 0s 5ms/step - loss: 0.3914 - accuracy: 0.8641 - val_loss: 1.3316 - val_accuracy: 0.5000\n",
      "Epoch 50/80\n",
      "36/36 [==============================] - 0s 5ms/step - loss: 0.3768 - accuracy: 0.8659 - val_loss: 1.3396 - val_accuracy: 0.4931\n",
      "Epoch 51/80\n",
      "36/36 [==============================] - 0s 5ms/step - loss: 0.3930 - accuracy: 0.8519 - val_loss: 1.3583 - val_accuracy: 0.4861\n",
      "Epoch 52/80\n",
      "36/36 [==============================] - 0s 5ms/step - loss: 0.3722 - accuracy: 0.8659 - val_loss: 1.3584 - val_accuracy: 0.4861\n",
      "Epoch 53/80\n",
      "36/36 [==============================] - 0s 5ms/step - loss: 0.3489 - accuracy: 0.8902 - val_loss: 1.3720 - val_accuracy: 0.4861\n",
      "Epoch 54/80\n",
      "36/36 [==============================] - 0s 5ms/step - loss: 0.3681 - accuracy: 0.8676 - val_loss: 1.3747 - val_accuracy: 0.4861\n",
      "Epoch 55/80\n",
      "36/36 [==============================] - 0s 8ms/step - loss: 0.3298 - accuracy: 0.8920 - val_loss: 1.4159 - val_accuracy: 0.4792\n",
      "Epoch 56/80\n",
      "36/36 [==============================] - 0s 5ms/step - loss: 0.3386 - accuracy: 0.8955 - val_loss: 1.4236 - val_accuracy: 0.4931\n",
      "Epoch 57/80\n",
      "36/36 [==============================] - 0s 6ms/step - loss: 0.3316 - accuracy: 0.8780 - val_loss: 1.3996 - val_accuracy: 0.5139\n",
      "Epoch 58/80\n",
      "36/36 [==============================] - 0s 5ms/step - loss: 0.3258 - accuracy: 0.8937 - val_loss: 1.4360 - val_accuracy: 0.5069\n",
      "Epoch 59/80\n",
      "36/36 [==============================] - 0s 5ms/step - loss: 0.3131 - accuracy: 0.8990 - val_loss: 1.4452 - val_accuracy: 0.5069\n",
      "Epoch 60/80\n",
      "36/36 [==============================] - 0s 5ms/step - loss: 0.2895 - accuracy: 0.9199 - val_loss: 1.4523 - val_accuracy: 0.5069\n",
      "Epoch 61/80\n",
      "36/36 [==============================] - 0s 5ms/step - loss: 0.3009 - accuracy: 0.9077 - val_loss: 1.4795 - val_accuracy: 0.5069\n",
      "Epoch 62/80\n",
      "36/36 [==============================] - 0s 6ms/step - loss: 0.3231 - accuracy: 0.8885 - val_loss: 1.5116 - val_accuracy: 0.5000\n",
      "Epoch 63/80\n",
      "36/36 [==============================] - 0s 6ms/step - loss: 0.2972 - accuracy: 0.8972 - val_loss: 1.4910 - val_accuracy: 0.5069\n",
      "Epoch 64/80\n",
      "36/36 [==============================] - 0s 5ms/step - loss: 0.2655 - accuracy: 0.9251 - val_loss: 1.5441 - val_accuracy: 0.5000\n",
      "Epoch 65/80\n",
      "36/36 [==============================] - 0s 13ms/step - loss: 0.2587 - accuracy: 0.9216 - val_loss: 1.4952 - val_accuracy: 0.5139\n",
      "Epoch 66/80\n",
      "36/36 [==============================] - 0s 9ms/step - loss: 0.2541 - accuracy: 0.9303 - val_loss: 1.4987 - val_accuracy: 0.5139\n",
      "Epoch 67/80\n",
      "36/36 [==============================] - 0s 9ms/step - loss: 0.2618 - accuracy: 0.9146 - val_loss: 1.5903 - val_accuracy: 0.4931\n",
      "Epoch 68/80\n",
      "36/36 [==============================] - 0s 6ms/step - loss: 0.2284 - accuracy: 0.9268 - val_loss: 1.6195 - val_accuracy: 0.5069\n",
      "Epoch 69/80\n",
      "36/36 [==============================] - 0s 8ms/step - loss: 0.2275 - accuracy: 0.9321 - val_loss: 1.5934 - val_accuracy: 0.5069\n",
      "Epoch 70/80\n",
      "36/36 [==============================] - 0s 5ms/step - loss: 0.2173 - accuracy: 0.9477 - val_loss: 1.6291 - val_accuracy: 0.5069\n",
      "Epoch 71/80\n",
      "36/36 [==============================] - 0s 5ms/step - loss: 0.2318 - accuracy: 0.9268 - val_loss: 1.6419 - val_accuracy: 0.5069\n",
      "Epoch 72/80\n",
      "36/36 [==============================] - 0s 6ms/step - loss: 0.2244 - accuracy: 0.9251 - val_loss: 1.6536 - val_accuracy: 0.5139\n",
      "Epoch 73/80\n",
      "36/36 [==============================] - 0s 5ms/step - loss: 0.2194 - accuracy: 0.9286 - val_loss: 1.6870 - val_accuracy: 0.5139\n",
      "Epoch 74/80\n",
      "36/36 [==============================] - 0s 5ms/step - loss: 0.1986 - accuracy: 0.9425 - val_loss: 1.7014 - val_accuracy: 0.5000\n",
      "Epoch 75/80\n",
      "36/36 [==============================] - 0s 5ms/step - loss: 0.2068 - accuracy: 0.9390 - val_loss: 1.7488 - val_accuracy: 0.5000\n",
      "Epoch 76/80\n",
      "36/36 [==============================] - 0s 5ms/step - loss: 0.1814 - accuracy: 0.9547 - val_loss: 1.7664 - val_accuracy: 0.4931\n",
      "Epoch 77/80\n",
      "36/36 [==============================] - 0s 5ms/step - loss: 0.2030 - accuracy: 0.9425 - val_loss: 1.7646 - val_accuracy: 0.4931\n",
      "Epoch 78/80\n",
      "36/36 [==============================] - 0s 7ms/step - loss: 0.1894 - accuracy: 0.9530 - val_loss: 1.7203 - val_accuracy: 0.5069\n",
      "Epoch 79/80\n",
      "36/36 [==============================] - 0s 5ms/step - loss: 0.1872 - accuracy: 0.9390 - val_loss: 1.7545 - val_accuracy: 0.5000\n",
      "Epoch 80/80\n",
      "36/36 [==============================] - 0s 5ms/step - loss: 0.1878 - accuracy: 0.9460 - val_loss: 1.7909 - val_accuracy: 0.4931\n"
     ]
    }
   ],
   "source": [
    "history7 = model7.fit(X1_train_scaled, y1_train, \n",
    "                      validation_data=(X1_val_scaled, y1_val), \n",
    "                      batch_size=16, \n",
    "                      epochs=80)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da4d1c7f",
   "metadata": {},
   "source": [
    "- f1,k1: 16,5 -> 16,5\n",
    "- f2,k2: 32,7 -> 16,5\n",
    "- f3,k3: 32,7 -> 32,7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "a2bf3529",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"Conv1D_8\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " Conv1D_1 (Conv1D)           (None, 57, 16)            96        \n",
      "                                                                 \n",
      " BN1 (BatchNormalization)    (None, 57, 16)            64        \n",
      "                                                                 \n",
      " MaxPooling1 (MaxPooling1D)  (None, 28, 16)            0         \n",
      "                                                                 \n",
      " Conv1D_2 (Conv1D)           (None, 28, 16)            1296      \n",
      "                                                                 \n",
      " BN2 (BatchNormalization)    (None, 28, 16)            64        \n",
      "                                                                 \n",
      " MaxPooling2 (MaxPooling1D)  (None, 14, 16)            0         \n",
      "                                                                 \n",
      " Conv1D_3 (Conv1D)           (None, 14, 32)            3616      \n",
      "                                                                 \n",
      " BN3 (BatchNormalization)    (None, 14, 32)            128       \n",
      "                                                                 \n",
      " MaxPooling3 (MaxPooling1D)  (None, 7, 32)             0         \n",
      "                                                                 \n",
      " Flatten (Flatten)           (None, 224)               0         \n",
      "                                                                 \n",
      " Dense_1 (Dense)             (None, 128)               28800     \n",
      "                                                                 \n",
      " Dropout_1 (Dropout)         (None, 128)               0         \n",
      "                                                                 \n",
      " Dense_2 (Dense)             (None, 64)                8256      \n",
      "                                                                 \n",
      " Dropout_2 (Dropout)         (None, 64)                0         \n",
      "                                                                 \n",
      " Softmax (Dense)             (None, 4)                 260       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 42,580\n",
      "Trainable params: 42,452\n",
      "Non-trainable params: 128\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "f1,k1,a1 = 16,5,'relu'\n",
    "f2,k2,a2 = 16,5,'relu'\n",
    "f3,k3,a3 = 32,7,'relu'\n",
    "d1,dr1,da1,r1 = 128,0.2,'relu',0.15\n",
    "d2,dr2,da2,r2 = 64,0.2,'relu',0.15\n",
    "num = 8\n",
    "\n",
    "model8 = modelBuilder3L(X1_train_scaled,\n",
    "                        f1,k1,a1,\n",
    "                        f2,k2,a2,\n",
    "                        f3,k3,a3,\n",
    "                        d1,dr1,da1,r1,\n",
    "                        d2,dr2,da2,r2,\n",
    "                        num)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "d5549275",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/80\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "36/36 [==============================] - 4s 14ms/step - loss: 1.6382 - accuracy: 0.2927 - val_loss: 1.3826 - val_accuracy: 0.3264\n",
      "Epoch 2/80\n",
      "36/36 [==============================] - 0s 7ms/step - loss: 1.3884 - accuracy: 0.3467 - val_loss: 1.3837 - val_accuracy: 0.2431\n",
      "Epoch 3/80\n",
      "36/36 [==============================] - 0s 6ms/step - loss: 1.2441 - accuracy: 0.4286 - val_loss: 1.3856 - val_accuracy: 0.2431\n",
      "Epoch 4/80\n",
      "36/36 [==============================] - 0s 6ms/step - loss: 1.1695 - accuracy: 0.4599 - val_loss: 1.3867 - val_accuracy: 0.2778\n",
      "Epoch 5/80\n",
      "36/36 [==============================] - 0s 6ms/step - loss: 1.1128 - accuracy: 0.5052 - val_loss: 1.3880 - val_accuracy: 0.3403\n",
      "Epoch 6/80\n",
      "36/36 [==============================] - 0s 8ms/step - loss: 1.0936 - accuracy: 0.5139 - val_loss: 1.3908 - val_accuracy: 0.3472\n",
      "Epoch 7/80\n",
      "36/36 [==============================] - 0s 6ms/step - loss: 1.0265 - accuracy: 0.5436 - val_loss: 1.3953 - val_accuracy: 0.3542\n",
      "Epoch 8/80\n",
      "36/36 [==============================] - 0s 6ms/step - loss: 0.9483 - accuracy: 0.5836 - val_loss: 1.3854 - val_accuracy: 0.3681\n",
      "Epoch 9/80\n",
      "36/36 [==============================] - 0s 8ms/step - loss: 0.9709 - accuracy: 0.5557 - val_loss: 1.3710 - val_accuracy: 0.3333\n",
      "Epoch 10/80\n",
      "36/36 [==============================] - 0s 5ms/step - loss: 0.9588 - accuracy: 0.5645 - val_loss: 1.3485 - val_accuracy: 0.3542\n",
      "Epoch 11/80\n",
      "36/36 [==============================] - 0s 5ms/step - loss: 0.9281 - accuracy: 0.5836 - val_loss: 1.3018 - val_accuracy: 0.3750\n",
      "Epoch 12/80\n",
      "36/36 [==============================] - 0s 7ms/step - loss: 0.9152 - accuracy: 0.5976 - val_loss: 1.2504 - val_accuracy: 0.3889\n",
      "Epoch 13/80\n",
      "36/36 [==============================] - 0s 5ms/step - loss: 0.8949 - accuracy: 0.6080 - val_loss: 1.1811 - val_accuracy: 0.4444\n",
      "Epoch 14/80\n",
      "36/36 [==============================] - 0s 5ms/step - loss: 0.8727 - accuracy: 0.6220 - val_loss: 1.1251 - val_accuracy: 0.4792\n",
      "Epoch 15/80\n",
      "36/36 [==============================] - 0s 6ms/step - loss: 0.8460 - accuracy: 0.6463 - val_loss: 1.0788 - val_accuracy: 0.4583\n",
      "Epoch 16/80\n",
      "36/36 [==============================] - 0s 6ms/step - loss: 0.8182 - accuracy: 0.6429 - val_loss: 1.0524 - val_accuracy: 0.5000\n",
      "Epoch 17/80\n",
      "36/36 [==============================] - 0s 5ms/step - loss: 0.8103 - accuracy: 0.6551 - val_loss: 1.0396 - val_accuracy: 0.5208\n",
      "Epoch 18/80\n",
      "36/36 [==============================] - 0s 6ms/step - loss: 0.7850 - accuracy: 0.6847 - val_loss: 1.0404 - val_accuracy: 0.5208\n",
      "Epoch 19/80\n",
      "36/36 [==============================] - 0s 7ms/step - loss: 0.7395 - accuracy: 0.7160 - val_loss: 1.0457 - val_accuracy: 0.5208\n",
      "Epoch 20/80\n",
      "36/36 [==============================] - 0s 6ms/step - loss: 0.7258 - accuracy: 0.6969 - val_loss: 1.0505 - val_accuracy: 0.5139\n",
      "Epoch 21/80\n",
      "36/36 [==============================] - 0s 6ms/step - loss: 0.7115 - accuracy: 0.6986 - val_loss: 1.0570 - val_accuracy: 0.5208\n",
      "Epoch 22/80\n",
      "36/36 [==============================] - 0s 5ms/step - loss: 0.7171 - accuracy: 0.7021 - val_loss: 1.0534 - val_accuracy: 0.5278\n",
      "Epoch 23/80\n",
      "36/36 [==============================] - 0s 5ms/step - loss: 0.6864 - accuracy: 0.7021 - val_loss: 1.0685 - val_accuracy: 0.5278\n",
      "Epoch 24/80\n",
      "36/36 [==============================] - 0s 8ms/step - loss: 0.6811 - accuracy: 0.7369 - val_loss: 1.0972 - val_accuracy: 0.5139\n",
      "Epoch 25/80\n",
      "36/36 [==============================] - 0s 5ms/step - loss: 0.6667 - accuracy: 0.7300 - val_loss: 1.1102 - val_accuracy: 0.5139\n",
      "Epoch 26/80\n",
      "36/36 [==============================] - 0s 9ms/step - loss: 0.6295 - accuracy: 0.7491 - val_loss: 1.0844 - val_accuracy: 0.5278\n",
      "Epoch 27/80\n",
      "36/36 [==============================] - 0s 5ms/step - loss: 0.6530 - accuracy: 0.7369 - val_loss: 1.0803 - val_accuracy: 0.5625\n",
      "Epoch 28/80\n",
      "36/36 [==============================] - 0s 8ms/step - loss: 0.6150 - accuracy: 0.7474 - val_loss: 1.1161 - val_accuracy: 0.5000\n",
      "Epoch 29/80\n",
      "36/36 [==============================] - 0s 6ms/step - loss: 0.5793 - accuracy: 0.7631 - val_loss: 1.1223 - val_accuracy: 0.5208\n",
      "Epoch 30/80\n",
      "36/36 [==============================] - 0s 5ms/step - loss: 0.5942 - accuracy: 0.7840 - val_loss: 1.1308 - val_accuracy: 0.5208\n",
      "Epoch 31/80\n",
      "36/36 [==============================] - 0s 5ms/step - loss: 0.5669 - accuracy: 0.7909 - val_loss: 1.1376 - val_accuracy: 0.5139\n",
      "Epoch 32/80\n",
      "36/36 [==============================] - 0s 5ms/step - loss: 0.5811 - accuracy: 0.7631 - val_loss: 1.1286 - val_accuracy: 0.5278\n",
      "Epoch 33/80\n",
      "36/36 [==============================] - 0s 5ms/step - loss: 0.5204 - accuracy: 0.8136 - val_loss: 1.1331 - val_accuracy: 0.5139\n",
      "Epoch 34/80\n",
      "36/36 [==============================] - 0s 5ms/step - loss: 0.4974 - accuracy: 0.8171 - val_loss: 1.1276 - val_accuracy: 0.5278\n",
      "Epoch 35/80\n",
      "36/36 [==============================] - 0s 6ms/step - loss: 0.5007 - accuracy: 0.7979 - val_loss: 1.1447 - val_accuracy: 0.5278\n",
      "Epoch 36/80\n",
      "36/36 [==============================] - 0s 7ms/step - loss: 0.4773 - accuracy: 0.8345 - val_loss: 1.1476 - val_accuracy: 0.5208\n",
      "Epoch 37/80\n",
      "36/36 [==============================] - 0s 6ms/step - loss: 0.4669 - accuracy: 0.8275 - val_loss: 1.1803 - val_accuracy: 0.5069\n",
      "Epoch 38/80\n",
      "36/36 [==============================] - 0s 6ms/step - loss: 0.4829 - accuracy: 0.8101 - val_loss: 1.1756 - val_accuracy: 0.5139\n",
      "Epoch 39/80\n",
      "36/36 [==============================] - 0s 6ms/step - loss: 0.4910 - accuracy: 0.8171 - val_loss: 1.1818 - val_accuracy: 0.5000\n",
      "Epoch 40/80\n",
      "36/36 [==============================] - 0s 5ms/step - loss: 0.4623 - accuracy: 0.8275 - val_loss: 1.2013 - val_accuracy: 0.5000\n",
      "Epoch 41/80\n",
      "36/36 [==============================] - 0s 5ms/step - loss: 0.4279 - accuracy: 0.8589 - val_loss: 1.2036 - val_accuracy: 0.5000\n",
      "Epoch 42/80\n",
      "36/36 [==============================] - 0s 8ms/step - loss: 0.4204 - accuracy: 0.8310 - val_loss: 1.1913 - val_accuracy: 0.5069\n",
      "Epoch 43/80\n",
      "36/36 [==============================] - 0s 5ms/step - loss: 0.4397 - accuracy: 0.8310 - val_loss: 1.2193 - val_accuracy: 0.5069\n",
      "Epoch 44/80\n",
      "36/36 [==============================] - 0s 6ms/step - loss: 0.3970 - accuracy: 0.8589 - val_loss: 1.2435 - val_accuracy: 0.5278\n",
      "Epoch 45/80\n",
      "36/36 [==============================] - 0s 6ms/step - loss: 0.3909 - accuracy: 0.8641 - val_loss: 1.2338 - val_accuracy: 0.5139\n",
      "Epoch 46/80\n",
      "36/36 [==============================] - 0s 6ms/step - loss: 0.3971 - accuracy: 0.8571 - val_loss: 1.2367 - val_accuracy: 0.5000\n",
      "Epoch 47/80\n",
      "36/36 [==============================] - 0s 5ms/step - loss: 0.3735 - accuracy: 0.8711 - val_loss: 1.2219 - val_accuracy: 0.5069\n",
      "Epoch 48/80\n",
      "36/36 [==============================] - 0s 5ms/step - loss: 0.3549 - accuracy: 0.8798 - val_loss: 1.2395 - val_accuracy: 0.5069\n",
      "Epoch 49/80\n",
      "36/36 [==============================] - 0s 6ms/step - loss: 0.3617 - accuracy: 0.8728 - val_loss: 1.2967 - val_accuracy: 0.5000\n",
      "Epoch 50/80\n",
      "36/36 [==============================] - 0s 6ms/step - loss: 0.3687 - accuracy: 0.8763 - val_loss: 1.2494 - val_accuracy: 0.4931\n",
      "Epoch 51/80\n",
      "36/36 [==============================] - 0s 6ms/step - loss: 0.3736 - accuracy: 0.8693 - val_loss: 1.2714 - val_accuracy: 0.4931\n",
      "Epoch 52/80\n",
      "36/36 [==============================] - 0s 5ms/step - loss: 0.3532 - accuracy: 0.8815 - val_loss: 1.2956 - val_accuracy: 0.5069\n",
      "Epoch 53/80\n",
      "36/36 [==============================] - 0s 6ms/step - loss: 0.3229 - accuracy: 0.8902 - val_loss: 1.3001 - val_accuracy: 0.4931\n",
      "Epoch 54/80\n",
      "36/36 [==============================] - 0s 6ms/step - loss: 0.3128 - accuracy: 0.9024 - val_loss: 1.3191 - val_accuracy: 0.5000\n",
      "Epoch 55/80\n",
      "36/36 [==============================] - 0s 5ms/step - loss: 0.3063 - accuracy: 0.8990 - val_loss: 1.2903 - val_accuracy: 0.5139\n",
      "Epoch 56/80\n",
      "36/36 [==============================] - 0s 6ms/step - loss: 0.3143 - accuracy: 0.8955 - val_loss: 1.3355 - val_accuracy: 0.5069\n",
      "Epoch 57/80\n",
      "36/36 [==============================] - 0s 5ms/step - loss: 0.2829 - accuracy: 0.9321 - val_loss: 1.3490 - val_accuracy: 0.5069\n",
      "Epoch 58/80\n",
      "36/36 [==============================] - 0s 5ms/step - loss: 0.2850 - accuracy: 0.9059 - val_loss: 1.3464 - val_accuracy: 0.4861\n",
      "Epoch 59/80\n",
      "36/36 [==============================] - 0s 9ms/step - loss: 0.2720 - accuracy: 0.9129 - val_loss: 1.3535 - val_accuracy: 0.5139\n",
      "Epoch 60/80\n",
      "36/36 [==============================] - 0s 6ms/step - loss: 0.2631 - accuracy: 0.9111 - val_loss: 1.3865 - val_accuracy: 0.4931\n",
      "Epoch 61/80\n",
      "36/36 [==============================] - 0s 5ms/step - loss: 0.2541 - accuracy: 0.9233 - val_loss: 1.3962 - val_accuracy: 0.5139\n",
      "Epoch 62/80\n",
      "36/36 [==============================] - 0s 6ms/step - loss: 0.2735 - accuracy: 0.9094 - val_loss: 1.4294 - val_accuracy: 0.5000\n",
      "Epoch 63/80\n",
      "36/36 [==============================] - 0s 6ms/step - loss: 0.2238 - accuracy: 0.9338 - val_loss: 1.4053 - val_accuracy: 0.5069\n",
      "Epoch 64/80\n",
      "36/36 [==============================] - 0s 6ms/step - loss: 0.2230 - accuracy: 0.9443 - val_loss: 1.4238 - val_accuracy: 0.4931\n",
      "Epoch 65/80\n",
      "36/36 [==============================] - 0s 6ms/step - loss: 0.2409 - accuracy: 0.9251 - val_loss: 1.4150 - val_accuracy: 0.4931\n",
      "Epoch 66/80\n",
      "36/36 [==============================] - 0s 7ms/step - loss: 0.2243 - accuracy: 0.9338 - val_loss: 1.4459 - val_accuracy: 0.4931\n",
      "Epoch 67/80\n",
      "36/36 [==============================] - 0s 6ms/step - loss: 0.2022 - accuracy: 0.9495 - val_loss: 1.4779 - val_accuracy: 0.4792\n",
      "Epoch 68/80\n",
      "36/36 [==============================] - 0s 5ms/step - loss: 0.2064 - accuracy: 0.9408 - val_loss: 1.4630 - val_accuracy: 0.5139\n",
      "Epoch 69/80\n",
      "36/36 [==============================] - 0s 6ms/step - loss: 0.1950 - accuracy: 0.9460 - val_loss: 1.4703 - val_accuracy: 0.4931\n",
      "Epoch 70/80\n",
      "36/36 [==============================] - 0s 6ms/step - loss: 0.2242 - accuracy: 0.9303 - val_loss: 1.5100 - val_accuracy: 0.4931\n",
      "Epoch 71/80\n",
      "36/36 [==============================] - 0s 6ms/step - loss: 0.2012 - accuracy: 0.9390 - val_loss: 1.5308 - val_accuracy: 0.4931\n",
      "Epoch 72/80\n",
      "36/36 [==============================] - 0s 5ms/step - loss: 0.1875 - accuracy: 0.9512 - val_loss: 1.5553 - val_accuracy: 0.4861\n",
      "Epoch 73/80\n",
      "36/36 [==============================] - 0s 5ms/step - loss: 0.1874 - accuracy: 0.9443 - val_loss: 1.5473 - val_accuracy: 0.4861\n",
      "Epoch 74/80\n",
      "36/36 [==============================] - 0s 6ms/step - loss: 0.1988 - accuracy: 0.9408 - val_loss: 1.5517 - val_accuracy: 0.4861\n",
      "Epoch 75/80\n",
      "36/36 [==============================] - 0s 6ms/step - loss: 0.1828 - accuracy: 0.9460 - val_loss: 1.5632 - val_accuracy: 0.4861\n",
      "Epoch 76/80\n",
      "36/36 [==============================] - 0s 7ms/step - loss: 0.1976 - accuracy: 0.9303 - val_loss: 1.5923 - val_accuracy: 0.4861\n",
      "Epoch 77/80\n",
      "36/36 [==============================] - 0s 9ms/step - loss: 0.1601 - accuracy: 0.9512 - val_loss: 1.5898 - val_accuracy: 0.5000\n",
      "Epoch 78/80\n",
      "36/36 [==============================] - 0s 5ms/step - loss: 0.1559 - accuracy: 0.9617 - val_loss: 1.6132 - val_accuracy: 0.5000\n",
      "Epoch 79/80\n",
      "36/36 [==============================] - 0s 6ms/step - loss: 0.1687 - accuracy: 0.9564 - val_loss: 1.6299 - val_accuracy: 0.5069\n",
      "Epoch 80/80\n",
      "36/36 [==============================] - 0s 5ms/step - loss: 0.1398 - accuracy: 0.9669 - val_loss: 1.6258 - val_accuracy: 0.4931\n"
     ]
    }
   ],
   "source": [
    "history8 = model8.fit(X1_train_scaled, y1_train, \n",
    "                      validation_data=(X1_val_scaled, y1_val), \n",
    "                      batch_size=16, \n",
    "                      epochs=80)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "133387af",
   "metadata": {},
   "source": [
    "**Simplify the networks**</br>\n",
    "Overfitting problems might be due to the complexity of the neural networks. Here only 2 conv2D layers are used, decreasing 1 layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "352aeb23",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def modelBuilder2L(X_train,\n",
    "                   f1,k1,a1,\n",
    "                   f2,k2,a2,                \n",
    "                   d1,dr1,da1,r1,\n",
    "                   d2,dr2,da2,r2,\n",
    "                   num):\n",
    "    '''\n",
    "    args:\n",
    "    \n",
    "    X_train: training data\n",
    "    f1,k1,a1: num of filters, filter size and activation func of 1st conv1D layer\n",
    "    f2,k2,a2: num of filters, filter size and activation func of 2nd conv1D layer\n",
    "    d1,dr1,da1, r1: num of units, dropout, activation func and regularizer para of 1st fully connected layer\n",
    "    d2,dr2,da2, r2: num of units, dropout, activation func and regularizer para  of 2nd fully connected layer\n",
    "    num: integer for distinguishing different model\n",
    "    \n",
    "    return:\n",
    "    model\n",
    "    '''\n",
    "    model = Sequential(name=\"Conv1D_\"+str(num))\n",
    "\n",
    "    m,n = X_train.shape\n",
    "    \n",
    "    #L1\n",
    "    model.add(Conv1D(filters = f1, kernel_size = k1, input_shape = (n,1), padding = 'same', activation = a1, name ='Conv1D_1'))\n",
    "    model.add(BatchNormalization(name = \"BN1\"))\n",
    "    model.add(MaxPooling1D(name = \"MaxPooling1\"))\n",
    "    #L2\n",
    "    model.add(Conv1D(filters = f2, kernel_size = k2, activation = a2, padding='same', name = \"Conv1D_2\"))\n",
    "    model.add(BatchNormalization(name = \"BN2\"))\n",
    "    model.add(MaxPooling1D(name = \"MaxPooling2\"))\n",
    "\n",
    "    #Flatten output\n",
    "    model.add(Flatten(name = \"Flatten\"))\n",
    "\n",
    "    #Fully connected layer 3\n",
    "    model.add(Dense(d1, activation = da1, name = \"Dense_1\"))\n",
    "    #Prevent overfitting\n",
    "    model.add(Dropout(dr1, name = \"Dropout_1\"))\n",
    "\n",
    "    #Fully connected layer 4 \n",
    "    model.add(Dense(d2, activation = da2, name = \"Dense_2\"))\n",
    "    #Prevent overfitting\n",
    "    model.add(Dropout(dr2, name = \"Dropout_2\"))\n",
    "\n",
    "    #Output layer\n",
    "    model.add(Dense(4, activation = 'softmax', name = \"Softmax\"))\n",
    "\n",
    "    #model compiling\n",
    "    optimizer = Adam(learning_rate=0.0001)\n",
    "    model.compile(loss = 'sparse_categorical_crossentropy', optimizer = optimizer, metrics=['accuracy'])\n",
    "    model.summary()\n",
    "    \n",
    "    return model   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "1ea98757",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"Conv1D_9\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " Conv1D_1 (Conv1D)           (None, 57, 16)            96        \n",
      "                                                                 \n",
      " BN1 (BatchNormalization)    (None, 57, 16)            64        \n",
      "                                                                 \n",
      " MaxPooling1 (MaxPooling1D)  (None, 28, 16)            0         \n",
      "                                                                 \n",
      " Conv1D_2 (Conv1D)           (None, 28, 32)            3616      \n",
      "                                                                 \n",
      " BN2 (BatchNormalization)    (None, 28, 32)            128       \n",
      "                                                                 \n",
      " MaxPooling2 (MaxPooling1D)  (None, 14, 32)            0         \n",
      "                                                                 \n",
      " Flatten (Flatten)           (None, 448)               0         \n",
      "                                                                 \n",
      " Dense_1 (Dense)             (None, 128)               57472     \n",
      "                                                                 \n",
      " Dropout_1 (Dropout)         (None, 128)               0         \n",
      "                                                                 \n",
      " Dense_2 (Dense)             (None, 64)                8256      \n",
      "                                                                 \n",
      " Dropout_2 (Dropout)         (None, 64)                0         \n",
      "                                                                 \n",
      " Softmax (Dense)             (None, 4)                 260       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 69,892\n",
      "Trainable params: 69,796\n",
      "Non-trainable params: 96\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "f1,k1,a1 = 16,5,'relu'\n",
    "f2,k2,a2 = 32,7,'relu'\n",
    "d1,dr1,da1,r1 = 128,0.2,'relu',0.15\n",
    "d2,dr2,da2,r2 = 64,0.2,'relu',0.15\n",
    "num = 9\n",
    "\n",
    "model9 = modelBuilder2L(X1_train_scaled,\n",
    "                        f1,k1,a1,\n",
    "                        f2,k2,a2,\n",
    "                        d1,dr1,da1,r1,\n",
    "                        d2,dr2,da2,r2,\n",
    "                        num)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "a79257c3",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/80\n",
      "36/36 [==============================] - 3s 16ms/step - loss: 1.6346 - accuracy: 0.2875 - val_loss: 1.3836 - val_accuracy: 0.3264\n",
      "Epoch 2/80\n",
      "36/36 [==============================] - 0s 6ms/step - loss: 1.3796 - accuracy: 0.3693 - val_loss: 1.3801 - val_accuracy: 0.2778\n",
      "Epoch 3/80\n",
      "36/36 [==============================] - 0s 5ms/step - loss: 1.2477 - accuracy: 0.4634 - val_loss: 1.3754 - val_accuracy: 0.2917\n",
      "Epoch 4/80\n",
      "36/36 [==============================] - 0s 5ms/step - loss: 1.1579 - accuracy: 0.4808 - val_loss: 1.3681 - val_accuracy: 0.2986\n",
      "Epoch 5/80\n",
      "36/36 [==============================] - 0s 5ms/step - loss: 1.1247 - accuracy: 0.5017 - val_loss: 1.3598 - val_accuracy: 0.2986\n",
      "Epoch 6/80\n",
      "36/36 [==============================] - 0s 6ms/step - loss: 1.0630 - accuracy: 0.5366 - val_loss: 1.3492 - val_accuracy: 0.3333\n",
      "Epoch 7/80\n",
      "36/36 [==============================] - 0s 5ms/step - loss: 0.9906 - accuracy: 0.5679 - val_loss: 1.3367 - val_accuracy: 0.3472\n",
      "Epoch 8/80\n",
      "36/36 [==============================] - 0s 5ms/step - loss: 0.9850 - accuracy: 0.5645 - val_loss: 1.3199 - val_accuracy: 0.3681\n",
      "Epoch 9/80\n",
      "36/36 [==============================] - 0s 5ms/step - loss: 0.9451 - accuracy: 0.5889 - val_loss: 1.3008 - val_accuracy: 0.3958\n",
      "Epoch 10/80\n",
      "36/36 [==============================] - 0s 5ms/step - loss: 0.9097 - accuracy: 0.6185 - val_loss: 1.2787 - val_accuracy: 0.4236\n",
      "Epoch 11/80\n",
      "36/36 [==============================] - 0s 5ms/step - loss: 0.8907 - accuracy: 0.6359 - val_loss: 1.2506 - val_accuracy: 0.4514\n",
      "Epoch 12/80\n",
      "36/36 [==============================] - 0s 5ms/step - loss: 0.8668 - accuracy: 0.6289 - val_loss: 1.2025 - val_accuracy: 0.4653\n",
      "Epoch 13/80\n",
      "36/36 [==============================] - 0s 5ms/step - loss: 0.8238 - accuracy: 0.6829 - val_loss: 1.1715 - val_accuracy: 0.4653\n",
      "Epoch 14/80\n",
      "36/36 [==============================] - 0s 5ms/step - loss: 0.7890 - accuracy: 0.6882 - val_loss: 1.1431 - val_accuracy: 0.4722\n",
      "Epoch 15/80\n",
      "36/36 [==============================] - 0s 6ms/step - loss: 0.7930 - accuracy: 0.6603 - val_loss: 1.1271 - val_accuracy: 0.4861\n",
      "Epoch 16/80\n",
      "36/36 [==============================] - 0s 5ms/step - loss: 0.7877 - accuracy: 0.6707 - val_loss: 1.1035 - val_accuracy: 0.5208\n",
      "Epoch 17/80\n",
      "36/36 [==============================] - 0s 5ms/step - loss: 0.7605 - accuracy: 0.7108 - val_loss: 1.0909 - val_accuracy: 0.5139\n",
      "Epoch 18/80\n",
      "36/36 [==============================] - 0s 5ms/step - loss: 0.7239 - accuracy: 0.6916 - val_loss: 1.0980 - val_accuracy: 0.5000\n",
      "Epoch 19/80\n",
      "36/36 [==============================] - 0s 5ms/step - loss: 0.6769 - accuracy: 0.7387 - val_loss: 1.0904 - val_accuracy: 0.5069\n",
      "Epoch 20/80\n",
      "36/36 [==============================] - 0s 10ms/step - loss: 0.7184 - accuracy: 0.7021 - val_loss: 1.1084 - val_accuracy: 0.4861\n",
      "Epoch 21/80\n",
      "36/36 [==============================] - 0s 5ms/step - loss: 0.6834 - accuracy: 0.7300 - val_loss: 1.1236 - val_accuracy: 0.4861\n",
      "Epoch 22/80\n",
      "36/36 [==============================] - 0s 5ms/step - loss: 0.6639 - accuracy: 0.7108 - val_loss: 1.1125 - val_accuracy: 0.4722\n",
      "Epoch 23/80\n",
      "36/36 [==============================] - 0s 5ms/step - loss: 0.6326 - accuracy: 0.7613 - val_loss: 1.1138 - val_accuracy: 0.5000\n",
      "Epoch 24/80\n",
      "36/36 [==============================] - 0s 6ms/step - loss: 0.6146 - accuracy: 0.7422 - val_loss: 1.1389 - val_accuracy: 0.4931\n",
      "Epoch 25/80\n",
      "36/36 [==============================] - 0s 5ms/step - loss: 0.6245 - accuracy: 0.7491 - val_loss: 1.1331 - val_accuracy: 0.5000\n",
      "Epoch 26/80\n",
      "36/36 [==============================] - 0s 6ms/step - loss: 0.6112 - accuracy: 0.7753 - val_loss: 1.1466 - val_accuracy: 0.4722\n",
      "Epoch 27/80\n",
      "36/36 [==============================] - 0s 6ms/step - loss: 0.5799 - accuracy: 0.7753 - val_loss: 1.1449 - val_accuracy: 0.4861\n",
      "Epoch 28/80\n",
      "36/36 [==============================] - 0s 5ms/step - loss: 0.5655 - accuracy: 0.7700 - val_loss: 1.1483 - val_accuracy: 0.4722\n",
      "Epoch 29/80\n",
      "36/36 [==============================] - 0s 5ms/step - loss: 0.5271 - accuracy: 0.7909 - val_loss: 1.1336 - val_accuracy: 0.4931\n",
      "Epoch 30/80\n",
      "36/36 [==============================] - 0s 5ms/step - loss: 0.5588 - accuracy: 0.7787 - val_loss: 1.1502 - val_accuracy: 0.4931\n",
      "Epoch 31/80\n",
      "36/36 [==============================] - 0s 5ms/step - loss: 0.5177 - accuracy: 0.8206 - val_loss: 1.1521 - val_accuracy: 0.5000\n",
      "Epoch 32/80\n",
      "36/36 [==============================] - 0s 6ms/step - loss: 0.5082 - accuracy: 0.8066 - val_loss: 1.1697 - val_accuracy: 0.5069\n",
      "Epoch 33/80\n",
      "36/36 [==============================] - 0s 5ms/step - loss: 0.4927 - accuracy: 0.8188 - val_loss: 1.1649 - val_accuracy: 0.5000\n",
      "Epoch 34/80\n",
      "36/36 [==============================] - 0s 6ms/step - loss: 0.4810 - accuracy: 0.8101 - val_loss: 1.1641 - val_accuracy: 0.5139\n",
      "Epoch 35/80\n",
      "36/36 [==============================] - 0s 5ms/step - loss: 0.4815 - accuracy: 0.8258 - val_loss: 1.1845 - val_accuracy: 0.5208\n",
      "Epoch 36/80\n",
      "36/36 [==============================] - 0s 5ms/step - loss: 0.5075 - accuracy: 0.8101 - val_loss: 1.1693 - val_accuracy: 0.5139\n",
      "Epoch 37/80\n",
      "36/36 [==============================] - 0s 10ms/step - loss: 0.4503 - accuracy: 0.8449 - val_loss: 1.1813 - val_accuracy: 0.5139\n",
      "Epoch 38/80\n",
      "36/36 [==============================] - 0s 5ms/step - loss: 0.4288 - accuracy: 0.8571 - val_loss: 1.1814 - val_accuracy: 0.5208\n",
      "Epoch 39/80\n",
      "36/36 [==============================] - 0s 5ms/step - loss: 0.4037 - accuracy: 0.8589 - val_loss: 1.2061 - val_accuracy: 0.5139\n",
      "Epoch 40/80\n",
      "36/36 [==============================] - 0s 5ms/step - loss: 0.4042 - accuracy: 0.8728 - val_loss: 1.2512 - val_accuracy: 0.5000\n",
      "Epoch 41/80\n",
      "36/36 [==============================] - 0s 5ms/step - loss: 0.3964 - accuracy: 0.8537 - val_loss: 1.2538 - val_accuracy: 0.5069\n",
      "Epoch 42/80\n",
      "36/36 [==============================] - 0s 5ms/step - loss: 0.3798 - accuracy: 0.8763 - val_loss: 1.2599 - val_accuracy: 0.5000\n",
      "Epoch 43/80\n",
      "36/36 [==============================] - 0s 5ms/step - loss: 0.4028 - accuracy: 0.8571 - val_loss: 1.2636 - val_accuracy: 0.5208\n",
      "Epoch 44/80\n",
      "36/36 [==============================] - 0s 6ms/step - loss: 0.3876 - accuracy: 0.8780 - val_loss: 1.2801 - val_accuracy: 0.5139\n",
      "Epoch 45/80\n",
      "36/36 [==============================] - 0s 5ms/step - loss: 0.3509 - accuracy: 0.8780 - val_loss: 1.3096 - val_accuracy: 0.5208\n",
      "Epoch 46/80\n",
      "36/36 [==============================] - 0s 5ms/step - loss: 0.3542 - accuracy: 0.8955 - val_loss: 1.2937 - val_accuracy: 0.5139\n",
      "Epoch 47/80\n",
      "36/36 [==============================] - 0s 5ms/step - loss: 0.3447 - accuracy: 0.8693 - val_loss: 1.3038 - val_accuracy: 0.4931\n",
      "Epoch 48/80\n",
      "36/36 [==============================] - 0s 5ms/step - loss: 0.3264 - accuracy: 0.8920 - val_loss: 1.3032 - val_accuracy: 0.5000\n",
      "Epoch 49/80\n",
      "36/36 [==============================] - 0s 5ms/step - loss: 0.3370 - accuracy: 0.8798 - val_loss: 1.2644 - val_accuracy: 0.5139\n",
      "Epoch 50/80\n",
      "36/36 [==============================] - 0s 6ms/step - loss: 0.3171 - accuracy: 0.8937 - val_loss: 1.3545 - val_accuracy: 0.5139\n",
      "Epoch 51/80\n",
      "36/36 [==============================] - 0s 6ms/step - loss: 0.3015 - accuracy: 0.9024 - val_loss: 1.3200 - val_accuracy: 0.5000\n",
      "Epoch 52/80\n",
      "36/36 [==============================] - 0s 5ms/step - loss: 0.2967 - accuracy: 0.9077 - val_loss: 1.3318 - val_accuracy: 0.5208\n",
      "Epoch 53/80\n",
      "36/36 [==============================] - 0s 12ms/step - loss: 0.2464 - accuracy: 0.9390 - val_loss: 1.3274 - val_accuracy: 0.5069\n",
      "Epoch 54/80\n",
      "36/36 [==============================] - 0s 9ms/step - loss: 0.2611 - accuracy: 0.9355 - val_loss: 1.3563 - val_accuracy: 0.5069\n",
      "Epoch 55/80\n",
      "36/36 [==============================] - 0s 10ms/step - loss: 0.2755 - accuracy: 0.9111 - val_loss: 1.3604 - val_accuracy: 0.5000\n",
      "Epoch 56/80\n",
      "36/36 [==============================] - 0s 7ms/step - loss: 0.2764 - accuracy: 0.9146 - val_loss: 1.3746 - val_accuracy: 0.4931\n",
      "Epoch 57/80\n",
      "36/36 [==============================] - 0s 5ms/step - loss: 0.2556 - accuracy: 0.9251 - val_loss: 1.3816 - val_accuracy: 0.5208\n",
      "Epoch 58/80\n",
      "36/36 [==============================] - 0s 5ms/step - loss: 0.2387 - accuracy: 0.9251 - val_loss: 1.3529 - val_accuracy: 0.4861\n",
      "Epoch 59/80\n",
      "36/36 [==============================] - 0s 5ms/step - loss: 0.2447 - accuracy: 0.9216 - val_loss: 1.3962 - val_accuracy: 0.5208\n",
      "Epoch 60/80\n",
      "36/36 [==============================] - 0s 5ms/step - loss: 0.2368 - accuracy: 0.9303 - val_loss: 1.3928 - val_accuracy: 0.5000\n",
      "Epoch 61/80\n",
      "36/36 [==============================] - 0s 6ms/step - loss: 0.2210 - accuracy: 0.9303 - val_loss: 1.4316 - val_accuracy: 0.5208\n",
      "Epoch 62/80\n",
      "36/36 [==============================] - 0s 5ms/step - loss: 0.2209 - accuracy: 0.9303 - val_loss: 1.4392 - val_accuracy: 0.5208\n",
      "Epoch 63/80\n",
      "36/36 [==============================] - 0s 5ms/step - loss: 0.2286 - accuracy: 0.9321 - val_loss: 1.4016 - val_accuracy: 0.4861\n",
      "Epoch 64/80\n",
      "36/36 [==============================] - 0s 5ms/step - loss: 0.2226 - accuracy: 0.9303 - val_loss: 1.4070 - val_accuracy: 0.4861\n",
      "Epoch 65/80\n",
      "36/36 [==============================] - 0s 5ms/step - loss: 0.1947 - accuracy: 0.9547 - val_loss: 1.4473 - val_accuracy: 0.4931\n",
      "Epoch 66/80\n",
      "36/36 [==============================] - 0s 5ms/step - loss: 0.2143 - accuracy: 0.9303 - val_loss: 1.4410 - val_accuracy: 0.4861\n",
      "Epoch 67/80\n",
      "36/36 [==============================] - 0s 5ms/step - loss: 0.2056 - accuracy: 0.9460 - val_loss: 1.4802 - val_accuracy: 0.5208\n",
      "Epoch 68/80\n",
      "36/36 [==============================] - 0s 5ms/step - loss: 0.1994 - accuracy: 0.9408 - val_loss: 1.4606 - val_accuracy: 0.4792\n",
      "Epoch 69/80\n",
      "36/36 [==============================] - 0s 6ms/step - loss: 0.1865 - accuracy: 0.9477 - val_loss: 1.5197 - val_accuracy: 0.4861\n",
      "Epoch 70/80\n",
      "36/36 [==============================] - 0s 5ms/step - loss: 0.1840 - accuracy: 0.9547 - val_loss: 1.5150 - val_accuracy: 0.5208\n",
      "Epoch 71/80\n",
      "36/36 [==============================] - 0s 6ms/step - loss: 0.1425 - accuracy: 0.9686 - val_loss: 1.5189 - val_accuracy: 0.4861\n",
      "Epoch 72/80\n",
      "36/36 [==============================] - 0s 5ms/step - loss: 0.1624 - accuracy: 0.9530 - val_loss: 1.5525 - val_accuracy: 0.5000\n",
      "Epoch 73/80\n",
      "36/36 [==============================] - 0s 7ms/step - loss: 0.1408 - accuracy: 0.9634 - val_loss: 1.5488 - val_accuracy: 0.4931\n",
      "Epoch 74/80\n",
      "36/36 [==============================] - 0s 6ms/step - loss: 0.1648 - accuracy: 0.9512 - val_loss: 1.5455 - val_accuracy: 0.4861\n",
      "Epoch 75/80\n",
      "36/36 [==============================] - 0s 5ms/step - loss: 0.1488 - accuracy: 0.9582 - val_loss: 1.5991 - val_accuracy: 0.4792\n",
      "Epoch 76/80\n",
      "36/36 [==============================] - 0s 5ms/step - loss: 0.1394 - accuracy: 0.9704 - val_loss: 1.5877 - val_accuracy: 0.4861\n",
      "Epoch 77/80\n",
      "36/36 [==============================] - 0s 5ms/step - loss: 0.1389 - accuracy: 0.9634 - val_loss: 1.6413 - val_accuracy: 0.4792\n",
      "Epoch 78/80\n",
      "36/36 [==============================] - 0s 5ms/step - loss: 0.1411 - accuracy: 0.9512 - val_loss: 1.6561 - val_accuracy: 0.5069\n",
      "Epoch 79/80\n",
      "36/36 [==============================] - 0s 5ms/step - loss: 0.1373 - accuracy: 0.9652 - val_loss: 1.6207 - val_accuracy: 0.4861\n",
      "Epoch 80/80\n",
      "36/36 [==============================] - 0s 5ms/step - loss: 0.1337 - accuracy: 0.9704 - val_loss: 1.6334 - val_accuracy: 0.5139\n"
     ]
    }
   ],
   "source": [
    "history9 = model9.fit(X1_train_scaled, y1_train, \n",
    "                      validation_data=(X1_val_scaled, y1_val), \n",
    "                      batch_size=16, \n",
    "                      epochs=80)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e74296e6",
   "metadata": {},
   "source": [
    "Simplifying the layer leads to the underfitting of the model, which lowers the training accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "c2ffb932",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"Conv1D_10\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " Conv1D_1 (Conv1D)           (None, 57, 8)             32        \n",
      "                                                                 \n",
      " BN1 (BatchNormalization)    (None, 57, 8)             32        \n",
      "                                                                 \n",
      " MaxPooling1 (MaxPooling1D)  (None, 28, 8)             0         \n",
      "                                                                 \n",
      " Conv1D_2 (Conv1D)           (None, 28, 16)            656       \n",
      "                                                                 \n",
      " BN2 (BatchNormalization)    (None, 28, 16)            64        \n",
      "                                                                 \n",
      " MaxPooling2 (MaxPooling1D)  (None, 14, 16)            0         \n",
      "                                                                 \n",
      " Flatten (Flatten)           (None, 224)               0         \n",
      "                                                                 \n",
      " Dense_1 (Dense)             (None, 64)                14400     \n",
      "                                                                 \n",
      " Dropout_1 (Dropout)         (None, 64)                0         \n",
      "                                                                 \n",
      " Dense_2 (Dense)             (None, 32)                2080      \n",
      "                                                                 \n",
      " Dropout_2 (Dropout)         (None, 32)                0         \n",
      "                                                                 \n",
      " Softmax (Dense)             (None, 4)                 132       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 17,396\n",
      "Trainable params: 17,348\n",
      "Non-trainable params: 48\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "f1,k1,a1 = 8,3,'relu'\n",
    "f2,k2,a2 = 16,5,'relu'\n",
    "d1,dr1,da1,r1 = 64,0.2,'relu',0.15\n",
    "d2,dr2,da2,r2 = 32,0.2,'relu',0.15\n",
    "num = 10\n",
    "\n",
    "model10 = modelBuilder2L(X1_train_scaled,\n",
    "                        f1,k1,a1,\n",
    "                        f2,k2,a2,\n",
    "                        d1,dr1,da1,r1,\n",
    "                        d2,dr2,da2,r2,\n",
    "                        num)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "11db717a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/80\n",
      "36/36 [==============================] - 2s 11ms/step - loss: 2.0590 - accuracy: 0.2387 - val_loss: 1.3733 - val_accuracy: 0.2708\n",
      "Epoch 2/80\n",
      "36/36 [==============================] - 0s 5ms/step - loss: 1.4984 - accuracy: 0.3293 - val_loss: 1.3687 - val_accuracy: 0.2500\n",
      "Epoch 3/80\n",
      "36/36 [==============================] - 0s 5ms/step - loss: 1.3783 - accuracy: 0.3833 - val_loss: 1.3762 - val_accuracy: 0.2500\n",
      "Epoch 4/80\n",
      "36/36 [==============================] - 0s 5ms/step - loss: 1.2202 - accuracy: 0.4512 - val_loss: 1.3863 - val_accuracy: 0.2500\n",
      "Epoch 5/80\n",
      "36/36 [==============================] - 0s 4ms/step - loss: 1.2126 - accuracy: 0.4443 - val_loss: 1.3980 - val_accuracy: 0.2500\n",
      "Epoch 6/80\n",
      "36/36 [==============================] - 0s 4ms/step - loss: 1.2354 - accuracy: 0.4477 - val_loss: 1.4039 - val_accuracy: 0.2500\n",
      "Epoch 7/80\n",
      "36/36 [==============================] - 0s 4ms/step - loss: 1.1743 - accuracy: 0.4739 - val_loss: 1.3997 - val_accuracy: 0.2639\n",
      "Epoch 8/80\n",
      "36/36 [==============================] - 0s 4ms/step - loss: 1.1487 - accuracy: 0.4878 - val_loss: 1.3869 - val_accuracy: 0.2639\n",
      "Epoch 9/80\n",
      "36/36 [==============================] - 0s 4ms/step - loss: 1.1382 - accuracy: 0.4652 - val_loss: 1.3634 - val_accuracy: 0.2847\n",
      "Epoch 10/80\n",
      "36/36 [==============================] - 0s 4ms/step - loss: 1.1102 - accuracy: 0.4861 - val_loss: 1.3336 - val_accuracy: 0.3125\n",
      "Epoch 11/80\n",
      "36/36 [==============================] - 0s 4ms/step - loss: 1.1030 - accuracy: 0.4965 - val_loss: 1.2894 - val_accuracy: 0.3264\n",
      "Epoch 12/80\n",
      "36/36 [==============================] - 0s 4ms/step - loss: 1.0670 - accuracy: 0.5418 - val_loss: 1.2472 - val_accuracy: 0.4167\n",
      "Epoch 13/80\n",
      "36/36 [==============================] - 0s 4ms/step - loss: 1.0840 - accuracy: 0.4983 - val_loss: 1.2101 - val_accuracy: 0.4306\n",
      "Epoch 14/80\n",
      "36/36 [==============================] - 0s 4ms/step - loss: 1.0513 - accuracy: 0.5348 - val_loss: 1.1825 - val_accuracy: 0.4653\n",
      "Epoch 15/80\n",
      "36/36 [==============================] - 0s 7ms/step - loss: 1.0279 - accuracy: 0.5348 - val_loss: 1.1691 - val_accuracy: 0.4722\n",
      "Epoch 16/80\n",
      "36/36 [==============================] - 0s 4ms/step - loss: 1.0357 - accuracy: 0.5348 - val_loss: 1.1635 - val_accuracy: 0.4792\n",
      "Epoch 17/80\n",
      "36/36 [==============================] - 0s 4ms/step - loss: 1.0003 - accuracy: 0.5540 - val_loss: 1.1625 - val_accuracy: 0.4653\n",
      "Epoch 18/80\n",
      "36/36 [==============================] - 0s 4ms/step - loss: 1.0173 - accuracy: 0.5383 - val_loss: 1.1707 - val_accuracy: 0.4583\n",
      "Epoch 19/80\n",
      "36/36 [==============================] - 0s 4ms/step - loss: 0.9723 - accuracy: 0.5976 - val_loss: 1.1769 - val_accuracy: 0.4653\n",
      "Epoch 20/80\n",
      "36/36 [==============================] - 0s 4ms/step - loss: 0.9627 - accuracy: 0.5714 - val_loss: 1.1820 - val_accuracy: 0.4653\n",
      "Epoch 21/80\n",
      "36/36 [==============================] - 0s 4ms/step - loss: 0.9761 - accuracy: 0.5575 - val_loss: 1.1859 - val_accuracy: 0.4722\n",
      "Epoch 22/80\n",
      "36/36 [==============================] - 0s 4ms/step - loss: 0.9785 - accuracy: 0.5749 - val_loss: 1.1829 - val_accuracy: 0.4792\n",
      "Epoch 23/80\n",
      "36/36 [==============================] - 0s 4ms/step - loss: 0.9346 - accuracy: 0.5993 - val_loss: 1.1878 - val_accuracy: 0.4861\n",
      "Epoch 24/80\n",
      "36/36 [==============================] - 0s 4ms/step - loss: 0.9144 - accuracy: 0.6115 - val_loss: 1.1787 - val_accuracy: 0.4931\n",
      "Epoch 25/80\n",
      "36/36 [==============================] - 0s 4ms/step - loss: 0.8687 - accuracy: 0.6359 - val_loss: 1.1834 - val_accuracy: 0.5139\n",
      "Epoch 26/80\n",
      "36/36 [==============================] - 0s 4ms/step - loss: 0.8867 - accuracy: 0.6150 - val_loss: 1.1903 - val_accuracy: 0.4931\n",
      "Epoch 27/80\n",
      "36/36 [==============================] - 0s 4ms/step - loss: 0.9075 - accuracy: 0.6220 - val_loss: 1.1968 - val_accuracy: 0.4931\n",
      "Epoch 28/80\n",
      "36/36 [==============================] - 0s 4ms/step - loss: 0.8808 - accuracy: 0.6010 - val_loss: 1.1910 - val_accuracy: 0.5000\n",
      "Epoch 29/80\n",
      "36/36 [==============================] - 0s 4ms/step - loss: 0.8816 - accuracy: 0.6045 - val_loss: 1.2024 - val_accuracy: 0.5139\n",
      "Epoch 30/80\n",
      "36/36 [==============================] - 0s 4ms/step - loss: 0.8547 - accuracy: 0.6376 - val_loss: 1.1885 - val_accuracy: 0.5069\n",
      "Epoch 31/80\n",
      "36/36 [==============================] - 0s 4ms/step - loss: 0.8747 - accuracy: 0.6324 - val_loss: 1.1992 - val_accuracy: 0.5069\n",
      "Epoch 32/80\n",
      "36/36 [==============================] - 0s 4ms/step - loss: 0.8125 - accuracy: 0.6760 - val_loss: 1.2014 - val_accuracy: 0.5000\n",
      "Epoch 33/80\n",
      "36/36 [==============================] - 0s 5ms/step - loss: 0.8285 - accuracy: 0.6585 - val_loss: 1.2220 - val_accuracy: 0.5069\n",
      "Epoch 34/80\n",
      "36/36 [==============================] - 0s 4ms/step - loss: 0.8819 - accuracy: 0.6202 - val_loss: 1.2375 - val_accuracy: 0.4861\n",
      "Epoch 35/80\n",
      "36/36 [==============================] - 0s 4ms/step - loss: 0.8534 - accuracy: 0.6359 - val_loss: 1.2160 - val_accuracy: 0.4861\n",
      "Epoch 36/80\n",
      "36/36 [==============================] - 0s 4ms/step - loss: 0.8468 - accuracy: 0.6429 - val_loss: 1.2174 - val_accuracy: 0.5069\n",
      "Epoch 37/80\n",
      "36/36 [==============================] - 0s 4ms/step - loss: 0.7865 - accuracy: 0.6603 - val_loss: 1.2208 - val_accuracy: 0.5139\n",
      "Epoch 38/80\n",
      "36/36 [==============================] - 0s 4ms/step - loss: 0.7975 - accuracy: 0.6603 - val_loss: 1.2138 - val_accuracy: 0.5208\n",
      "Epoch 39/80\n",
      "36/36 [==============================] - 0s 6ms/step - loss: 0.7926 - accuracy: 0.6603 - val_loss: 1.2268 - val_accuracy: 0.5000\n",
      "Epoch 40/80\n",
      "36/36 [==============================] - 0s 4ms/step - loss: 0.8223 - accuracy: 0.6638 - val_loss: 1.2029 - val_accuracy: 0.5139\n",
      "Epoch 41/80\n",
      "36/36 [==============================] - 0s 4ms/step - loss: 0.7545 - accuracy: 0.6916 - val_loss: 1.2008 - val_accuracy: 0.5069\n",
      "Epoch 42/80\n",
      "36/36 [==============================] - 0s 4ms/step - loss: 0.7710 - accuracy: 0.6742 - val_loss: 1.2212 - val_accuracy: 0.5139\n",
      "Epoch 43/80\n",
      "36/36 [==============================] - 0s 5ms/step - loss: 0.7622 - accuracy: 0.6899 - val_loss: 1.2165 - val_accuracy: 0.5069\n",
      "Epoch 44/80\n",
      "36/36 [==============================] - 0s 4ms/step - loss: 0.7392 - accuracy: 0.7143 - val_loss: 1.2181 - val_accuracy: 0.5139\n",
      "Epoch 45/80\n",
      "36/36 [==============================] - 0s 4ms/step - loss: 0.7527 - accuracy: 0.6847 - val_loss: 1.2070 - val_accuracy: 0.5208\n",
      "Epoch 46/80\n",
      "36/36 [==============================] - 0s 4ms/step - loss: 0.7588 - accuracy: 0.6707 - val_loss: 1.2369 - val_accuracy: 0.4931\n",
      "Epoch 47/80\n",
      "36/36 [==============================] - 0s 4ms/step - loss: 0.7455 - accuracy: 0.6864 - val_loss: 1.2434 - val_accuracy: 0.5139\n",
      "Epoch 48/80\n",
      "36/36 [==============================] - 0s 4ms/step - loss: 0.7373 - accuracy: 0.7073 - val_loss: 1.2515 - val_accuracy: 0.4931\n",
      "Epoch 49/80\n",
      "36/36 [==============================] - 0s 4ms/step - loss: 0.7312 - accuracy: 0.7230 - val_loss: 1.2772 - val_accuracy: 0.4792\n",
      "Epoch 50/80\n",
      "36/36 [==============================] - 0s 4ms/step - loss: 0.7313 - accuracy: 0.6794 - val_loss: 1.2817 - val_accuracy: 0.4861\n",
      "Epoch 51/80\n",
      "36/36 [==============================] - 0s 4ms/step - loss: 0.7445 - accuracy: 0.7021 - val_loss: 1.2639 - val_accuracy: 0.5069\n",
      "Epoch 52/80\n",
      "36/36 [==============================] - 0s 4ms/step - loss: 0.7376 - accuracy: 0.7038 - val_loss: 1.2826 - val_accuracy: 0.4931\n",
      "Epoch 53/80\n",
      "36/36 [==============================] - 0s 4ms/step - loss: 0.6676 - accuracy: 0.7178 - val_loss: 1.2685 - val_accuracy: 0.4931\n",
      "Epoch 54/80\n",
      "36/36 [==============================] - 0s 4ms/step - loss: 0.6910 - accuracy: 0.7300 - val_loss: 1.2772 - val_accuracy: 0.5139\n",
      "Epoch 55/80\n",
      "36/36 [==============================] - 0s 4ms/step - loss: 0.6924 - accuracy: 0.7387 - val_loss: 1.2651 - val_accuracy: 0.5139\n",
      "Epoch 56/80\n",
      "36/36 [==============================] - 0s 4ms/step - loss: 0.7128 - accuracy: 0.7021 - val_loss: 1.3010 - val_accuracy: 0.5069\n",
      "Epoch 57/80\n",
      "36/36 [==============================] - 0s 4ms/step - loss: 0.6759 - accuracy: 0.7230 - val_loss: 1.3171 - val_accuracy: 0.4792\n",
      "Epoch 58/80\n",
      "36/36 [==============================] - 0s 8ms/step - loss: 0.6673 - accuracy: 0.7369 - val_loss: 1.3174 - val_accuracy: 0.5139\n",
      "Epoch 59/80\n",
      "36/36 [==============================] - 0s 4ms/step - loss: 0.6693 - accuracy: 0.7195 - val_loss: 1.3059 - val_accuracy: 0.5000\n",
      "Epoch 60/80\n",
      "36/36 [==============================] - 0s 4ms/step - loss: 0.6705 - accuracy: 0.7526 - val_loss: 1.2890 - val_accuracy: 0.5139\n",
      "Epoch 61/80\n",
      "36/36 [==============================] - 0s 5ms/step - loss: 0.6806 - accuracy: 0.7230 - val_loss: 1.2864 - val_accuracy: 0.5208\n",
      "Epoch 62/80\n",
      "36/36 [==============================] - 0s 4ms/step - loss: 0.6983 - accuracy: 0.7073 - val_loss: 1.2795 - val_accuracy: 0.5069\n",
      "Epoch 63/80\n",
      "36/36 [==============================] - 0s 4ms/step - loss: 0.6555 - accuracy: 0.7282 - val_loss: 1.2976 - val_accuracy: 0.5139\n",
      "Epoch 64/80\n",
      "36/36 [==============================] - 0s 5ms/step - loss: 0.6134 - accuracy: 0.7666 - val_loss: 1.2774 - val_accuracy: 0.5069\n",
      "Epoch 65/80\n",
      "36/36 [==============================] - 0s 4ms/step - loss: 0.6390 - accuracy: 0.7613 - val_loss: 1.2691 - val_accuracy: 0.5139\n",
      "Epoch 66/80\n",
      "36/36 [==============================] - 0s 4ms/step - loss: 0.6488 - accuracy: 0.7352 - val_loss: 1.2812 - val_accuracy: 0.5347\n",
      "Epoch 67/80\n",
      "36/36 [==============================] - 0s 5ms/step - loss: 0.6104 - accuracy: 0.7631 - val_loss: 1.3030 - val_accuracy: 0.5069\n",
      "Epoch 68/80\n",
      "36/36 [==============================] - 0s 5ms/step - loss: 0.6001 - accuracy: 0.7700 - val_loss: 1.3098 - val_accuracy: 0.5000\n",
      "Epoch 69/80\n",
      "36/36 [==============================] - 0s 5ms/step - loss: 0.6265 - accuracy: 0.7526 - val_loss: 1.3121 - val_accuracy: 0.5000\n",
      "Epoch 70/80\n",
      "36/36 [==============================] - 0s 4ms/step - loss: 0.6043 - accuracy: 0.7596 - val_loss: 1.3028 - val_accuracy: 0.5069\n",
      "Epoch 71/80\n",
      "36/36 [==============================] - 0s 5ms/step - loss: 0.6474 - accuracy: 0.7596 - val_loss: 1.3342 - val_accuracy: 0.4931\n",
      "Epoch 72/80\n",
      "36/36 [==============================] - 0s 4ms/step - loss: 0.5717 - accuracy: 0.7840 - val_loss: 1.3303 - val_accuracy: 0.4931\n",
      "Epoch 73/80\n",
      "36/36 [==============================] - 0s 4ms/step - loss: 0.5824 - accuracy: 0.7857 - val_loss: 1.3195 - val_accuracy: 0.5000\n",
      "Epoch 74/80\n",
      "36/36 [==============================] - 0s 8ms/step - loss: 0.5988 - accuracy: 0.7613 - val_loss: 1.3068 - val_accuracy: 0.5069\n",
      "Epoch 75/80\n",
      "36/36 [==============================] - 0s 4ms/step - loss: 0.5581 - accuracy: 0.8014 - val_loss: 1.3138 - val_accuracy: 0.5069\n",
      "Epoch 76/80\n",
      "36/36 [==============================] - 0s 4ms/step - loss: 0.5644 - accuracy: 0.7944 - val_loss: 1.3204 - val_accuracy: 0.5000\n",
      "Epoch 77/80\n",
      "36/36 [==============================] - 0s 4ms/step - loss: 0.5696 - accuracy: 0.7979 - val_loss: 1.3279 - val_accuracy: 0.5000\n",
      "Epoch 78/80\n",
      "36/36 [==============================] - 0s 4ms/step - loss: 0.5667 - accuracy: 0.7770 - val_loss: 1.3284 - val_accuracy: 0.5069\n",
      "Epoch 79/80\n",
      "36/36 [==============================] - 0s 4ms/step - loss: 0.5340 - accuracy: 0.7962 - val_loss: 1.3311 - val_accuracy: 0.5069\n",
      "Epoch 80/80\n",
      "36/36 [==============================] - 0s 5ms/step - loss: 0.5742 - accuracy: 0.7909 - val_loss: 1.3720 - val_accuracy: 0.5000\n"
     ]
    }
   ],
   "source": [
    "history10 = model10.fit(X1_train_scaled, y1_train, \n",
    "                      validation_data=(X1_val_scaled, y1_val), \n",
    "                      batch_size=16, \n",
    "                      epochs=80)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6cca19c2",
   "metadata": {},
   "source": [
    "### 3.2 Change dataset\n",
    "After trying out multiple ways to address the overfitting problems, the model accuracy still remain almost same and did not improve significantly. Because the number of data in this dataset is sufficient, therefore, changing of the dataset is considered. Here the dataset2 is used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "c538707f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"Conv1D_11\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " Conv1D_1 (Conv1D)           (None, 57, 16)            96        \n",
      "                                                                 \n",
      " BN1 (BatchNormalization)    (None, 57, 16)            64        \n",
      "                                                                 \n",
      " MaxPooling1 (MaxPooling1D)  (None, 28, 16)            0         \n",
      "                                                                 \n",
      " Conv1D_2 (Conv1D)           (None, 28, 32)            3616      \n",
      "                                                                 \n",
      " BN2 (BatchNormalization)    (None, 28, 32)            128       \n",
      "                                                                 \n",
      " MaxPooling2 (MaxPooling1D)  (None, 14, 32)            0         \n",
      "                                                                 \n",
      " Conv1D_3 (Conv1D)           (None, 14, 32)            7200      \n",
      "                                                                 \n",
      " BN3 (BatchNormalization)    (None, 14, 32)            128       \n",
      "                                                                 \n",
      " MaxPooling3 (MaxPooling1D)  (None, 7, 32)             0         \n",
      "                                                                 \n",
      " Flatten (Flatten)           (None, 224)               0         \n",
      "                                                                 \n",
      " Dense_1 (Dense)             (None, 128)               28800     \n",
      "                                                                 \n",
      " Dropout_1 (Dropout)         (None, 128)               0         \n",
      "                                                                 \n",
      " Dense_2 (Dense)             (None, 64)                8256      \n",
      "                                                                 \n",
      " Dropout_2 (Dropout)         (None, 64)                0         \n",
      "                                                                 \n",
      " Softmax (Dense)             (None, 4)                 260       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 48,548\n",
      "Trainable params: 48,388\n",
      "Non-trainable params: 160\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "f1,k1,a1 = 16,5,'relu'\n",
    "f2,k2,a2 = 32,7,'relu'\n",
    "f3,k3,a3 = 32,7,'relu'\n",
    "d1,dr1,da1,r1 = 128,0.2,'relu',0.15\n",
    "d2,dr2,da2,r2 = 64,0.2,'relu',0.15\n",
    "num = 11\n",
    "\n",
    "model11 = modelBuilder3L(X2_train_scaled,\n",
    "                        f1,k1,a1,\n",
    "                        f2,k2,a2,\n",
    "                        f3,k3,a3,\n",
    "                        d1,dr1,da1,r1,\n",
    "                        d2,dr2,da2,r2,\n",
    "                        num)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "7948184b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/80\n",
      "16/16 [==============================] - 3s 29ms/step - loss: 1.9771 - accuracy: 0.2266 - val_loss: 1.3864 - val_accuracy: 0.2656\n",
      "Epoch 2/80\n",
      "16/16 [==============================] - 0s 9ms/step - loss: 1.6505 - accuracy: 0.3125 - val_loss: 1.3873 - val_accuracy: 0.2500\n",
      "Epoch 3/80\n",
      "16/16 [==============================] - 0s 9ms/step - loss: 1.4294 - accuracy: 0.3672 - val_loss: 1.3886 - val_accuracy: 0.2500\n",
      "Epoch 4/80\n",
      "16/16 [==============================] - 0s 9ms/step - loss: 1.2604 - accuracy: 0.4414 - val_loss: 1.3860 - val_accuracy: 0.2500\n",
      "Epoch 5/80\n",
      "16/16 [==============================] - 0s 9ms/step - loss: 1.0926 - accuracy: 0.5312 - val_loss: 1.3825 - val_accuracy: 0.2344\n",
      "Epoch 6/80\n",
      "16/16 [==============================] - 0s 9ms/step - loss: 1.0482 - accuracy: 0.5586 - val_loss: 1.3805 - val_accuracy: 0.2656\n",
      "Epoch 7/80\n",
      "16/16 [==============================] - 0s 8ms/step - loss: 0.9486 - accuracy: 0.5820 - val_loss: 1.3794 - val_accuracy: 0.2969\n",
      "Epoch 8/80\n",
      "16/16 [==============================] - 0s 9ms/step - loss: 0.8850 - accuracy: 0.6602 - val_loss: 1.3806 - val_accuracy: 0.3281\n",
      "Epoch 9/80\n",
      "16/16 [==============================] - 0s 8ms/step - loss: 0.8440 - accuracy: 0.6445 - val_loss: 1.3835 - val_accuracy: 0.3281\n",
      "Epoch 10/80\n",
      "16/16 [==============================] - 0s 8ms/step - loss: 0.8268 - accuracy: 0.6875 - val_loss: 1.3879 - val_accuracy: 0.2969\n",
      "Epoch 11/80\n",
      "16/16 [==============================] - 0s 8ms/step - loss: 0.7774 - accuracy: 0.7031 - val_loss: 1.3918 - val_accuracy: 0.2969\n",
      "Epoch 12/80\n",
      "16/16 [==============================] - 0s 8ms/step - loss: 0.7624 - accuracy: 0.6523 - val_loss: 1.3954 - val_accuracy: 0.3125\n",
      "Epoch 13/80\n",
      "16/16 [==============================] - 0s 9ms/step - loss: 0.7299 - accuracy: 0.7227 - val_loss: 1.3974 - val_accuracy: 0.3125\n",
      "Epoch 14/80\n",
      "16/16 [==============================] - 0s 8ms/step - loss: 0.6513 - accuracy: 0.7695 - val_loss: 1.3952 - val_accuracy: 0.3281\n",
      "Epoch 15/80\n",
      "16/16 [==============================] - 0s 8ms/step - loss: 0.6697 - accuracy: 0.7539 - val_loss: 1.3906 - val_accuracy: 0.3438\n",
      "Epoch 16/80\n",
      "16/16 [==============================] - 0s 8ms/step - loss: 0.6419 - accuracy: 0.7773 - val_loss: 1.3834 - val_accuracy: 0.3281\n",
      "Epoch 17/80\n",
      "16/16 [==============================] - 0s 7ms/step - loss: 0.5632 - accuracy: 0.8164 - val_loss: 1.3735 - val_accuracy: 0.3281\n",
      "Epoch 18/80\n",
      "16/16 [==============================] - 0s 7ms/step - loss: 0.5358 - accuracy: 0.8125 - val_loss: 1.3606 - val_accuracy: 0.3281\n",
      "Epoch 19/80\n",
      "16/16 [==============================] - 0s 7ms/step - loss: 0.5452 - accuracy: 0.8125 - val_loss: 1.3412 - val_accuracy: 0.3438\n",
      "Epoch 20/80\n",
      "16/16 [==============================] - 0s 7ms/step - loss: 0.5098 - accuracy: 0.8164 - val_loss: 1.3177 - val_accuracy: 0.3438\n",
      "Epoch 21/80\n",
      "16/16 [==============================] - 0s 7ms/step - loss: 0.4566 - accuracy: 0.8438 - val_loss: 1.2806 - val_accuracy: 0.3594\n",
      "Epoch 22/80\n",
      "16/16 [==============================] - 0s 10ms/step - loss: 0.5063 - accuracy: 0.8125 - val_loss: 1.2449 - val_accuracy: 0.3594\n",
      "Epoch 23/80\n",
      "16/16 [==============================] - 0s 8ms/step - loss: 0.4593 - accuracy: 0.8555 - val_loss: 1.2056 - val_accuracy: 0.3906\n",
      "Epoch 24/80\n",
      "16/16 [==============================] - 0s 14ms/step - loss: 0.4497 - accuracy: 0.8516 - val_loss: 1.1599 - val_accuracy: 0.4219\n",
      "Epoch 25/80\n",
      "16/16 [==============================] - 0s 9ms/step - loss: 0.4702 - accuracy: 0.8477 - val_loss: 1.1123 - val_accuracy: 0.4375\n",
      "Epoch 26/80\n",
      "16/16 [==============================] - 0s 9ms/step - loss: 0.4195 - accuracy: 0.8477 - val_loss: 1.0496 - val_accuracy: 0.5000\n",
      "Epoch 27/80\n",
      "16/16 [==============================] - 0s 7ms/step - loss: 0.3570 - accuracy: 0.8984 - val_loss: 1.0046 - val_accuracy: 0.5469\n",
      "Epoch 28/80\n",
      "16/16 [==============================] - 0s 8ms/step - loss: 0.3637 - accuracy: 0.8789 - val_loss: 0.9560 - val_accuracy: 0.5938\n",
      "Epoch 29/80\n",
      "16/16 [==============================] - 0s 7ms/step - loss: 0.3693 - accuracy: 0.8906 - val_loss: 0.9041 - val_accuracy: 0.6406\n",
      "Epoch 30/80\n",
      "16/16 [==============================] - 0s 8ms/step - loss: 0.3166 - accuracy: 0.9141 - val_loss: 0.8591 - val_accuracy: 0.7031\n",
      "Epoch 31/80\n",
      "16/16 [==============================] - 0s 8ms/step - loss: 0.3677 - accuracy: 0.8828 - val_loss: 0.8202 - val_accuracy: 0.7188\n",
      "Epoch 32/80\n",
      "16/16 [==============================] - 0s 7ms/step - loss: 0.3000 - accuracy: 0.9062 - val_loss: 0.7830 - val_accuracy: 0.7188\n",
      "Epoch 33/80\n",
      "16/16 [==============================] - 0s 8ms/step - loss: 0.2700 - accuracy: 0.9258 - val_loss: 0.7480 - val_accuracy: 0.7188\n",
      "Epoch 34/80\n",
      "16/16 [==============================] - 0s 9ms/step - loss: 0.2829 - accuracy: 0.9219 - val_loss: 0.7191 - val_accuracy: 0.7188\n",
      "Epoch 35/80\n",
      "16/16 [==============================] - 0s 8ms/step - loss: 0.3026 - accuracy: 0.9062 - val_loss: 0.7028 - val_accuracy: 0.7500\n",
      "Epoch 36/80\n",
      "16/16 [==============================] - 0s 8ms/step - loss: 0.2653 - accuracy: 0.9297 - val_loss: 0.6950 - val_accuracy: 0.7500\n",
      "Epoch 37/80\n",
      "16/16 [==============================] - 0s 8ms/step - loss: 0.2546 - accuracy: 0.9258 - val_loss: 0.6881 - val_accuracy: 0.7656\n",
      "Epoch 38/80\n",
      "16/16 [==============================] - 0s 15ms/step - loss: 0.2472 - accuracy: 0.9336 - val_loss: 0.6840 - val_accuracy: 0.7656\n",
      "Epoch 39/80\n",
      "16/16 [==============================] - 0s 7ms/step - loss: 0.2372 - accuracy: 0.9453 - val_loss: 0.6757 - val_accuracy: 0.7656\n",
      "Epoch 40/80\n",
      "16/16 [==============================] - 0s 9ms/step - loss: 0.2318 - accuracy: 0.9336 - val_loss: 0.6745 - val_accuracy: 0.7656\n",
      "Epoch 41/80\n",
      "16/16 [==============================] - 0s 7ms/step - loss: 0.2318 - accuracy: 0.9414 - val_loss: 0.6751 - val_accuracy: 0.7656\n",
      "Epoch 42/80\n",
      "16/16 [==============================] - 0s 7ms/step - loss: 0.2342 - accuracy: 0.9336 - val_loss: 0.6675 - val_accuracy: 0.7812\n",
      "Epoch 43/80\n",
      "16/16 [==============================] - 0s 7ms/step - loss: 0.2230 - accuracy: 0.9336 - val_loss: 0.6708 - val_accuracy: 0.7812\n",
      "Epoch 44/80\n",
      "16/16 [==============================] - 0s 8ms/step - loss: 0.2092 - accuracy: 0.9375 - val_loss: 0.6709 - val_accuracy: 0.7812\n",
      "Epoch 45/80\n",
      "16/16 [==============================] - 0s 7ms/step - loss: 0.1847 - accuracy: 0.9688 - val_loss: 0.6670 - val_accuracy: 0.7812\n",
      "Epoch 46/80\n",
      "16/16 [==============================] - 0s 9ms/step - loss: 0.2149 - accuracy: 0.9180 - val_loss: 0.6684 - val_accuracy: 0.7812\n",
      "Epoch 47/80\n",
      "16/16 [==============================] - 0s 7ms/step - loss: 0.2084 - accuracy: 0.9258 - val_loss: 0.6863 - val_accuracy: 0.7969\n",
      "Epoch 48/80\n",
      "16/16 [==============================] - 0s 7ms/step - loss: 0.1652 - accuracy: 0.9570 - val_loss: 0.6786 - val_accuracy: 0.7812\n",
      "Epoch 49/80\n",
      "16/16 [==============================] - 0s 8ms/step - loss: 0.1740 - accuracy: 0.9570 - val_loss: 0.6793 - val_accuracy: 0.7500\n",
      "Epoch 50/80\n",
      "16/16 [==============================] - 0s 9ms/step - loss: 0.1324 - accuracy: 0.9727 - val_loss: 0.6892 - val_accuracy: 0.7812\n",
      "Epoch 51/80\n",
      "16/16 [==============================] - 0s 7ms/step - loss: 0.1399 - accuracy: 0.9766 - val_loss: 0.6922 - val_accuracy: 0.7812\n",
      "Epoch 52/80\n",
      "16/16 [==============================] - 0s 7ms/step - loss: 0.1250 - accuracy: 0.9766 - val_loss: 0.7028 - val_accuracy: 0.7812\n",
      "Epoch 53/80\n",
      "16/16 [==============================] - 0s 7ms/step - loss: 0.1414 - accuracy: 0.9766 - val_loss: 0.7119 - val_accuracy: 0.7656\n",
      "Epoch 54/80\n",
      "16/16 [==============================] - 0s 7ms/step - loss: 0.1401 - accuracy: 0.9609 - val_loss: 0.7108 - val_accuracy: 0.7656\n",
      "Epoch 55/80\n",
      "16/16 [==============================] - 0s 7ms/step - loss: 0.1159 - accuracy: 0.9727 - val_loss: 0.7019 - val_accuracy: 0.7656\n",
      "Epoch 56/80\n",
      "16/16 [==============================] - 0s 8ms/step - loss: 0.1452 - accuracy: 0.9648 - val_loss: 0.7066 - val_accuracy: 0.7812\n",
      "Epoch 57/80\n",
      "16/16 [==============================] - 0s 7ms/step - loss: 0.1337 - accuracy: 0.9688 - val_loss: 0.7202 - val_accuracy: 0.7969\n",
      "Epoch 58/80\n",
      "16/16 [==============================] - 0s 8ms/step - loss: 0.1177 - accuracy: 0.9648 - val_loss: 0.7107 - val_accuracy: 0.7812\n",
      "Epoch 59/80\n",
      "16/16 [==============================] - 0s 7ms/step - loss: 0.1208 - accuracy: 0.9648 - val_loss: 0.7296 - val_accuracy: 0.7812\n",
      "Epoch 60/80\n",
      "16/16 [==============================] - 0s 7ms/step - loss: 0.0747 - accuracy: 0.9922 - val_loss: 0.7445 - val_accuracy: 0.7812\n",
      "Epoch 61/80\n",
      "16/16 [==============================] - 0s 13ms/step - loss: 0.1153 - accuracy: 0.9648 - val_loss: 0.7654 - val_accuracy: 0.7656\n",
      "Epoch 62/80\n",
      "16/16 [==============================] - 0s 8ms/step - loss: 0.0896 - accuracy: 0.9805 - val_loss: 0.7597 - val_accuracy: 0.7500\n",
      "Epoch 63/80\n",
      "16/16 [==============================] - 0s 7ms/step - loss: 0.0966 - accuracy: 0.9805 - val_loss: 0.7404 - val_accuracy: 0.7500\n",
      "Epoch 64/80\n",
      "16/16 [==============================] - 0s 7ms/step - loss: 0.0923 - accuracy: 0.9805 - val_loss: 0.7332 - val_accuracy: 0.7656\n",
      "Epoch 65/80\n",
      "16/16 [==============================] - 0s 7ms/step - loss: 0.0951 - accuracy: 0.9805 - val_loss: 0.7616 - val_accuracy: 0.7344\n",
      "Epoch 66/80\n",
      "16/16 [==============================] - 0s 7ms/step - loss: 0.0723 - accuracy: 0.9883 - val_loss: 0.7742 - val_accuracy: 0.7344\n",
      "Epoch 67/80\n",
      "16/16 [==============================] - 0s 9ms/step - loss: 0.1096 - accuracy: 0.9570 - val_loss: 0.7731 - val_accuracy: 0.7344\n",
      "Epoch 68/80\n",
      "16/16 [==============================] - 0s 8ms/step - loss: 0.0846 - accuracy: 0.9805 - val_loss: 0.7628 - val_accuracy: 0.7344\n",
      "Epoch 69/80\n",
      "16/16 [==============================] - 0s 7ms/step - loss: 0.0781 - accuracy: 0.9805 - val_loss: 0.7805 - val_accuracy: 0.7188\n",
      "Epoch 70/80\n",
      "16/16 [==============================] - 0s 7ms/step - loss: 0.0989 - accuracy: 0.9766 - val_loss: 0.7716 - val_accuracy: 0.7188\n",
      "Epoch 71/80\n",
      "16/16 [==============================] - 0s 8ms/step - loss: 0.0849 - accuracy: 0.9766 - val_loss: 0.7825 - val_accuracy: 0.7188\n",
      "Epoch 72/80\n",
      "16/16 [==============================] - 0s 8ms/step - loss: 0.0659 - accuracy: 0.9883 - val_loss: 0.7954 - val_accuracy: 0.7031\n",
      "Epoch 73/80\n",
      "16/16 [==============================] - 0s 8ms/step - loss: 0.0685 - accuracy: 0.9922 - val_loss: 0.7933 - val_accuracy: 0.7031\n",
      "Epoch 74/80\n",
      "16/16 [==============================] - 0s 7ms/step - loss: 0.0594 - accuracy: 1.0000 - val_loss: 0.7911 - val_accuracy: 0.7031\n",
      "Epoch 75/80\n",
      "16/16 [==============================] - 0s 7ms/step - loss: 0.0531 - accuracy: 1.0000 - val_loss: 0.7934 - val_accuracy: 0.7188\n",
      "Epoch 76/80\n",
      "16/16 [==============================] - 0s 7ms/step - loss: 0.0763 - accuracy: 0.9844 - val_loss: 0.7933 - val_accuracy: 0.7031\n",
      "Epoch 77/80\n",
      "16/16 [==============================] - 0s 7ms/step - loss: 0.0557 - accuracy: 0.9844 - val_loss: 0.8137 - val_accuracy: 0.7031\n",
      "Epoch 78/80\n",
      "16/16 [==============================] - 0s 7ms/step - loss: 0.0573 - accuracy: 0.9961 - val_loss: 0.8107 - val_accuracy: 0.7031\n",
      "Epoch 79/80\n",
      "16/16 [==============================] - 0s 10ms/step - loss: 0.0668 - accuracy: 0.9805 - val_loss: 0.7770 - val_accuracy: 0.7500\n",
      "Epoch 80/80\n",
      "16/16 [==============================] - 0s 7ms/step - loss: 0.0519 - accuracy: 0.9922 - val_loss: 0.8006 - val_accuracy: 0.7188\n"
     ]
    }
   ],
   "source": [
    "history11 = model11.fit(X2_train_scaled, y2_train, \n",
    "                        validation_data=(X2_val_scaled, y2_val), \n",
    "                        batch_size=16, \n",
    "                        epochs=80)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7149e7d3",
   "metadata": {},
   "source": [
    "**Different number of features**</br>\n",
    "**MFCCs Only**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "d2789432",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"Conv1D_12\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " Conv1D_1 (Conv1D)           (None, 40, 16)            96        \n",
      "                                                                 \n",
      " BN1 (BatchNormalization)    (None, 40, 16)            64        \n",
      "                                                                 \n",
      " MaxPooling1 (MaxPooling1D)  (None, 20, 16)            0         \n",
      "                                                                 \n",
      " Conv1D_2 (Conv1D)           (None, 20, 32)            3616      \n",
      "                                                                 \n",
      " BN2 (BatchNormalization)    (None, 20, 32)            128       \n",
      "                                                                 \n",
      " MaxPooling2 (MaxPooling1D)  (None, 10, 32)            0         \n",
      "                                                                 \n",
      " Conv1D_3 (Conv1D)           (None, 10, 32)            7200      \n",
      "                                                                 \n",
      " BN3 (BatchNormalization)    (None, 10, 32)            128       \n",
      "                                                                 \n",
      " MaxPooling3 (MaxPooling1D)  (None, 5, 32)             0         \n",
      "                                                                 \n",
      " Flatten (Flatten)           (None, 160)               0         \n",
      "                                                                 \n",
      " Dense_1 (Dense)             (None, 128)               20608     \n",
      "                                                                 \n",
      " Dropout_1 (Dropout)         (None, 128)               0         \n",
      "                                                                 \n",
      " Dense_2 (Dense)             (None, 64)                8256      \n",
      "                                                                 \n",
      " Dropout_2 (Dropout)         (None, 64)                0         \n",
      "                                                                 \n",
      " Softmax (Dense)             (None, 4)                 260       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 40,356\n",
      "Trainable params: 40,196\n",
      "Non-trainable params: 160\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "f1,k1,a1 = 16,5,'relu'\n",
    "f2,k2,a2 = 32,7,'relu'\n",
    "f3,k3,a3 = 32,7,'relu'\n",
    "d1,dr1,da1,r1 = 128,0.2,'relu',0.15\n",
    "d2,dr2,da2,r2 = 64,0.2,'relu',0.15\n",
    "num = 12\n",
    "\n",
    "model12 = modelBuilder3L(X2_train_scaled[:,17:],\n",
    "                        f1,k1,a1,\n",
    "                        f2,k2,a2,\n",
    "                        f3,k3,a3,\n",
    "                        d1,dr1,da1,r1,\n",
    "                        d2,dr2,da2,r2,\n",
    "                        num)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "c8ca28d1",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16/16 [==============================] - 3s 27ms/step - loss: 1.5735 - accuracy: 0.3008 - val_loss: 1.3815 - val_accuracy: 0.3906\n",
      "Epoch 2/50\n",
      "16/16 [==============================] - 0s 8ms/step - loss: 1.4005 - accuracy: 0.3789 - val_loss: 1.3743 - val_accuracy: 0.4375\n",
      "Epoch 3/50\n",
      "16/16 [==============================] - 0s 7ms/step - loss: 1.2571 - accuracy: 0.4375 - val_loss: 1.3719 - val_accuracy: 0.4062\n",
      "Epoch 4/50\n",
      "16/16 [==============================] - 0s 7ms/step - loss: 1.1489 - accuracy: 0.4922 - val_loss: 1.3707 - val_accuracy: 0.3750\n",
      "Epoch 5/50\n",
      "16/16 [==============================] - 0s 7ms/step - loss: 1.0413 - accuracy: 0.5977 - val_loss: 1.3709 - val_accuracy: 0.3438\n",
      "Epoch 6/50\n",
      "16/16 [==============================] - 0s 7ms/step - loss: 0.9460 - accuracy: 0.6172 - val_loss: 1.3715 - val_accuracy: 0.3125\n",
      "Epoch 7/50\n",
      "16/16 [==============================] - 0s 6ms/step - loss: 0.9452 - accuracy: 0.6094 - val_loss: 1.3730 - val_accuracy: 0.2656\n",
      "Epoch 8/50\n",
      "16/16 [==============================] - 0s 6ms/step - loss: 0.9385 - accuracy: 0.5898 - val_loss: 1.3743 - val_accuracy: 0.2656\n",
      "Epoch 9/50\n",
      "16/16 [==============================] - 0s 6ms/step - loss: 0.8266 - accuracy: 0.6992 - val_loss: 1.3774 - val_accuracy: 0.2656\n",
      "Epoch 10/50\n",
      "16/16 [==============================] - 0s 6ms/step - loss: 0.8085 - accuracy: 0.6836 - val_loss: 1.3803 - val_accuracy: 0.2656\n",
      "Epoch 11/50\n",
      "16/16 [==============================] - 0s 6ms/step - loss: 0.7506 - accuracy: 0.7227 - val_loss: 1.3828 - val_accuracy: 0.2812\n",
      "Epoch 12/50\n",
      "16/16 [==============================] - 0s 7ms/step - loss: 0.7605 - accuracy: 0.6953 - val_loss: 1.3809 - val_accuracy: 0.2969\n",
      "Epoch 13/50\n",
      "16/16 [==============================] - 0s 6ms/step - loss: 0.7296 - accuracy: 0.7539 - val_loss: 1.3761 - val_accuracy: 0.3125\n",
      "Epoch 14/50\n",
      "16/16 [==============================] - 0s 6ms/step - loss: 0.7369 - accuracy: 0.7383 - val_loss: 1.3718 - val_accuracy: 0.3281\n",
      "Epoch 15/50\n",
      "16/16 [==============================] - 0s 6ms/step - loss: 0.6915 - accuracy: 0.7227 - val_loss: 1.3592 - val_accuracy: 0.3438\n",
      "Epoch 16/50\n",
      "16/16 [==============================] - 0s 7ms/step - loss: 0.6026 - accuracy: 0.7930 - val_loss: 1.3515 - val_accuracy: 0.3438\n",
      "Epoch 17/50\n",
      "16/16 [==============================] - 0s 8ms/step - loss: 0.6260 - accuracy: 0.7773 - val_loss: 1.3398 - val_accuracy: 0.3438\n",
      "Epoch 18/50\n",
      "16/16 [==============================] - 0s 7ms/step - loss: 0.6737 - accuracy: 0.7031 - val_loss: 1.3263 - val_accuracy: 0.3438\n",
      "Epoch 19/50\n",
      "16/16 [==============================] - 0s 7ms/step - loss: 0.5803 - accuracy: 0.7969 - val_loss: 1.3030 - val_accuracy: 0.3594\n",
      "Epoch 20/50\n",
      "16/16 [==============================] - 0s 7ms/step - loss: 0.5230 - accuracy: 0.8359 - val_loss: 1.2757 - val_accuracy: 0.4062\n",
      "Epoch 21/50\n",
      "16/16 [==============================] - 0s 7ms/step - loss: 0.5127 - accuracy: 0.8125 - val_loss: 1.2542 - val_accuracy: 0.4062\n",
      "Epoch 22/50\n",
      "16/16 [==============================] - 0s 6ms/step - loss: 0.5229 - accuracy: 0.8320 - val_loss: 1.2265 - val_accuracy: 0.3906\n",
      "Epoch 23/50\n",
      "16/16 [==============================] - 0s 6ms/step - loss: 0.5067 - accuracy: 0.8242 - val_loss: 1.1928 - val_accuracy: 0.4062\n",
      "Epoch 24/50\n",
      "16/16 [==============================] - 0s 7ms/step - loss: 0.4674 - accuracy: 0.8516 - val_loss: 1.1470 - val_accuracy: 0.4531\n",
      "Epoch 25/50\n",
      "16/16 [==============================] - 0s 6ms/step - loss: 0.4904 - accuracy: 0.8438 - val_loss: 1.0992 - val_accuracy: 0.4844\n",
      "Epoch 26/50\n",
      "16/16 [==============================] - 0s 6ms/step - loss: 0.4658 - accuracy: 0.8320 - val_loss: 1.0661 - val_accuracy: 0.4844\n",
      "Epoch 27/50\n",
      "16/16 [==============================] - 0s 6ms/step - loss: 0.4909 - accuracy: 0.8086 - val_loss: 1.0275 - val_accuracy: 0.5469\n",
      "Epoch 28/50\n",
      "16/16 [==============================] - 0s 6ms/step - loss: 0.4731 - accuracy: 0.8359 - val_loss: 0.9831 - val_accuracy: 0.5781\n",
      "Epoch 29/50\n",
      "16/16 [==============================] - 0s 6ms/step - loss: 0.4060 - accuracy: 0.8555 - val_loss: 0.9455 - val_accuracy: 0.6406\n",
      "Epoch 30/50\n",
      "16/16 [==============================] - 0s 6ms/step - loss: 0.3836 - accuracy: 0.8945 - val_loss: 0.9261 - val_accuracy: 0.6719\n",
      "Epoch 31/50\n",
      "16/16 [==============================] - 0s 8ms/step - loss: 0.3723 - accuracy: 0.9023 - val_loss: 0.8973 - val_accuracy: 0.6875\n",
      "Epoch 32/50\n",
      "16/16 [==============================] - 0s 7ms/step - loss: 0.4011 - accuracy: 0.8555 - val_loss: 0.8788 - val_accuracy: 0.7188\n",
      "Epoch 33/50\n",
      "16/16 [==============================] - 0s 6ms/step - loss: 0.3327 - accuracy: 0.9219 - val_loss: 0.8533 - val_accuracy: 0.7188\n",
      "Epoch 34/50\n",
      "16/16 [==============================] - 0s 6ms/step - loss: 0.3340 - accuracy: 0.9141 - val_loss: 0.8228 - val_accuracy: 0.7344\n",
      "Epoch 35/50\n",
      "16/16 [==============================] - 0s 7ms/step - loss: 0.3699 - accuracy: 0.8789 - val_loss: 0.8007 - val_accuracy: 0.7188\n",
      "Epoch 36/50\n",
      "16/16 [==============================] - 0s 6ms/step - loss: 0.3564 - accuracy: 0.8594 - val_loss: 0.8002 - val_accuracy: 0.7188\n",
      "Epoch 37/50\n",
      "16/16 [==============================] - 0s 6ms/step - loss: 0.2939 - accuracy: 0.9180 - val_loss: 0.7791 - val_accuracy: 0.7031\n",
      "Epoch 38/50\n",
      "16/16 [==============================] - 0s 6ms/step - loss: 0.2736 - accuracy: 0.9219 - val_loss: 0.7717 - val_accuracy: 0.6875\n",
      "Epoch 39/50\n",
      "16/16 [==============================] - 0s 7ms/step - loss: 0.3351 - accuracy: 0.8984 - val_loss: 0.7656 - val_accuracy: 0.6719\n",
      "Epoch 40/50\n",
      "16/16 [==============================] - 0s 11ms/step - loss: 0.2864 - accuracy: 0.9023 - val_loss: 0.7511 - val_accuracy: 0.6875\n",
      "Epoch 41/50\n",
      "16/16 [==============================] - 0s 6ms/step - loss: 0.2726 - accuracy: 0.9258 - val_loss: 0.7463 - val_accuracy: 0.7031\n",
      "Epoch 42/50\n",
      "16/16 [==============================] - 0s 6ms/step - loss: 0.2496 - accuracy: 0.9531 - val_loss: 0.7417 - val_accuracy: 0.6875\n",
      "Epoch 43/50\n",
      "16/16 [==============================] - 0s 8ms/step - loss: 0.2847 - accuracy: 0.9023 - val_loss: 0.7353 - val_accuracy: 0.7188\n",
      "Epoch 44/50\n",
      "16/16 [==============================] - 0s 6ms/step - loss: 0.2778 - accuracy: 0.9180 - val_loss: 0.7345 - val_accuracy: 0.7188\n",
      "Epoch 45/50\n",
      "16/16 [==============================] - 0s 6ms/step - loss: 0.2616 - accuracy: 0.9297 - val_loss: 0.7406 - val_accuracy: 0.7188\n",
      "Epoch 46/50\n",
      "16/16 [==============================] - 0s 6ms/step - loss: 0.2567 - accuracy: 0.9102 - val_loss: 0.7279 - val_accuracy: 0.7344\n",
      "Epoch 47/50\n",
      "16/16 [==============================] - 0s 6ms/step - loss: 0.2612 - accuracy: 0.9102 - val_loss: 0.7205 - val_accuracy: 0.7031\n",
      "Epoch 48/50\n",
      "16/16 [==============================] - 0s 6ms/step - loss: 0.2375 - accuracy: 0.9570 - val_loss: 0.7088 - val_accuracy: 0.7188\n",
      "Epoch 49/50\n",
      "16/16 [==============================] - 0s 6ms/step - loss: 0.2102 - accuracy: 0.9453 - val_loss: 0.7087 - val_accuracy: 0.7344\n",
      "Epoch 50/50\n",
      "16/16 [==============================] - 0s 6ms/step - loss: 0.1899 - accuracy: 0.9492 - val_loss: 0.7084 - val_accuracy: 0.7188\n"
     ]
    }
   ],
   "source": [
    "history12 = model12.fit(X2_train_scaled[:,17:], y2_train, \n",
    "                    validation_data=(X2_val_scaled[:,17:], y2_val), \n",
    "                    batch_size=16, epochs=50)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "313777ce",
   "metadata": {},
   "source": [
    "**Mel Spectrogram + MFCCs**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "8e08c557",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"Conv1D_13\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " Conv1D_1 (Conv1D)           (None, 42, 16)            96        \n",
      "                                                                 \n",
      " BN1 (BatchNormalization)    (None, 42, 16)            64        \n",
      "                                                                 \n",
      " MaxPooling1 (MaxPooling1D)  (None, 21, 16)            0         \n",
      "                                                                 \n",
      " Conv1D_2 (Conv1D)           (None, 21, 32)            3616      \n",
      "                                                                 \n",
      " BN2 (BatchNormalization)    (None, 21, 32)            128       \n",
      "                                                                 \n",
      " MaxPooling2 (MaxPooling1D)  (None, 10, 32)            0         \n",
      "                                                                 \n",
      " Conv1D_3 (Conv1D)           (None, 10, 32)            7200      \n",
      "                                                                 \n",
      " BN3 (BatchNormalization)    (None, 10, 32)            128       \n",
      "                                                                 \n",
      " MaxPooling3 (MaxPooling1D)  (None, 5, 32)             0         \n",
      "                                                                 \n",
      " Flatten (Flatten)           (None, 160)               0         \n",
      "                                                                 \n",
      " Dense_1 (Dense)             (None, 128)               20608     \n",
      "                                                                 \n",
      " Dropout_1 (Dropout)         (None, 128)               0         \n",
      "                                                                 \n",
      " Dense_2 (Dense)             (None, 64)                8256      \n",
      "                                                                 \n",
      " Dropout_2 (Dropout)         (None, 64)                0         \n",
      "                                                                 \n",
      " Softmax (Dense)             (None, 4)                 260       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 40,356\n",
      "Trainable params: 40,196\n",
      "Non-trainable params: 160\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "f1,k1,a1 = 16,5,'relu'\n",
    "f2,k2,a2 = 32,7,'relu'\n",
    "f3,k3,a3 = 32,7,'relu'\n",
    "d1,dr1,da1,r1 = 128,0.2,'relu',0.15\n",
    "d2,dr2,da2,r2 = 64,0.2,'relu',0.15\n",
    "num = 13\n",
    "\n",
    "model13 = modelBuilder3L(X2_train_scaled[:,15:],\n",
    "                        f1,k1,a1,\n",
    "                        f2,k2,a2,\n",
    "                        f3,k3,a3,\n",
    "                        d1,dr1,da1,r1,\n",
    "                        d2,dr2,da2,r2,\n",
    "                        num)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "90ab1fc3",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "16/16 [==============================] - 3s 27ms/step - loss: 1.6709 - accuracy: 0.2461 - val_loss: 1.3858 - val_accuracy: 0.2969\n",
      "Epoch 2/50\n",
      "16/16 [==============================] - 0s 7ms/step - loss: 1.4097 - accuracy: 0.3945 - val_loss: 1.3867 - val_accuracy: 0.2188\n",
      "Epoch 3/50\n",
      "16/16 [==============================] - 0s 6ms/step - loss: 1.3042 - accuracy: 0.3867 - val_loss: 1.3919 - val_accuracy: 0.2500\n",
      "Epoch 4/50\n",
      "16/16 [==============================] - 0s 6ms/step - loss: 1.2483 - accuracy: 0.4297 - val_loss: 1.3991 - val_accuracy: 0.2500\n",
      "Epoch 5/50\n",
      "16/16 [==============================] - 0s 7ms/step - loss: 1.0991 - accuracy: 0.5586 - val_loss: 1.4069 - val_accuracy: 0.2500\n",
      "Epoch 6/50\n",
      "16/16 [==============================] - 0s 6ms/step - loss: 1.0377 - accuracy: 0.5703 - val_loss: 1.4125 - val_accuracy: 0.2500\n",
      "Epoch 7/50\n",
      "16/16 [==============================] - 0s 6ms/step - loss: 0.9320 - accuracy: 0.6016 - val_loss: 1.4186 - val_accuracy: 0.2500\n",
      "Epoch 8/50\n",
      "16/16 [==============================] - 0s 6ms/step - loss: 0.9034 - accuracy: 0.6484 - val_loss: 1.4271 - val_accuracy: 0.2344\n",
      "Epoch 9/50\n",
      "16/16 [==============================] - 0s 7ms/step - loss: 0.8936 - accuracy: 0.6328 - val_loss: 1.4389 - val_accuracy: 0.2344\n",
      "Epoch 10/50\n",
      "16/16 [==============================] - 0s 6ms/step - loss: 0.8290 - accuracy: 0.6680 - val_loss: 1.4451 - val_accuracy: 0.2500\n",
      "Epoch 11/50\n",
      "16/16 [==============================] - 0s 6ms/step - loss: 0.8041 - accuracy: 0.6680 - val_loss: 1.4539 - val_accuracy: 0.2500\n",
      "Epoch 12/50\n",
      "16/16 [==============================] - 0s 6ms/step - loss: 0.7621 - accuracy: 0.7305 - val_loss: 1.4583 - val_accuracy: 0.2500\n",
      "Epoch 13/50\n",
      "16/16 [==============================] - 0s 6ms/step - loss: 0.7754 - accuracy: 0.6758 - val_loss: 1.4615 - val_accuracy: 0.2812\n",
      "Epoch 14/50\n",
      "16/16 [==============================] - 0s 6ms/step - loss: 0.7560 - accuracy: 0.7305 - val_loss: 1.4618 - val_accuracy: 0.2188\n",
      "Epoch 15/50\n",
      "16/16 [==============================] - 0s 6ms/step - loss: 0.7044 - accuracy: 0.7383 - val_loss: 1.4627 - val_accuracy: 0.2656\n",
      "Epoch 16/50\n",
      "16/16 [==============================] - 0s 6ms/step - loss: 0.6783 - accuracy: 0.7578 - val_loss: 1.4667 - val_accuracy: 0.2656\n",
      "Epoch 17/50\n",
      "16/16 [==============================] - 0s 6ms/step - loss: 0.6128 - accuracy: 0.7969 - val_loss: 1.4593 - val_accuracy: 0.2656\n",
      "Epoch 18/50\n",
      "16/16 [==============================] - 0s 6ms/step - loss: 0.6021 - accuracy: 0.7656 - val_loss: 1.4466 - val_accuracy: 0.2656\n",
      "Epoch 19/50\n",
      "16/16 [==============================] - 0s 6ms/step - loss: 0.6070 - accuracy: 0.7617 - val_loss: 1.4400 - val_accuracy: 0.2812\n",
      "Epoch 20/50\n",
      "16/16 [==============================] - 0s 7ms/step - loss: 0.6048 - accuracy: 0.7305 - val_loss: 1.4239 - val_accuracy: 0.2812\n",
      "Epoch 21/50\n",
      "16/16 [==============================] - 0s 6ms/step - loss: 0.5348 - accuracy: 0.7930 - val_loss: 1.3919 - val_accuracy: 0.2969\n",
      "Epoch 22/50\n",
      "16/16 [==============================] - 0s 7ms/step - loss: 0.5169 - accuracy: 0.8320 - val_loss: 1.3635 - val_accuracy: 0.2969\n",
      "Epoch 23/50\n",
      "16/16 [==============================] - 0s 7ms/step - loss: 0.4988 - accuracy: 0.8281 - val_loss: 1.3064 - val_accuracy: 0.3281\n",
      "Epoch 24/50\n",
      "16/16 [==============================] - 0s 7ms/step - loss: 0.5092 - accuracy: 0.8477 - val_loss: 1.2627 - val_accuracy: 0.3750\n",
      "Epoch 25/50\n",
      "16/16 [==============================] - 0s 14ms/step - loss: 0.4820 - accuracy: 0.8438 - val_loss: 1.2331 - val_accuracy: 0.4219\n",
      "Epoch 26/50\n",
      "16/16 [==============================] - 0s 6ms/step - loss: 0.4685 - accuracy: 0.8281 - val_loss: 1.1886 - val_accuracy: 0.4531\n",
      "Epoch 27/50\n",
      "16/16 [==============================] - 0s 6ms/step - loss: 0.4698 - accuracy: 0.8398 - val_loss: 1.1436 - val_accuracy: 0.4531\n",
      "Epoch 28/50\n",
      "16/16 [==============================] - 0s 7ms/step - loss: 0.4910 - accuracy: 0.8086 - val_loss: 1.1095 - val_accuracy: 0.5000\n",
      "Epoch 29/50\n",
      "16/16 [==============================] - 0s 7ms/step - loss: 0.4046 - accuracy: 0.8867 - val_loss: 1.0725 - val_accuracy: 0.5469\n",
      "Epoch 30/50\n",
      "16/16 [==============================] - 0s 6ms/step - loss: 0.4010 - accuracy: 0.8750 - val_loss: 1.0388 - val_accuracy: 0.5469\n",
      "Epoch 31/50\n",
      "16/16 [==============================] - 0s 6ms/step - loss: 0.4128 - accuracy: 0.8594 - val_loss: 1.0008 - val_accuracy: 0.5625\n",
      "Epoch 32/50\n",
      "16/16 [==============================] - 0s 6ms/step - loss: 0.3993 - accuracy: 0.8750 - val_loss: 0.9728 - val_accuracy: 0.5625\n",
      "Epoch 33/50\n",
      "16/16 [==============================] - 0s 7ms/step - loss: 0.3618 - accuracy: 0.9062 - val_loss: 0.9719 - val_accuracy: 0.5781\n",
      "Epoch 34/50\n",
      "16/16 [==============================] - 0s 6ms/step - loss: 0.4019 - accuracy: 0.8906 - val_loss: 0.9554 - val_accuracy: 0.5781\n",
      "Epoch 35/50\n",
      "16/16 [==============================] - 0s 6ms/step - loss: 0.3264 - accuracy: 0.9258 - val_loss: 0.9377 - val_accuracy: 0.5781\n",
      "Epoch 36/50\n",
      "16/16 [==============================] - 0s 6ms/step - loss: 0.3501 - accuracy: 0.8711 - val_loss: 0.9154 - val_accuracy: 0.5938\n",
      "Epoch 37/50\n",
      "16/16 [==============================] - 0s 6ms/step - loss: 0.3510 - accuracy: 0.8984 - val_loss: 0.9091 - val_accuracy: 0.5781\n",
      "Epoch 38/50\n",
      "16/16 [==============================] - 0s 6ms/step - loss: 0.2927 - accuracy: 0.9141 - val_loss: 0.9159 - val_accuracy: 0.5938\n",
      "Epoch 39/50\n",
      "16/16 [==============================] - 0s 7ms/step - loss: 0.3319 - accuracy: 0.8906 - val_loss: 0.9125 - val_accuracy: 0.6094\n",
      "Epoch 40/50\n",
      "16/16 [==============================] - 0s 6ms/step - loss: 0.3151 - accuracy: 0.9023 - val_loss: 0.9063 - val_accuracy: 0.6250\n",
      "Epoch 41/50\n",
      "16/16 [==============================] - 0s 6ms/step - loss: 0.2970 - accuracy: 0.9102 - val_loss: 0.9154 - val_accuracy: 0.6406\n",
      "Epoch 42/50\n",
      "16/16 [==============================] - 0s 7ms/step - loss: 0.2939 - accuracy: 0.9062 - val_loss: 0.9261 - val_accuracy: 0.6562\n",
      "Epoch 43/50\n",
      "16/16 [==============================] - 0s 6ms/step - loss: 0.2866 - accuracy: 0.9219 - val_loss: 0.9353 - val_accuracy: 0.6562\n",
      "Epoch 44/50\n",
      "16/16 [==============================] - 0s 6ms/step - loss: 0.2789 - accuracy: 0.9141 - val_loss: 0.9178 - val_accuracy: 0.6719\n",
      "Epoch 45/50\n",
      "16/16 [==============================] - 0s 6ms/step - loss: 0.2515 - accuracy: 0.9258 - val_loss: 0.8996 - val_accuracy: 0.6719\n",
      "Epoch 46/50\n",
      "16/16 [==============================] - 0s 6ms/step - loss: 0.2430 - accuracy: 0.9453 - val_loss: 0.9133 - val_accuracy: 0.6719\n",
      "Epoch 47/50\n",
      "16/16 [==============================] - 0s 6ms/step - loss: 0.2778 - accuracy: 0.9102 - val_loss: 0.9315 - val_accuracy: 0.6875\n",
      "Epoch 48/50\n",
      "16/16 [==============================] - 0s 6ms/step - loss: 0.2575 - accuracy: 0.9180 - val_loss: 0.9402 - val_accuracy: 0.6562\n",
      "Epoch 49/50\n",
      "16/16 [==============================] - 0s 6ms/step - loss: 0.2112 - accuracy: 0.9570 - val_loss: 0.9696 - val_accuracy: 0.6562\n",
      "Epoch 50/50\n",
      "16/16 [==============================] - 0s 7ms/step - loss: 0.2689 - accuracy: 0.9141 - val_loss: 0.9943 - val_accuracy: 0.6562\n"
     ]
    }
   ],
   "source": [
    "history13 = model13.fit(X2_train_scaled[:,15:], y2_train, \n",
    "                    validation_data=(X2_val_scaled[:,15:], y2_val), \n",
    "                    batch_size=16, epochs=50)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ab1b1bd",
   "metadata": {},
   "source": [
    "**Mel Spectrogram + MFCCs + Tonnetz**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "25cc8197",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"Conv1D_14\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " Conv1D_1 (Conv1D)           (None, 44, 16)            96        \n",
      "                                                                 \n",
      " BN1 (BatchNormalization)    (None, 44, 16)            64        \n",
      "                                                                 \n",
      " MaxPooling1 (MaxPooling1D)  (None, 22, 16)            0         \n",
      "                                                                 \n",
      " Conv1D_2 (Conv1D)           (None, 22, 32)            3616      \n",
      "                                                                 \n",
      " BN2 (BatchNormalization)    (None, 22, 32)            128       \n",
      "                                                                 \n",
      " MaxPooling2 (MaxPooling1D)  (None, 11, 32)            0         \n",
      "                                                                 \n",
      " Conv1D_3 (Conv1D)           (None, 11, 32)            7200      \n",
      "                                                                 \n",
      " BN3 (BatchNormalization)    (None, 11, 32)            128       \n",
      "                                                                 \n",
      " MaxPooling3 (MaxPooling1D)  (None, 5, 32)             0         \n",
      "                                                                 \n",
      " Flatten (Flatten)           (None, 160)               0         \n",
      "                                                                 \n",
      " Dense_1 (Dense)             (None, 128)               20608     \n",
      "                                                                 \n",
      " Dropout_1 (Dropout)         (None, 128)               0         \n",
      "                                                                 \n",
      " Dense_2 (Dense)             (None, 64)                8256      \n",
      "                                                                 \n",
      " Dropout_2 (Dropout)         (None, 64)                0         \n",
      "                                                                 \n",
      " Softmax (Dense)             (None, 4)                 260       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 40,356\n",
      "Trainable params: 40,196\n",
      "Non-trainable params: 160\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "f1,k1,a1 = 16,5,'relu'\n",
    "f2,k2,a2 = 32,7,'relu'\n",
    "f3,k3,a3 = 32,7,'relu'\n",
    "d1,dr1,da1,r1 = 128,0.2,'relu',0.15\n",
    "d2,dr2,da2,r2 = 64,0.2,'relu',0.15\n",
    "num = 14\n",
    "\n",
    "model14 = modelBuilder3L(X2_train_scaled[:,13:],\n",
    "                        f1,k1,a1,\n",
    "                        f2,k2,a2,\n",
    "                        f3,k3,a3,\n",
    "                        d1,dr1,da1,r1,\n",
    "                        d2,dr2,da2,r2,\n",
    "                        num)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "5a12d654",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/80\n",
      "16/16 [==============================] - 4s 29ms/step - loss: 1.8445 - accuracy: 0.2461 - val_loss: 1.3826 - val_accuracy: 0.3594\n",
      "Epoch 2/80\n",
      "16/16 [==============================] - 0s 8ms/step - loss: 1.5098 - accuracy: 0.3594 - val_loss: 1.3874 - val_accuracy: 0.2500\n",
      "Epoch 3/80\n",
      "16/16 [==============================] - 0s 19ms/step - loss: 1.3249 - accuracy: 0.4180 - val_loss: 1.3854 - val_accuracy: 0.1875\n",
      "Epoch 4/80\n",
      "16/16 [==============================] - 0s 8ms/step - loss: 1.2079 - accuracy: 0.4844 - val_loss: 1.3829 - val_accuracy: 0.1094\n",
      "Epoch 5/80\n",
      "16/16 [==============================] - 0s 8ms/step - loss: 1.1331 - accuracy: 0.5273 - val_loss: 1.3785 - val_accuracy: 0.2031\n",
      "Epoch 6/80\n",
      "16/16 [==============================] - 0s 10ms/step - loss: 1.0997 - accuracy: 0.5469 - val_loss: 1.3726 - val_accuracy: 0.2188\n",
      "Epoch 7/80\n",
      "16/16 [==============================] - 0s 7ms/step - loss: 1.0317 - accuracy: 0.5898 - val_loss: 1.3678 - val_accuracy: 0.2656\n",
      "Epoch 8/80\n",
      "16/16 [==============================] - 0s 8ms/step - loss: 0.8903 - accuracy: 0.6680 - val_loss: 1.3640 - val_accuracy: 0.1875\n",
      "Epoch 9/80\n",
      "16/16 [==============================] - 0s 11ms/step - loss: 0.9306 - accuracy: 0.6250 - val_loss: 1.3616 - val_accuracy: 0.2344\n",
      "Epoch 10/80\n",
      "16/16 [==============================] - 0s 9ms/step - loss: 0.8300 - accuracy: 0.7031 - val_loss: 1.3588 - val_accuracy: 0.2656\n",
      "Epoch 11/80\n",
      "16/16 [==============================] - 0s 9ms/step - loss: 0.7925 - accuracy: 0.6836 - val_loss: 1.3570 - val_accuracy: 0.2656\n",
      "Epoch 12/80\n",
      "16/16 [==============================] - 0s 8ms/step - loss: 0.7183 - accuracy: 0.7617 - val_loss: 1.3515 - val_accuracy: 0.3125\n",
      "Epoch 13/80\n",
      "16/16 [==============================] - 0s 7ms/step - loss: 0.7266 - accuracy: 0.7461 - val_loss: 1.3429 - val_accuracy: 0.2969\n",
      "Epoch 14/80\n",
      "16/16 [==============================] - 0s 7ms/step - loss: 0.6908 - accuracy: 0.7578 - val_loss: 1.3366 - val_accuracy: 0.3281\n",
      "Epoch 15/80\n",
      "16/16 [==============================] - 0s 7ms/step - loss: 0.6873 - accuracy: 0.7383 - val_loss: 1.3298 - val_accuracy: 0.3281\n",
      "Epoch 16/80\n",
      "16/16 [==============================] - 0s 7ms/step - loss: 0.6358 - accuracy: 0.7617 - val_loss: 1.3201 - val_accuracy: 0.3750\n",
      "Epoch 17/80\n",
      "16/16 [==============================] - 0s 6ms/step - loss: 0.6332 - accuracy: 0.7695 - val_loss: 1.3055 - val_accuracy: 0.3906\n",
      "Epoch 18/80\n",
      "16/16 [==============================] - 0s 7ms/step - loss: 0.6309 - accuracy: 0.7539 - val_loss: 1.2828 - val_accuracy: 0.4062\n",
      "Epoch 19/80\n",
      "16/16 [==============================] - 0s 6ms/step - loss: 0.6018 - accuracy: 0.7578 - val_loss: 1.2625 - val_accuracy: 0.4219\n",
      "Epoch 20/80\n",
      "16/16 [==============================] - 0s 6ms/step - loss: 0.5573 - accuracy: 0.7773 - val_loss: 1.2429 - val_accuracy: 0.3906\n",
      "Epoch 21/80\n",
      "16/16 [==============================] - 0s 14ms/step - loss: 0.5115 - accuracy: 0.8203 - val_loss: 1.2173 - val_accuracy: 0.4688\n",
      "Epoch 22/80\n",
      "16/16 [==============================] - 0s 7ms/step - loss: 0.4938 - accuracy: 0.8281 - val_loss: 1.1810 - val_accuracy: 0.5156\n",
      "Epoch 23/80\n",
      "16/16 [==============================] - 0s 9ms/step - loss: 0.5408 - accuracy: 0.7930 - val_loss: 1.1416 - val_accuracy: 0.5312\n",
      "Epoch 24/80\n",
      "16/16 [==============================] - 0s 7ms/step - loss: 0.4945 - accuracy: 0.8242 - val_loss: 1.0989 - val_accuracy: 0.5625\n",
      "Epoch 25/80\n",
      "16/16 [==============================] - 0s 7ms/step - loss: 0.4725 - accuracy: 0.8242 - val_loss: 1.0634 - val_accuracy: 0.5625\n",
      "Epoch 26/80\n",
      "16/16 [==============================] - 0s 7ms/step - loss: 0.4474 - accuracy: 0.8438 - val_loss: 1.0279 - val_accuracy: 0.5781\n",
      "Epoch 27/80\n",
      "16/16 [==============================] - 0s 7ms/step - loss: 0.4302 - accuracy: 0.8477 - val_loss: 0.9845 - val_accuracy: 0.6094\n",
      "Epoch 28/80\n",
      "16/16 [==============================] - 0s 8ms/step - loss: 0.4300 - accuracy: 0.8516 - val_loss: 0.9516 - val_accuracy: 0.6094\n",
      "Epoch 29/80\n",
      "16/16 [==============================] - 0s 7ms/step - loss: 0.4292 - accuracy: 0.8516 - val_loss: 0.9259 - val_accuracy: 0.6406\n",
      "Epoch 30/80\n",
      "16/16 [==============================] - 0s 8ms/step - loss: 0.4109 - accuracy: 0.8477 - val_loss: 0.8979 - val_accuracy: 0.6719\n",
      "Epoch 31/80\n",
      "16/16 [==============================] - 0s 7ms/step - loss: 0.4137 - accuracy: 0.8477 - val_loss: 0.8620 - val_accuracy: 0.7031\n",
      "Epoch 32/80\n",
      "16/16 [==============================] - 0s 7ms/step - loss: 0.4059 - accuracy: 0.8633 - val_loss: 0.8541 - val_accuracy: 0.6562\n",
      "Epoch 33/80\n",
      "16/16 [==============================] - 0s 7ms/step - loss: 0.3747 - accuracy: 0.8750 - val_loss: 0.8202 - val_accuracy: 0.7031\n",
      "Epoch 34/80\n",
      "16/16 [==============================] - 0s 7ms/step - loss: 0.3254 - accuracy: 0.8984 - val_loss: 0.7866 - val_accuracy: 0.7188\n",
      "Epoch 35/80\n",
      "16/16 [==============================] - 0s 7ms/step - loss: 0.2938 - accuracy: 0.9336 - val_loss: 0.7571 - val_accuracy: 0.7188\n",
      "Epoch 36/80\n",
      "16/16 [==============================] - 0s 7ms/step - loss: 0.3121 - accuracy: 0.9102 - val_loss: 0.7366 - val_accuracy: 0.7188\n",
      "Epoch 37/80\n",
      "16/16 [==============================] - 0s 7ms/step - loss: 0.3045 - accuracy: 0.9102 - val_loss: 0.7241 - val_accuracy: 0.7188\n",
      "Epoch 38/80\n",
      "16/16 [==============================] - 0s 8ms/step - loss: 0.3155 - accuracy: 0.9141 - val_loss: 0.7066 - val_accuracy: 0.7188\n",
      "Epoch 39/80\n",
      "16/16 [==============================] - 0s 8ms/step - loss: 0.2624 - accuracy: 0.9258 - val_loss: 0.6911 - val_accuracy: 0.7344\n",
      "Epoch 40/80\n",
      "16/16 [==============================] - 0s 8ms/step - loss: 0.2569 - accuracy: 0.9219 - val_loss: 0.6738 - val_accuracy: 0.7344\n",
      "Epoch 41/80\n",
      "16/16 [==============================] - 0s 6ms/step - loss: 0.2480 - accuracy: 0.9180 - val_loss: 0.6726 - val_accuracy: 0.7344\n",
      "Epoch 42/80\n",
      "16/16 [==============================] - 0s 8ms/step - loss: 0.2972 - accuracy: 0.9102 - val_loss: 0.6529 - val_accuracy: 0.7344\n",
      "Epoch 43/80\n",
      "16/16 [==============================] - 0s 13ms/step - loss: 0.2487 - accuracy: 0.9297 - val_loss: 0.6433 - val_accuracy: 0.7500\n",
      "Epoch 44/80\n",
      "16/16 [==============================] - 0s 9ms/step - loss: 0.2487 - accuracy: 0.9414 - val_loss: 0.6455 - val_accuracy: 0.7656\n",
      "Epoch 45/80\n",
      "16/16 [==============================] - 0s 7ms/step - loss: 0.2311 - accuracy: 0.9297 - val_loss: 0.6425 - val_accuracy: 0.7656\n",
      "Epoch 46/80\n",
      "16/16 [==============================] - 0s 7ms/step - loss: 0.2352 - accuracy: 0.9297 - val_loss: 0.6413 - val_accuracy: 0.7656\n",
      "Epoch 47/80\n",
      "16/16 [==============================] - 0s 7ms/step - loss: 0.2152 - accuracy: 0.9531 - val_loss: 0.6291 - val_accuracy: 0.7656\n",
      "Epoch 48/80\n",
      "16/16 [==============================] - 0s 7ms/step - loss: 0.2492 - accuracy: 0.9141 - val_loss: 0.6223 - val_accuracy: 0.7656\n",
      "Epoch 49/80\n",
      "16/16 [==============================] - 0s 7ms/step - loss: 0.1970 - accuracy: 0.9414 - val_loss: 0.6219 - val_accuracy: 0.7500\n",
      "Epoch 50/80\n",
      "16/16 [==============================] - 0s 7ms/step - loss: 0.2271 - accuracy: 0.9492 - val_loss: 0.6213 - val_accuracy: 0.7500\n",
      "Epoch 51/80\n",
      "16/16 [==============================] - 0s 7ms/step - loss: 0.2087 - accuracy: 0.9375 - val_loss: 0.6289 - val_accuracy: 0.7344\n",
      "Epoch 52/80\n",
      "16/16 [==============================] - 0s 8ms/step - loss: 0.1929 - accuracy: 0.9492 - val_loss: 0.6328 - val_accuracy: 0.7500\n",
      "Epoch 53/80\n",
      "16/16 [==============================] - 0s 7ms/step - loss: 0.2343 - accuracy: 0.9336 - val_loss: 0.6211 - val_accuracy: 0.7500\n",
      "Epoch 54/80\n",
      "16/16 [==============================] - 0s 7ms/step - loss: 0.1889 - accuracy: 0.9453 - val_loss: 0.6237 - val_accuracy: 0.7656\n",
      "Epoch 55/80\n",
      "16/16 [==============================] - 0s 7ms/step - loss: 0.1513 - accuracy: 0.9648 - val_loss: 0.6197 - val_accuracy: 0.7656\n",
      "Epoch 56/80\n",
      "16/16 [==============================] - 0s 8ms/step - loss: 0.1559 - accuracy: 0.9648 - val_loss: 0.6187 - val_accuracy: 0.7500\n",
      "Epoch 57/80\n",
      "16/16 [==============================] - 0s 7ms/step - loss: 0.2004 - accuracy: 0.9414 - val_loss: 0.6440 - val_accuracy: 0.7188\n",
      "Epoch 58/80\n",
      "16/16 [==============================] - 0s 9ms/step - loss: 0.1551 - accuracy: 0.9727 - val_loss: 0.6585 - val_accuracy: 0.7188\n",
      "Epoch 59/80\n",
      "16/16 [==============================] - 0s 8ms/step - loss: 0.1567 - accuracy: 0.9648 - val_loss: 0.6417 - val_accuracy: 0.7344\n",
      "Epoch 60/80\n",
      "16/16 [==============================] - 0s 7ms/step - loss: 0.1661 - accuracy: 0.9414 - val_loss: 0.6478 - val_accuracy: 0.7500\n",
      "Epoch 61/80\n",
      "16/16 [==============================] - 0s 7ms/step - loss: 0.1326 - accuracy: 0.9766 - val_loss: 0.6347 - val_accuracy: 0.7500\n",
      "Epoch 62/80\n",
      "16/16 [==============================] - 0s 7ms/step - loss: 0.1456 - accuracy: 0.9648 - val_loss: 0.6250 - val_accuracy: 0.7344\n",
      "Epoch 63/80\n",
      "16/16 [==============================] - 0s 8ms/step - loss: 0.1311 - accuracy: 0.9766 - val_loss: 0.6334 - val_accuracy: 0.7344\n",
      "Epoch 64/80\n",
      "16/16 [==============================] - 0s 7ms/step - loss: 0.1152 - accuracy: 0.9727 - val_loss: 0.6523 - val_accuracy: 0.7344\n",
      "Epoch 65/80\n",
      "16/16 [==============================] - 0s 7ms/step - loss: 0.1032 - accuracy: 0.9844 - val_loss: 0.6510 - val_accuracy: 0.7188\n",
      "Epoch 66/80\n",
      "16/16 [==============================] - 0s 7ms/step - loss: 0.0908 - accuracy: 1.0000 - val_loss: 0.6567 - val_accuracy: 0.7344\n",
      "Epoch 67/80\n",
      "16/16 [==============================] - 0s 16ms/step - loss: 0.1477 - accuracy: 0.9727 - val_loss: 0.6226 - val_accuracy: 0.7656\n",
      "Epoch 68/80\n",
      "16/16 [==============================] - 0s 9ms/step - loss: 0.1538 - accuracy: 0.9492 - val_loss: 0.6204 - val_accuracy: 0.7344\n",
      "Epoch 69/80\n",
      "16/16 [==============================] - 0s 8ms/step - loss: 0.1172 - accuracy: 0.9727 - val_loss: 0.6312 - val_accuracy: 0.7656\n",
      "Epoch 70/80\n",
      "16/16 [==============================] - 0s 7ms/step - loss: 0.0992 - accuracy: 0.9805 - val_loss: 0.6521 - val_accuracy: 0.7656\n",
      "Epoch 71/80\n",
      "16/16 [==============================] - 0s 7ms/step - loss: 0.0776 - accuracy: 0.9844 - val_loss: 0.6623 - val_accuracy: 0.7500\n",
      "Epoch 72/80\n",
      "16/16 [==============================] - 0s 7ms/step - loss: 0.1244 - accuracy: 0.9648 - val_loss: 0.6710 - val_accuracy: 0.7500\n",
      "Epoch 73/80\n",
      "16/16 [==============================] - 0s 8ms/step - loss: 0.1257 - accuracy: 0.9727 - val_loss: 0.6642 - val_accuracy: 0.7344\n",
      "Epoch 74/80\n",
      "16/16 [==============================] - 0s 9ms/step - loss: 0.0740 - accuracy: 0.9922 - val_loss: 0.6410 - val_accuracy: 0.7188\n",
      "Epoch 75/80\n",
      "16/16 [==============================] - 0s 7ms/step - loss: 0.0875 - accuracy: 0.9844 - val_loss: 0.6318 - val_accuracy: 0.7188\n",
      "Epoch 76/80\n",
      "16/16 [==============================] - 0s 7ms/step - loss: 0.0790 - accuracy: 0.9922 - val_loss: 0.6266 - val_accuracy: 0.7188\n",
      "Epoch 77/80\n",
      "16/16 [==============================] - 0s 6ms/step - loss: 0.1056 - accuracy: 0.9688 - val_loss: 0.6521 - val_accuracy: 0.7031\n",
      "Epoch 78/80\n",
      "16/16 [==============================] - 0s 7ms/step - loss: 0.0954 - accuracy: 0.9805 - val_loss: 0.6619 - val_accuracy: 0.7188\n",
      "Epoch 79/80\n",
      "16/16 [==============================] - 0s 7ms/step - loss: 0.0681 - accuracy: 0.9922 - val_loss: 0.6590 - val_accuracy: 0.7344\n",
      "Epoch 80/80\n",
      "16/16 [==============================] - 0s 7ms/step - loss: 0.0889 - accuracy: 0.9727 - val_loss: 0.6612 - val_accuracy: 0.7031\n"
     ]
    }
   ],
   "source": [
    "history14 = model14.fit(X2_train_scaled[:,13:], y2_train, \n",
    "                    validation_data=(X2_val_scaled[:,13:], y2_val), \n",
    "                    batch_size=16, epochs=80)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d483091",
   "metadata": {},
   "source": [
    "**Mel Spectrogram + MFCCs + Tonnetz + Centroid**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "c8b84a22",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "X2_train_scaled_4 = np.concatenate([X2_train_scaled[:,7:9],X2_train_scaled[:,13:]],axis=1)\n",
    "X2_val_scaled_4 = np.concatenate([X2_val_scaled[:,7:9],X2_val_scaled[:,13:]],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "a5c71a81",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"Conv1D_15\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " Conv1D_1 (Conv1D)           (None, 46, 16)            96        \n",
      "                                                                 \n",
      " BN1 (BatchNormalization)    (None, 46, 16)            64        \n",
      "                                                                 \n",
      " MaxPooling1 (MaxPooling1D)  (None, 23, 16)            0         \n",
      "                                                                 \n",
      " Conv1D_2 (Conv1D)           (None, 23, 32)            3616      \n",
      "                                                                 \n",
      " BN2 (BatchNormalization)    (None, 23, 32)            128       \n",
      "                                                                 \n",
      " MaxPooling2 (MaxPooling1D)  (None, 11, 32)            0         \n",
      "                                                                 \n",
      " Conv1D_3 (Conv1D)           (None, 11, 32)            7200      \n",
      "                                                                 \n",
      " BN3 (BatchNormalization)    (None, 11, 32)            128       \n",
      "                                                                 \n",
      " MaxPooling3 (MaxPooling1D)  (None, 5, 32)             0         \n",
      "                                                                 \n",
      " Flatten (Flatten)           (None, 160)               0         \n",
      "                                                                 \n",
      " Dense_1 (Dense)             (None, 128)               20608     \n",
      "                                                                 \n",
      " Dropout_1 (Dropout)         (None, 128)               0         \n",
      "                                                                 \n",
      " Dense_2 (Dense)             (None, 64)                8256      \n",
      "                                                                 \n",
      " Dropout_2 (Dropout)         (None, 64)                0         \n",
      "                                                                 \n",
      " Softmax (Dense)             (None, 4)                 260       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 40,356\n",
      "Trainable params: 40,196\n",
      "Non-trainable params: 160\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "f1,k1,a1 = 16,5,'relu'\n",
    "f2,k2,a2 = 32,7,'relu'\n",
    "f3,k3,a3 = 32,7,'relu'\n",
    "d1,dr1,da1,r1 = 128,0.2,'relu',0.15\n",
    "d2,dr2,da2,r2 = 64,0.2,'relu',0.15\n",
    "num = 15\n",
    "\n",
    "model15 = modelBuilder3L(X2_train_scaled_4,\n",
    "                        f1,k1,a1,\n",
    "                        f2,k2,a2,\n",
    "                        f3,k3,a3,\n",
    "                        d1,dr1,da1,r1,\n",
    "                        d2,dr2,da2,r2,\n",
    "                        num)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "93993958",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/80\n",
      "16/16 [==============================] - 4s 33ms/step - loss: 1.7459 - accuracy: 0.3008 - val_loss: 1.3863 - val_accuracy: 0.2344\n",
      "Epoch 2/80\n",
      "16/16 [==============================] - 0s 9ms/step - loss: 1.4802 - accuracy: 0.3867 - val_loss: 1.3869 - val_accuracy: 0.2656\n",
      "Epoch 3/80\n",
      "16/16 [==============================] - 0s 15ms/step - loss: 1.3291 - accuracy: 0.3828 - val_loss: 1.3881 - val_accuracy: 0.2500\n",
      "Epoch 4/80\n",
      "16/16 [==============================] - 0s 7ms/step - loss: 1.2368 - accuracy: 0.4727 - val_loss: 1.3885 - val_accuracy: 0.2500\n",
      "Epoch 5/80\n",
      "16/16 [==============================] - 0s 8ms/step - loss: 1.1300 - accuracy: 0.4727 - val_loss: 1.3888 - val_accuracy: 0.2500\n",
      "Epoch 6/80\n",
      "16/16 [==============================] - 0s 8ms/step - loss: 1.0033 - accuracy: 0.6016 - val_loss: 1.3888 - val_accuracy: 0.2500\n",
      "Epoch 7/80\n",
      "16/16 [==============================] - 0s 7ms/step - loss: 0.9657 - accuracy: 0.6328 - val_loss: 1.3885 - val_accuracy: 0.2500\n",
      "Epoch 8/80\n",
      "16/16 [==============================] - 0s 6ms/step - loss: 0.8985 - accuracy: 0.6484 - val_loss: 1.3887 - val_accuracy: 0.2500\n",
      "Epoch 9/80\n",
      "16/16 [==============================] - 0s 6ms/step - loss: 0.8442 - accuracy: 0.6445 - val_loss: 1.3887 - val_accuracy: 0.2500\n",
      "Epoch 10/80\n",
      "16/16 [==============================] - 0s 9ms/step - loss: 0.7754 - accuracy: 0.7344 - val_loss: 1.3887 - val_accuracy: 0.2500\n",
      "Epoch 11/80\n",
      "16/16 [==============================] - 0s 8ms/step - loss: 0.7657 - accuracy: 0.7227 - val_loss: 1.3877 - val_accuracy: 0.2500\n",
      "Epoch 12/80\n",
      "16/16 [==============================] - 0s 7ms/step - loss: 0.7077 - accuracy: 0.7031 - val_loss: 1.3858 - val_accuracy: 0.2500\n",
      "Epoch 13/80\n",
      "16/16 [==============================] - 0s 7ms/step - loss: 0.7167 - accuracy: 0.7227 - val_loss: 1.3814 - val_accuracy: 0.2500\n",
      "Epoch 14/80\n",
      "16/16 [==============================] - 0s 7ms/step - loss: 0.6297 - accuracy: 0.7891 - val_loss: 1.3755 - val_accuracy: 0.2656\n",
      "Epoch 15/80\n",
      "16/16 [==============================] - 0s 8ms/step - loss: 0.5823 - accuracy: 0.7930 - val_loss: 1.3657 - val_accuracy: 0.2656\n",
      "Epoch 16/80\n",
      "16/16 [==============================] - 0s 7ms/step - loss: 0.6172 - accuracy: 0.7578 - val_loss: 1.3573 - val_accuracy: 0.2656\n",
      "Epoch 17/80\n",
      "16/16 [==============================] - 0s 6ms/step - loss: 0.5998 - accuracy: 0.7891 - val_loss: 1.3459 - val_accuracy: 0.2500\n",
      "Epoch 18/80\n",
      "16/16 [==============================] - 0s 7ms/step - loss: 0.5890 - accuracy: 0.7656 - val_loss: 1.3293 - val_accuracy: 0.2656\n",
      "Epoch 19/80\n",
      "16/16 [==============================] - 0s 7ms/step - loss: 0.5229 - accuracy: 0.8047 - val_loss: 1.3124 - val_accuracy: 0.2812\n",
      "Epoch 20/80\n",
      "16/16 [==============================] - 0s 7ms/step - loss: 0.5361 - accuracy: 0.8047 - val_loss: 1.2985 - val_accuracy: 0.3281\n",
      "Epoch 21/80\n",
      "16/16 [==============================] - 0s 7ms/step - loss: 0.5170 - accuracy: 0.8086 - val_loss: 1.2828 - val_accuracy: 0.3438\n",
      "Epoch 22/80\n",
      "16/16 [==============================] - 0s 7ms/step - loss: 0.4923 - accuracy: 0.8477 - val_loss: 1.2549 - val_accuracy: 0.3750\n",
      "Epoch 23/80\n",
      "16/16 [==============================] - 0s 7ms/step - loss: 0.4042 - accuracy: 0.8672 - val_loss: 1.2271 - val_accuracy: 0.4219\n",
      "Epoch 24/80\n",
      "16/16 [==============================] - 0s 7ms/step - loss: 0.4836 - accuracy: 0.8203 - val_loss: 1.1964 - val_accuracy: 0.4375\n",
      "Epoch 25/80\n",
      "16/16 [==============================] - 0s 7ms/step - loss: 0.3914 - accuracy: 0.8711 - val_loss: 1.1656 - val_accuracy: 0.4531\n",
      "Epoch 26/80\n",
      "16/16 [==============================] - 0s 7ms/step - loss: 0.3922 - accuracy: 0.8789 - val_loss: 1.1384 - val_accuracy: 0.4844\n",
      "Epoch 27/80\n",
      "16/16 [==============================] - 0s 8ms/step - loss: 0.3989 - accuracy: 0.8633 - val_loss: 1.1049 - val_accuracy: 0.5156\n",
      "Epoch 28/80\n",
      "16/16 [==============================] - 0s 7ms/step - loss: 0.3803 - accuracy: 0.8711 - val_loss: 1.0710 - val_accuracy: 0.5469\n",
      "Epoch 29/80\n",
      "16/16 [==============================] - 0s 7ms/step - loss: 0.3881 - accuracy: 0.8750 - val_loss: 1.0323 - val_accuracy: 0.5938\n",
      "Epoch 30/80\n",
      "16/16 [==============================] - 0s 7ms/step - loss: 0.3740 - accuracy: 0.8828 - val_loss: 0.9856 - val_accuracy: 0.6562\n",
      "Epoch 31/80\n",
      "16/16 [==============================] - 0s 9ms/step - loss: 0.2834 - accuracy: 0.9297 - val_loss: 0.9535 - val_accuracy: 0.6719\n",
      "Epoch 32/80\n",
      "16/16 [==============================] - 0s 7ms/step - loss: 0.3231 - accuracy: 0.8867 - val_loss: 0.9160 - val_accuracy: 0.6719\n",
      "Epoch 33/80\n",
      "16/16 [==============================] - 0s 8ms/step - loss: 0.3035 - accuracy: 0.9102 - val_loss: 0.8787 - val_accuracy: 0.6875\n",
      "Epoch 34/80\n",
      "16/16 [==============================] - 0s 8ms/step - loss: 0.2927 - accuracy: 0.9219 - val_loss: 0.8423 - val_accuracy: 0.7656\n",
      "Epoch 35/80\n",
      "16/16 [==============================] - 0s 7ms/step - loss: 0.3076 - accuracy: 0.9023 - val_loss: 0.8176 - val_accuracy: 0.7656\n",
      "Epoch 36/80\n",
      "16/16 [==============================] - 0s 6ms/step - loss: 0.2689 - accuracy: 0.9219 - val_loss: 0.8145 - val_accuracy: 0.7344\n",
      "Epoch 37/80\n",
      "16/16 [==============================] - 0s 6ms/step - loss: 0.2645 - accuracy: 0.9102 - val_loss: 0.8001 - val_accuracy: 0.7500\n",
      "Epoch 38/80\n",
      "16/16 [==============================] - 0s 7ms/step - loss: 0.2958 - accuracy: 0.8945 - val_loss: 0.7727 - val_accuracy: 0.7656\n",
      "Epoch 39/80\n",
      "16/16 [==============================] - 0s 7ms/step - loss: 0.2492 - accuracy: 0.9375 - val_loss: 0.7480 - val_accuracy: 0.7500\n",
      "Epoch 40/80\n",
      "16/16 [==============================] - 0s 7ms/step - loss: 0.2464 - accuracy: 0.9336 - val_loss: 0.7237 - val_accuracy: 0.7344\n",
      "Epoch 41/80\n",
      "16/16 [==============================] - 0s 8ms/step - loss: 0.2393 - accuracy: 0.9219 - val_loss: 0.7091 - val_accuracy: 0.7344\n",
      "Epoch 42/80\n",
      "16/16 [==============================] - 0s 8ms/step - loss: 0.2670 - accuracy: 0.9062 - val_loss: 0.6929 - val_accuracy: 0.7344\n",
      "Epoch 43/80\n",
      "16/16 [==============================] - 0s 8ms/step - loss: 0.2085 - accuracy: 0.9414 - val_loss: 0.6785 - val_accuracy: 0.7656\n",
      "Epoch 44/80\n",
      "16/16 [==============================] - 0s 8ms/step - loss: 0.2170 - accuracy: 0.9453 - val_loss: 0.6764 - val_accuracy: 0.7500\n",
      "Epoch 45/80\n",
      "16/16 [==============================] - 0s 6ms/step - loss: 0.1804 - accuracy: 0.9648 - val_loss: 0.6651 - val_accuracy: 0.7656\n",
      "Epoch 46/80\n",
      "16/16 [==============================] - 0s 6ms/step - loss: 0.2162 - accuracy: 0.9453 - val_loss: 0.6635 - val_accuracy: 0.7812\n",
      "Epoch 47/80\n",
      "16/16 [==============================] - 0s 6ms/step - loss: 0.1826 - accuracy: 0.9570 - val_loss: 0.6703 - val_accuracy: 0.7656\n",
      "Epoch 48/80\n",
      "16/16 [==============================] - 0s 12ms/step - loss: 0.1921 - accuracy: 0.9414 - val_loss: 0.6821 - val_accuracy: 0.7656\n",
      "Epoch 49/80\n",
      "16/16 [==============================] - 0s 9ms/step - loss: 0.2214 - accuracy: 0.9219 - val_loss: 0.6667 - val_accuracy: 0.7656\n",
      "Epoch 50/80\n",
      "16/16 [==============================] - 0s 7ms/step - loss: 0.1706 - accuracy: 0.9648 - val_loss: 0.6488 - val_accuracy: 0.7656\n",
      "Epoch 51/80\n",
      "16/16 [==============================] - 0s 6ms/step - loss: 0.1701 - accuracy: 0.9492 - val_loss: 0.6412 - val_accuracy: 0.7812\n",
      "Epoch 52/80\n",
      "16/16 [==============================] - 0s 6ms/step - loss: 0.1562 - accuracy: 0.9492 - val_loss: 0.6464 - val_accuracy: 0.7812\n",
      "Epoch 53/80\n",
      "16/16 [==============================] - 0s 6ms/step - loss: 0.1792 - accuracy: 0.9570 - val_loss: 0.6453 - val_accuracy: 0.7656\n",
      "Epoch 54/80\n",
      "16/16 [==============================] - 0s 6ms/step - loss: 0.1489 - accuracy: 0.9648 - val_loss: 0.6521 - val_accuracy: 0.7656\n",
      "Epoch 55/80\n",
      "16/16 [==============================] - 0s 7ms/step - loss: 0.1704 - accuracy: 0.9453 - val_loss: 0.6564 - val_accuracy: 0.7500\n",
      "Epoch 56/80\n",
      "16/16 [==============================] - 0s 7ms/step - loss: 0.1190 - accuracy: 0.9883 - val_loss: 0.6510 - val_accuracy: 0.7500\n",
      "Epoch 57/80\n",
      "16/16 [==============================] - 0s 8ms/step - loss: 0.1495 - accuracy: 0.9609 - val_loss: 0.6348 - val_accuracy: 0.7656\n",
      "Epoch 58/80\n",
      "16/16 [==============================] - 0s 7ms/step - loss: 0.1364 - accuracy: 0.9727 - val_loss: 0.6313 - val_accuracy: 0.7656\n",
      "Epoch 59/80\n",
      "16/16 [==============================] - 0s 9ms/step - loss: 0.1239 - accuracy: 0.9688 - val_loss: 0.6355 - val_accuracy: 0.7656\n",
      "Epoch 60/80\n",
      "16/16 [==============================] - 0s 8ms/step - loss: 0.1311 - accuracy: 0.9609 - val_loss: 0.6441 - val_accuracy: 0.7500\n",
      "Epoch 61/80\n",
      "16/16 [==============================] - 0s 8ms/step - loss: 0.1548 - accuracy: 0.9648 - val_loss: 0.6407 - val_accuracy: 0.7656\n",
      "Epoch 62/80\n",
      "16/16 [==============================] - 0s 8ms/step - loss: 0.1238 - accuracy: 0.9688 - val_loss: 0.6412 - val_accuracy: 0.7500\n",
      "Epoch 63/80\n",
      "16/16 [==============================] - 0s 6ms/step - loss: 0.1319 - accuracy: 0.9727 - val_loss: 0.6254 - val_accuracy: 0.7812\n",
      "Epoch 64/80\n",
      "16/16 [==============================] - 0s 6ms/step - loss: 0.1178 - accuracy: 0.9766 - val_loss: 0.6124 - val_accuracy: 0.7812\n",
      "Epoch 65/80\n",
      "16/16 [==============================] - 0s 6ms/step - loss: 0.1118 - accuracy: 0.9727 - val_loss: 0.6214 - val_accuracy: 0.7812\n",
      "Epoch 66/80\n",
      "16/16 [==============================] - 0s 6ms/step - loss: 0.1047 - accuracy: 0.9766 - val_loss: 0.6333 - val_accuracy: 0.7656\n",
      "Epoch 67/80\n",
      "16/16 [==============================] - 0s 6ms/step - loss: 0.0957 - accuracy: 0.9922 - val_loss: 0.6406 - val_accuracy: 0.7500\n",
      "Epoch 68/80\n",
      "16/16 [==============================] - 0s 6ms/step - loss: 0.1084 - accuracy: 0.9766 - val_loss: 0.6462 - val_accuracy: 0.7500\n",
      "Epoch 69/80\n",
      "16/16 [==============================] - 0s 6ms/step - loss: 0.1173 - accuracy: 0.9648 - val_loss: 0.6353 - val_accuracy: 0.7500\n",
      "Epoch 70/80\n",
      "16/16 [==============================] - 0s 6ms/step - loss: 0.1053 - accuracy: 0.9766 - val_loss: 0.6372 - val_accuracy: 0.7656\n",
      "Epoch 71/80\n",
      "16/16 [==============================] - 0s 6ms/step - loss: 0.1239 - accuracy: 0.9727 - val_loss: 0.6284 - val_accuracy: 0.7812\n",
      "Epoch 72/80\n",
      "16/16 [==============================] - 0s 6ms/step - loss: 0.0875 - accuracy: 0.9805 - val_loss: 0.6326 - val_accuracy: 0.7812\n",
      "Epoch 73/80\n",
      "16/16 [==============================] - 0s 6ms/step - loss: 0.0729 - accuracy: 0.9883 - val_loss: 0.6501 - val_accuracy: 0.7812\n",
      "Epoch 74/80\n",
      "16/16 [==============================] - 0s 13ms/step - loss: 0.1025 - accuracy: 0.9727 - val_loss: 0.6335 - val_accuracy: 0.7812\n",
      "Epoch 75/80\n",
      "16/16 [==============================] - 0s 7ms/step - loss: 0.0862 - accuracy: 0.9766 - val_loss: 0.6346 - val_accuracy: 0.7812\n",
      "Epoch 76/80\n",
      "16/16 [==============================] - 0s 6ms/step - loss: 0.1115 - accuracy: 0.9609 - val_loss: 0.6257 - val_accuracy: 0.7812\n",
      "Epoch 77/80\n",
      "16/16 [==============================] - 0s 7ms/step - loss: 0.0648 - accuracy: 0.9961 - val_loss: 0.6148 - val_accuracy: 0.7812\n",
      "Epoch 78/80\n",
      "16/16 [==============================] - 0s 8ms/step - loss: 0.0616 - accuracy: 0.9922 - val_loss: 0.6251 - val_accuracy: 0.7656\n",
      "Epoch 79/80\n",
      "16/16 [==============================] - 0s 7ms/step - loss: 0.0717 - accuracy: 0.9844 - val_loss: 0.6524 - val_accuracy: 0.7969\n",
      "Epoch 80/80\n",
      "16/16 [==============================] - 0s 6ms/step - loss: 0.0701 - accuracy: 0.9922 - val_loss: 0.6529 - val_accuracy: 0.7969\n"
     ]
    }
   ],
   "source": [
    "history15 = model15.fit(X2_train_scaled_4, y2_train, \n",
    "                    validation_data=(X2_val_scaled_4, y2_val), \n",
    "                    batch_size=16, epochs=80)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7010ec1e",
   "metadata": {},
   "source": [
    "## 4. Model Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "9e1b5cae",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0, 0.5, 'loss')"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAA+kAAAGJCAYAAAD2VnIMAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjcuMSwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/bCgiHAAAACXBIWXMAAA9hAAAPYQGoP6dpAADP7klEQVR4nOzdd1xV9f/A8dflApcNssEFCuLeSq7UcuRKrdzlKEelZdq0aVb6a5mWlg1XfTVXaq4st7knblEURBEQVPbmnt8fR67eAAW8eEHez8fjPrj33HPPed/r4L7P5/15fzSKoigIIYQQQgghhBDC7CzMHYAQQgghhBBCCCFUkqQLIYQQQgghhBBlhCTpQgghhBBCCCFEGSFJuhBCCCGEEEIIUUZIki6EEEIIIYQQQpQRkqQLIYQQQgghhBBlhCTpQgghhBBCCCFEGSFJuhBCCCGEEEIIUUZIki6EEEIIIYQQQpQRkqQLcYufnx/Dhw83dxiFmjx5MhqNxmhbUWNesGABGo2GiIgIk8UTERGBRqNhwYIFJjumEEIIUd7I94fiMef3h+HDh+Pn5/fAzytEcUmSLsqNPXv2MHnyZBISEswdSoWyePFiZsyYYe4whBBCiBKR7w/mId8fhCg5S3MHIERR7dmzh48//pjhw4fj4uJi8uOHhoZiYVG+rls9iJgXL17MyZMnee2114y2V69enfT0dKysrEr1/EIIIcT9kO8P+cn3ByHKNknSxUNJr9eTlZWFjY1NkV+j0+lKMaLSYc6YNRpNsT7fikpRFDIyMrC1tTV3KEIIIe5Bvj+UPvn+IMS9la/LfqLCmjx5Mm+++SYA/v7+aDQaozlSGo2GcePGsWjRIurVq4dOp2Pjxo0AfPXVV7Ru3Ro3NzdsbW1p1qwZK1asyHeO/87PypuHtXv3biZOnIiHhwf29vb07duXuLi4u8b71VdfodFouHTpUr7nJk2ahLW1NTdv3gTg33//pV+/flSrVg2dTkfVqlWZMGEC6enp9/xcCppTdurUKR577DFsbW2pUqUKn376KXq9Pt9r//zzT3r06IGvry86nY6aNWvyySefkJuba9inQ4cOrF+/nkuXLhk+87y5XIXNKdu6dSvt2rXD3t4eFxcXevfuzZkzZ4z2yZsfFxYWZhjZcHZ2ZsSIEaSlpd3zfRfnMzt79iz9+/fHw8MDW1tbgoKCeO+994z2iYqK4oUXXjB8Fv7+/rz00ktkZWUZxftfBc3V8/Pzo2fPnvz99980b94cW1tbfvzxRwDmz5/PY489hqenJzqdjrp16/LDDz8U+B7/+usv2rdvj6OjI05OTrRo0YLFixcD8NFHH2FlZVXg38PRo0fj4uJCRkbGPT9HIYR42Mn3h4JV1O8PBUlNTeX111+natWq6HQ6goKC+Oqrr1AUxWi/TZs20bZtW1xcXHBwcCAoKIh3333XaJ/vvvuOevXqYWdnR6VKlWjevLnhd7cQxSEj6aJceOqppzh37hy///4733zzDe7u7gB4eHgY9tm6dSvLli1j3LhxuLu7G34ZzJw5kyeffJIhQ4aQlZXFkiVL6NevH+vWraNHjx73PPcrr7xCpUqV+Oijj4iIiGDGjBmMGzeOpUuXFvqa/v3789Zbb7Fs2TLDl4M8y5Yto0uXLlSqVAmA5cuXk5aWxksvvYSbmxsHDhzgu+++48qVKyxfvrxYn1NMTAwdO3YkJyeHd955B3t7e3766acCR3EXLFiAg4MDEydOxMHBga1bt/Lhhx+SlJTEl19+CcB7771HYmIiV65c4ZtvvgHAwcGh0PNv3ryZbt26UaNGDSZPnkx6ejrfffcdbdq04ciRI/matfTv3x9/f3+mTZvGkSNH+OWXX/D09OTzzz+/6/ss6md2/Phx2rVrh5WVFaNHj8bPz48LFy6wdu1aPvvsMwCuXr1Ky5YtSUhIYPTo0dSuXZuoqChWrFhBWloa1tbWRfrs7xQaGsqgQYMYM2YMo0aNIigoCIAffviBevXq8eSTT2JpacnatWt5+eWX0ev1jB071vD6BQsW8Pzzz1OvXj0mTZqEi4sLR48eZePGjQwePJjnnnuOKVOmsHTpUsaNG2d4XVZWFitWrODpp5+WUQohhEC+PxRVRfn+8F+KovDkk0+ybds2XnjhBRo3bszff//Nm2++SVRUlCH2U6dO0bNnTxo2bMiUKVPQ6XSEhYWxe/duw7F+/vlnXn31VZ555hnGjx9PRkYGx48fZ//+/QwePLhYcQmBIkQ58eWXXyqAEh4enu85QLGwsFBOnTqV77m0tDSjx1lZWUr9+vWVxx57zGh79erVlWHDhhkez58/XwGUTp06KXq93rB9woQJilarVRISEu4ab6tWrZRmzZoZbTtw4IACKL/++muh8SmKokybNk3RaDTKpUuXDNs++ugj5b//ZP8b82uvvaYAyv79+w3brl27pjg7O+f77Ao675gxYxQ7OzslIyPDsK1Hjx5K9erV8+0bHh6uAMr8+fMN2xo3bqx4enoq169fN2w7duyYYmFhoQwdOjTfe3n++eeNjtm3b1/Fzc0t37n+q6if2aOPPqo4OjoabVMUxejPc+jQoYqFhYVy8ODBfMfM26+gz15Rbv8dufNzrV69ugIoGzduLFLcXbt2VWrUqGF4nJCQoDg6OirBwcFKenp6oXG3atVKCQ4ONnp+5cqVCqBs27Yt33mEEKKiku8P8v0hz7Bhw4xiWr16tQIon376qdF+zzzzjKLRaJSwsDBFURTlm2++UQAlLi6u0GP37t1bqVev3j1jEKIopNxdPDTat29P3bp1822/8yrwzZs3SUxMpF27dhw5cqRIxx09erRRqXO7du3Izc0tsBTtTgMGDODw4cNcuHDBsG3p0qXodDp69+5dYHypqanEx8fTunVrFEXh6NGjRYoxz4YNG3jkkUdo2bKlYZuHhwdDhgzJt++d501OTiY+Pp527dqRlpbG2bNni3VegOjoaEJCQhg+fDiurq6G7Q0bNqRz585s2LAh32tefPFFo8ft2rXj+vXrJCUl3fVcRfnM4uLi2LlzJ88//zzVqlUzen3en6der2f16tX06tWL5s2b5ztPQSXuReHv70/Xrl3vGndiYiLx8fG0b9+eixcvkpiYCKjldMnJybzzzjv5RsPvjGfo0KHs37/f6O/XokWLqFq1Ku3bty9R3EIIURHJ94eK8/3hvzZs2IBWq+XVV1812v7666+jKAp//fUXgKHh4J9//lngFIC8fa5cucLBgweLFYMQBZEkXTw0/P39C9y+bt06HnnkEWxsbHB1dcXDw4MffvjBkBTdy38TvLwys7w5YYXp168fFhYWhrI2RVFYvnw53bp1w8nJybBfZGSk4ReTg4MDHh4ehiSrqDHmuXTpEoGBgfm255Vb3+nUqVP07dsXZ2dnnJyc8PDw4Nlnny3RefPOXdi56tSpQ3x8PKmpqUbbS/rZFuUzu3jxIgD169cv9DhxcXEkJSXddZ+SKOzv4u7du+nUqZNhvp2Hh4dhPlte3Hlfyu4V04ABA9DpdCxatMjw+nXr1jFkyJASX1wQQoiKSL4/VJzvDwWd29fXF0dHx3znvTO2AQMG0KZNG0aOHImXlxcDBw5k2bJlRgn722+/jYODAy1btiQwMJCxY8calcMLURySpIuHRkHzpv7991+efPJJbGxs+P7779mwYQObNm1i8ODB+RqCFEar1Ra4/V6v9/X1pV27dixbtgyAffv2ERkZyYABAwz75Obm0rlzZ9avX8/bb7/N6tWr2bRpk6GZSmFXa+9XQkIC7du359ixY0yZMoW1a9eyadMmw1yu0jrvf5XkszXHZ1ZY0ntnk5w7FfR38cKFCzz++OPEx8czffp01q9fz6ZNm5gwYQJQ/LgrVapEz549DUn6ihUryMzMNHxREkIIUTTy/aHoyvP3h/tha2vLzp072bx5M8899xzHjx9nwIABdO7c2fBdoE6dOoSGhrJkyRLatm3LH3/8Qdu2bfnoo49KJSbxcJPGcaLcKMno4B9//IGNjQ1///230XIj8+fPN2VohRowYAAvv/wyoaGhLF26FDs7O3r16mV4/sSJE5w7d46FCxcydOhQw/ZNmzaV6HzVq1fn/Pnz+baHhoYaPd6+fTvXr19n5cqVPProo4bt4eHh+V5b1M+9evXqBZ4L1A7r7u7u2NvbF+lYd1PUz6xGjRoAnDx5stBjeXh44OTkdNd94PYV+oSEBKM1du9VsnintWvXkpmZyZo1a4xGALZt22a0X82aNQ1xBwQE3PWYQ4cOpXfv3hw8eJBFixbRpEkT6tWrV+SYhBCiIpDvD/dWEb4/FHbuzZs3k5ycbDSanle2nxcbgIWFBY8//jiPP/4406dPZ+rUqbz33nts27aNTp06AWBvb8+AAQMYMGAAWVlZPPXUU3z22WdMmjRJGrqKYpGRdFFu5P0HnZCQUOTXaLVaNBqN0YhnREQEq1evNnF0BXv66afRarX8/vvvLF++nJ49exr9osm7EnznlV9FUZg5c2aJzte9e3f27dvHgQMHDNvi4uIMo613O29WVhbff/99vmPa29sXqXzNx8eHxo0bs3DhQqM/o5MnT/LPP//QvXv34r6dAhX1M/Pw8ODRRx9l3rx5REZGGj2X91oLCwv69OnD2rVrOXToUL5z5e2Xlzjv3LnT8FxqaioLFy68r7gTExPzfeHr0qULjo6OTJs2Ld8yav8dIejWrRvu7u58/vnn7NixQ0bRhRCiAPL94d4qwveHgnTv3p3c3FxmzZpltP2bb75Bo9HQrVs3AG7cuJHvtY0bNwYgMzMTgOvXrxs9b21tTd26dVEUhezs7FKIXjzMZCRdlBvNmjUD1GU9Bg4ciJWVFb169brr1dUePXowffp0nnjiCQYPHsy1a9eYPXs2AQEBHD9+vNRj9vT0pGPHjkyfPp3k5GSjUjWA2rVrU7NmTd544w2ioqJwcnLijz/+KPacqjxvvfUWv/32G0888QTjx483LKFSvXp1o/fbunVrKlWqxLBhw3j11VfRaDT89ttvBZaJNWvWjKVLlzJx4kRatGiBg4OD0dX8O3355Zd069aNVq1a8cILLxiWUHF2dmby5Mklek//VZzP7Ntvv6Vt27Y0bdqU0aNH4+/vT0REBOvXryckJASAqVOn8s8//9C+fXtGjx5NnTp1iI6OZvny5ezatQsXFxe6dOlCtWrVeOGFF3jzzTfRarXMmzcPDw+PfBcACtOlSxesra3p1asXY8aMISUlhZ9//hlPT0+io6MN+zk5OfHNN98wcuRIWrRoweDBg6lUqRLHjh0jLS3N6MKAlZUVAwcOZNasWWi1WgYNGnR/H64QQjyE5PvDvVWE7w8F6dWrFx07duS9994jIiKCRo0a8c8///Dnn3/y2muvGS7ST5kyhZ07d9KjRw+qV6/OtWvX+P7776lSpQpt27YF1N/z3t7etGnTBi8vL86cOcOsWbPo0aNHvjnvQtzTA+oiL4RJfPLJJ0rlypUVCwsLoyVBAGXs2LEFvmbu3LlKYGCgotPplNq1ayvz588v0nIkeUuo/Hdprm3bthVrmauff/5ZARRHR8d8S2opiqKcPn1a6dSpk+Lg4KC4u7sro0aNUo4dO5ZveZKixKwoinL8+HGlffv2io2NjVK5cmXlk08+UebOnZtvCZXdu3crjzzyiGJra6v4+voqb731lvL333/ne28pKSnK4MGDFRcXFwUwLF1S0BIqiqIomzdvVtq0aaPY2toqTk5OSq9evZTTp08b7ZP3Xv67lElBS5oVpKifmaIoysmTJ5W+ffsqLi4uio2NjRIUFKR88MEHRvtcunRJGTp0qOLh4aHodDqlRo0aytixY5XMzEzDPocPH1aCg4MVa2trpVq1asr06dMLXYKtR48eBca9Zs0apWHDhoqNjY3i5+enfP7558q8efMKfM9r1qxRWrdubfgcW7Zsqfz+++/5jpm3LE+XLl3u+pkJIURFJt8f5PuDouRfgk1RFCU5OVmZMGGC4uvrq1hZWSmBgYHKl19+abR83pYtW5TevXsrvr6+irW1teLr66sMGjRIOXfunGGfH3/8UXn00UcVNzc3RafTKTVr1lTefPNNJTEx8a4xCVEQjaKUUocFIYQQpe7YsWM0btyYX3/9leeee87c4QghhBBCiPskc9KFEKIc+/nnn3FwcOCpp54ydyhCCCGEEMIEZE66EEKUQ2vXruX06dP89NNPjBs3rtQ63wohhBBCiAdLyt2FEKIc8vPzIzY2lq5du/Lbb79JUxohhBBCiIeEJOlCCCGEEEIIIUQZIXPShRBCCCGEEEKIMkKSdCGEEEIIIYQQooyocI3j9Ho9V69exdHREY1GY+5whBBCCBRFITk5GV9fXyws5Pq5KcjveyGEEGVJcX7XV7gk/erVq1StWtXcYQghhBD5XL58mSpVqpg7jIeC/L4XQghRFhXld32FS9LzOiBfvnwZJycnM0cjhBBCQFJSElWrVpUu/SYkv++FEEKUJcX5XV/hkvS8kjcnJyf5pS2EEKJMkbJs05Hf90IIIcqiovyul4lvQgghhBBCCCFEGSFJuhBCCCGEEEIIUUZIki6EEEIIIYQQQpQRFW5OelEoikJOTg65ubnmDkWYgFarxdLSUuZ6CiGEEEKIMiU3N5fs7GxzhyFMxMrKCq1We9/HkST9P7KysoiOjiYtLc3coQgTsrOzw8fHB2tra3OHIoQQD41p06axcuVKzp49i62tLa1bt+bzzz8nKCjorq9bvnw5H3zwAREREQQGBvL555/TvXt3w/OKovDRRx/x888/k5CQQJs2bfjhhx8IDAws7bckhBAPTEpKCleuXEFRFHOHIkxEo9FQpUoVHBwc7us4kqTfQa/XEx4ejlarxdfXF2traxl9LecURSErK4u4uDjCw8MJDAzEwkJmeQghhCns2LGDsWPH0qJFC3Jycnj33Xfp0qULp0+fxt7evsDX7Nmzh0GDBjFt2jR69uzJ4sWL6dOnD0eOHKF+/foAfPHFF3z77bcsXLgQf39/PvjgA7p27crp06exsbF5kG9RCCFKRW5uLleuXMHOzg4PDw/JOR4CiqIQFxfHlStXCAwMvK8RdY1SwS7dJCUl4ezsTGJiYr4lWTIyMggPD6d69erY2dmZKUJRGtLS0rh06RL+/v7yBU8IUebc7XdTeRIXF4enpyc7duzg0UcfLXCfAQMGkJqayrp16wzbHnnkERo3bsycOXNQFAVfX19ef/113njjDQASExPx8vJiwYIFDBw4sMDjZmZmkpmZaXictx5tef9MhRAPp7y8w8/PD1tbW3OHI0wkPT2diIiIAnOO4vyuN+uQ4s6dO+nVqxe+vr5oNBpWr159z9ds376dpk2botPpCAgIYMGCBSaPS0ZaHz7yZyqEEKUvMTERAFdX10L32bt3L506dTLa1rVrV/bu3QtAeHg4MTExRvs4OzsTHBxs2Kcg06ZNw9nZ2XCrWrXq/bwVIYR4IGQE/eFiqj9Ps2YuqampNGrUiNmzZxdp//DwcHr06EHHjh0JCQnhtddeY+TIkfz999+lHKkQQggh7kav1/Paa6/Rpk0bQ9l6QWJiYvDy8jLa5uXlRUxMjOH5vG2F7VOQSZMmkZiYaLhdvny5pG9FCCGEMCuzzknv1q0b3bp1K/L+c+bMwd/fn6+//hqAOnXqsGvXLr755hu6du1aWmEKIYQow7Jz9ey9cJ20rByj7U42VjxSww0LCxmleBDGjh3LyZMn2bVrl1nOr9Pp0Ol0Jj9ufEomIZEJWGo1dAjyNPnxhRBCiP8qV43jCiuRe+211wp9TUFz1MTd+fn58dprr931cxVCiLJg1/l4Pl57ivPXUgp8/slGvswc2FjKCUvZuHHjWLduHTt37qRKlSp33dfb25vY2FijbbGxsXh7exuez9vm4+NjtE/jxo1NG3gRHLl0k9G/HaZRFWdJ0oUQwoQk5yhcuZqoW1iJXFJSEunp6QW+pqLMUevQoYPJ/oIfPHiQ0aNHm+RYQghRGiKvpzHq10M8O3c/56+l4GJnRfPqlQy3ZtUrYWmhYc2xq3yz6VyRj5uYns24xUcYMf8Aiemybu29KIrCuHHjWLVqFVu3bsXf3/+er2nVqhVbtmwx2rZp0yZatWoFgL+/P97e3kb7JCUlsX//fsM+D5KXk9r4JzYp8x57CiHEw09yjgejXI2kl8SkSZOYOHGi4XFet9eKRlEUcnNzsbS89x+5h4fHA4hICFFSyRnZrA65Sp/GvjjaWJk7nAdGURRCY5NZdTSK+bsjyMrRo7XQ8Nwj1ZnQqRbOdsafxdKDkbz9xwm+3RpGNTd7nml29xHeKzfTeH7BQc7FqqPyYxcdYf6IFlhp81/PzsnV8+n6M3Ss7Un7WhX3/8yxY8eyePFi/vzzTxwdHQ1zxp2dnQ3diocOHUrlypWZNm0aAOPHj6d9+/Z8/fXX9OjRgyVLlnDo0CF++uknQG2689prr/Hpp58SGBhoWILN19eXPn36PPD3mJekx6VkkqtX0Mr0CSGEKJTkHKZRrkbSCyuRc3JyKnTpAp1Oh5OTk9GtOBRFIS0rxyy3oq6ON3z4cHbs2MHMmTPRaDRoNBoWLFiARqPhr7/+olmzZuh0Onbt2sWFCxfo3bs3Xl5eODg40KJFCzZv3mx0PD8/P2bMmGF4rNFo+OWXX+jbty92dnYEBgayZs2aYn2OQgjT+fDPU3yw+iQf/XnqgZ5XUZQi/79kKmlZOWw6HcuklSdo/X9beWLGv/y44yJZOXraBrjz1/h2TH6yXr4EHWBAi2q83KEmAJNWHmfPhfhCz3MyKpG+3+/hXGwKno467Ky17AqL5/1VJ/O957SsHMb8dpgFeyIYt/gICWlZpn3T5cgPP/xAYmIiHTp0wMfHx3BbunSpYZ/IyEiio6MNj1u3bs3ixYv56aefaNSoEStWrGD16tVGzebeeustXnnlFUaPHk2LFi1ISUlh48aNZllC093BGgsN5OoVrqfKaLoQonRIziE5x53K1Uh6q1at2LBhg9G2O0vkSkN6di51PzRP9/jTU7piZ33vP6KZM2dy7tw56tevz5QpUwA4dUr98v7OO+/w1VdfUaNGDSpVqsTly5fp3r07n332GTqdjl9//ZVevXoRGhpKtWrVCj3Hxx9/zBdffMGXX37Jd999x5AhQ7h06dJdl9kRQpheeHwqf4ZEAbA6JIpXHw/Ez92+VM+ZmJ7Nt1vO8799lxjZzp83u9Yu1fPl2XvhOi8vOszNtNtl5zZWFrSu6c7AFlXpXNfrnnPN3+gSROSNNNYdj+bF3w6z8uXWBHg6Gu2z7ew1xi4+QlpWLkFejswf0YIz0UmM+vUQSw9dxs/dnpduJfvXkjMYufAQx68korO04IunG+JiZ236N19OFOWL3fbt2/Nt69evH/369Sv0NRqNhilTphh+p5mTpdYCdwcd15IziU3MxNPxwV8oEEI8/CTnUEnOoTJrkp6SkkJYWJjhcXh4OCEhIbi6ulKtWjUmTZpEVFQUv/76KwAvvvgis2bN4q233uL5559n69atLFu2jPXr15vrLZQJzs7OWFtbY2dnZ2i4c/bsWQCmTJlC586dDfu6urrSqFEjw+NPPvmEVatWsWbNGsaNG1foOYYPH86gQYMAmDp1Kt9++y0HDhzgiSeeKI23JIQoxOxtYehv5UV6RX38Zb9Gd39RCeXqFZYfusyXf4dyPVUdLf55Zzgj2vjj7mD6Ltp3WnX0Cm+tOE52roKvsw2d6nrRsbYnrWq4YWOlLfJxLCw0fNWvEdGJGRy+dJNnfzlAs+qVDM9n5+rZfCYWvQJtA9z5/tmmONlY4etiy0e96vHRmlN8vvEs1VztCPJ2YNi8g0QlpONqb83PQ5sbHUs8vLycbNQkPSmDBjibOxwhhDALyTkeHLMm6YcOHaJjx46Gx3lzx4cNG8aCBQuIjo4mMjLS8Ly/vz/r169nwoQJzJw5kypVqvDLL7+U6vJrtlZaTk8xz/JutsX4IlqY5s2bGz1OSUlh8uTJrF+/nujoaHJyckhPTzf6nAvSsGFDw317e3ucnJy4du3afccnhCi6yOtprDqqjqJ/1KsuH689zaqj6mh6VVe7fPtn5uQSEplgSOqLIykjm++2nudklLoiRg0PdbT+YlwqSw5EMu6xwAJfl52r59jlBLJzjU/qoLOkrq/TPefzKorCrK1hfH2r2Vv3Bt5M79+4WIn5f9lYafnpuWb0/X4PkTfSWH8iOt8+TzetwrSnGmBteXsW2LDWfkRcT2X+7ggmLAvBxtKCpIwc/NzsWDCiZalXMIiyw8tJx4koiE3OMHcoQoiHlOQcKsk5VGZN0jt06HDXUrkFCxYU+JqjR4+WYlTGNBpNkco/yip7e+MvkW+88QabNm3iq6++IiAgAFtbW5555hmysu4+p9LKyni+p0ajQa/XmzxeIUThftgRRq5e4dFaHoxo48/Ws9f493w832+/wLSnGhjtm5iWTb8f9xiaoJWUo86S8Z0CGdbaj3XHrzJh6TF+23eJMe1rFthQ7YPVJ1ly8HKBx3Kxs+LRQA861vagfS1PXO2Ny8Szc/W8v+okSw+prx/9aA3eeaK2SdY5d3PQsXpsGzaejCE71/j/riqVbHmstmeBpfPv96jL5RvpbD4TS1aOnmbVK/Hz0Ob5YhcPN+nwLoQobZJzqCTnUJXfvwnCiLW1Nbm5uffcb/fu3QwfPpy+ffsC6lWuiIiIUo5OiLIpJ1fP0csJbD17jT1h8aRm3fvfkLOtFe92r/PAy5yv3ExjxeErAIx/PODWz0D+PR/PisOXGfdYAJVd1AaaWTl6Xlp0mHOxKTjqLPFyLv4cWg3Q0t+VCZ1rGUrbuzfw4bP1Z4lNymTjyRh6NfI1es3xKwmGBL2mh71R0nstKYOEtGzWHLvKmmNX0WjAz83eaGQ9NTOH6MQMLDQw+cl6DG3lV+y478bV3prBwYXPgyuI1kLDt4Ma8+7KEzjbWjGpe537GtUX5ZMhSU+UkXQhRMUmOceDIUn6Q8LPz4/9+/cTERGBg4NDoVecAgMDWblyJb169UKj0fDBBx9UyKtTomLbdDqWNceusvNcXInWwn5h4UFWvtSaGh4OBT4fdi2Fi3EpPF7Hy2TLNc3ZcYHsXIU2AW40q642T2nu50rrmm7suXCdOdsv8Emf+iiKwnurTrDnwnXsrbUsHdOKur7FW9WiMDpLLUOCqzFzy3kW7IkwStIVRWHK2tMA9G1SmW8GNDZ6bU6unpBbF0S2hcZxJjqJ8PjUfOewtdLy3aAmdKrrZZKYTcHO2pIZA5uYOwxhRl5O6oUqKXcXQlR0knM8GJKkPyTeeOMNhg0bRt26dUlPT2f+/PkF7jd9+nSef/55Wrdujbu7O2+//TZJSUkPOFohzOfXvRF8eMfSZS52VrSv5UGHIA+8nQpeyjGPgsIXG0MJuZzAiAUHWfVym3xlz3+fimH8kqNkZOup6+PE5Cfr0dL//jqSRiems+ygOor+yn/mgr/yWCB7Llxn6cHLjO0YwB9HrrD88BUsNDBrcFOTJeh5hjxSje+3h3H40k2OX0mgYRUXANYej+bQpZvYWml564mgfK+z1FrQ3M+V5n6uvPVEbWISMwpM0gO9HEq9KZ0QxeUp5e5CCAFIzvGgaJQHveitmSUlJeHs7ExiYmK+NdMzMjIIDw/H39/fLGuxitIjf7blx45zcby+LIRP+zTgifreJj321rOxjFx4CL0Cg1pW5ZlmVWhctVKxRrvjkjPp+/1urtxMp3n1SvxvZLCh/Hn+7nCmrDuNooCFBkPDtp4NfXi3ex18XWy5mZrFzvNxbD17jV3n47Gx0tIhyIPHanvSqqZbgfPRJq85xYI9EbT0d2XZGOMlJxVFYcCP+zgQcYNGVV04djkBgE961+M5E5eL55mwNIRVR6N4qmllpvdvTEZ2Lo99tZ2riRlM6FSL8Z0KbionCne3302iZEz5mZ6JTqLbzH9xs7fm8Aed7/0CIYS4B/lu+nC6259rcX4vyUi6EKLMUBSFLzaeJT4li//tu2TSJP3U1UTGLT6KXoEBzasytW+De66xXRAPRx3zh7fgqR/2cOjSTd5ccZxv+jdi6oazzNsdDsDg4Gq89ngg32w+z5KDkaw7Hs3mM7EEeTtx4kr+buuL9keyaH8k1pYWPFLDjSqVbo/oKwqsPJI3Fz1/8qvRaHj18UCenbvfkKCPbOtfagk6wPDWfqw6GsW6Y9FM6laH3w9EcjUxA19nG0Y/WqPUziuEueTNSb+emkVWjt5oFQAhhBDC1CRJF0KUGYcu3eTUVbUU6kDEDTKyc03SpCs6MZ3nFxwkLSuXNgFufNq3fokS9DyBXo7MebYZw+YdYO2xq5yKSuTirdLtt5+ozYvta6DRaJj2VAOGBFdjytrTHIi4YUiia3s70rG2Jx1qeZCalcO2s+rIelRCOjvPxRV4zmbVK9G6pluBz7UJcKNpNReORCbQpa4Xk7rXKfF7K4pGVV1oUs2Fo5EJfLP5HKuOqMvCvdO9DrbW0lRNPHwq2VlhpdWQnatwLTmDKpXyL3kohBBCmIok6UKIMmPB7gjD/awcPYcv3aRNgPt9HTMlM4fnFxwiNimTQE8Hvh/SrMClw4qrTYA7U59qwFsrjnMxPhVrrQVf9W/Ek//peF6/sjNLxzzC9tA44lIyaRvgjq+L8dz3x2p7MUVRCLuWwr/n40nOyDF63lKroU+TyoVeWNBoNMwa3JQd5+Lo26SyyZrV3c3w1n4cjQxh8X51vdPm1SvRq6FPqZ9XCHPQaDR4OtoQlZBObFKmJOlCCCFKlSTpQogy4WpCOhtPxQDQsIozx68ksjssvlhJ+i//XmTjyRijbXEpmVy6noa7gzXzhrfA2daqkFcXX//mVUlKV5cVe79H3UIbxGk0GjrW9rzrsTQaDYFejgR6OZYoFl8XWwa1LN7yYvejW30fPnM8w7VktZHWh73q3ld1ghBlnZeTjqiEdK4lSYd3IYQQpUuSdCFEmfC/fZfI1Ss8UsOVfs2q8vryY+wOiy/y66/cTGPqhjP55nsD2FhZ8MuwFlR1Nf3o18h2NRjZruLNw7a2tOD5tv78319n6d+8iqHLuxAPK2/nvA7vkqQLIYQoXZKkCyHMLiM7l98PqGXTw1v707iqCwDHoxJJTMvG2e7eo9+/7buEXoGm1VzyNS9rVNUFH+e7L68mim90uxo0q17J8OclxMPM01FN0mNkGTYhhBClTJJ0IYTZrQm5ys20bCq72NK5rhdaCw0Bng6EXUth78V4nqh/97nO6Vm5LDlwGYCXOgTQua7Xgwi7wrOw0NDC7/7WgBeivMjr8C7l7kIIIUqbrCEihChU2LUUFu6JICUz5947l5CiKMzfEwHAsNbVDU3P2tzqZL477Po9j7E6JIrE9Gyqutry2D3mfgshREl4OekAiE2WJF0IIUTpkiRdCFGgHefi6DN7Nx+tOcWAH/fedR5mTq6ejOzcEp3nQPgNzkQnYWulZUDz243P8hrG3WteuqIohq7ww1r5PZDO5kKIisfbKW9OupS7CyGEKF2SpAsA/Pz8mDFjhuGxRqNh9erVhe4fERGBRqMhJCTkvs5rquMI01p6MJLnFxwkJTMHjQZOXU2i7+zdhMYkG+2nKAqrjl6hzedbafHZZn759yLZufpinWvBrVH0vk0rG809f6SmGxYauBifSlRCeqGv33vxOqGxydhZa+nXvGqxzi2EEEXl6SSN44QQ4n5JzlE0kqSLAkVHR9OtWzeTHnP48OH06dPHaFvVqlWJjo6mfv36Jj2XKBlFUfj6n1De/uMEuXqFp5pUZvPE9tTwsOdqYgbP/LDHMLJ9/EoCT/+whwlLjxGblElyRg6frj/DEzN2suNc3D3PdTUhnd/2XeLvW8uuDW/tZ/S8k40VjW41JLvbaHreKPrTTauYdHk1IYS4U165e3JGDmlZpTcFSAghKhLJOQomjeNEgby9vR/IebRa7QM7l7i7zJxcJv1xgpVHowB49bEAJnSuhUajYeVLrRn962EORNxg2LwDdAjyZMvZWBQF7Ky1jO0YgJu9NV/+HcqFuFSGzTtApzqeDA6uhtbi9rXAnFw9hy7dZNvZa5y9Y1T+0Voe1CpgffA2Nd05GpnAnrB4+hcwSn75Rhqbz8QC6nx2IYQoLQ46S+ystaRl5RKblIm/u3yFEkKI+yU5R8FkJP1eFAWyUs1zUwpY8LkAP/30E76+vuj1xmXGvXv35vnnn+fChQv07t0bLy8vHBwcaNGiBZs3b77rMf9benLgwAGaNGmCjY0NzZs35+jRo0b75+bm8sILL+Dv74+trS1BQUHMnDnT8PzkyZNZuHAhf/75JxqNBo1Gw/bt2wssPdmxYwctW7ZEp9Ph4+PDO++8Q07O7VGLDh068Oqrr/LWW2/h6uqKt7c3kydPLtJnJQq29WwsT8z4l5VHo7C00PDF0w2Z2CUIjUad3+1iZ82vL7SkVyNfcvQKm8+oCXrfJpXZ+noHxnYMYGDLamx7swMj2/pjaaFh85lrPL/gEMPmHTDcXlh4iB+2X+BsTDIWGmhWvRJvdKnF7MFNCowrb176rrDrKAX8e8hbdq1doDsBnvmTfCGEMBWNRnPHvHQpeRdCmJjkHIDkHHnkMvC9ZKfBVF/znPvdq2Btf8/d+vXrxyuvvMK2bdt4/PHHAbhx4wYbN25kw4YNpKSk0L17dz777DN0Oh2//vorvXr1IjQ0lGrVqt3j6JCSkkLPnj3p3Lkz//vf/wgPD2f8+PFG++j1eqpUqcLy5ctxc3Njz549jB49Gh8fH/r3788bb7zBmTNnSEpKYv78+QC4urpy9epVo+NERUXRvXt3hg8fzq+//srZs2cZNWoUNjY2Rv8oFi5cyMSJE9m/fz979+5l+PDhtGnThs6dO9/z/VQ015IyeHXJURx0lnSs7UnHIE98XdQ1w8OupfDp+tNsD1XL090ddEzv34hHa3nkO46NlZaZAxpT29uRI5du8nLHAJpVr2S0j5ONFe/3rMvAltX4ZvM5IuJT8x0nwNOBx2p78migB5Xsre8ae9PqLthYWRCfksm52BSCvG8n4mlZOSy5tbb6iDZ+xfpMhBCiJDyddFyMT5UkXQhhepJzSM5xB0nSHwKVKlWiW7duLF682PAPZsWKFbi7u9OxY0csLCxo1KiRYf9PPvmEVatWsWbNGsaNG3fP4y9evBi9Xs/cuXOxsbGhXr16XLlyhZdeesmwj5WVFR9//LHhsb+/P3v37mXZsmX0798fBwcHbG1tyczMvGupyffff0/VqlWZNWsWGo2G2rVrc/XqVd5++20+/PBDLG6VTjds2JCPPvoIgMDAQGbNmsWWLVse6iT9bEwSp6KSaB3gho+zbZFfN+2vs+y7eAOAzWeuAVDb25FaXo5sOBFNjl7BSqvh+bb+jOsYgKNN4fO6LSw0jO0YcM9zBng6MHtw0yLHWBidpZaW/m7sPBfHrrB4oyR9xeErJGXkUN3Njg61ZNk1IUTp85KRdCFEBSY5x4PLOSRJvxcrO/XqkrnOXURDhgxh1KhRfP/99+h0OhYtWsTAgQOxsLAgJSWFyZMns379eqKjo8nJySE9PZ3IyMgiHfvMmTM0bNgQGxsbw7ZWrVrl22/27NnMmzePyMhI0tPTycrKonHjxkV+D3nnatWqlaHMGqBNmzakpKRw5coVw1W4hg0bGr3Ox8eHa9euFetc5cnVhHT6zdlLcoZaglPb25GOtT15rLYnTaq6YKkteObKkcibrDoahUYDo9rV4PClmxyNvMnZmGTDnPDHa3vyfs+6+Lvf+wqqObSpqSbpe8LieaGtPzdSs/jqn1DDKPqwVn5YyLJrQogHwEuWYRNClBbJOSTnuIMk6fei0RSp/MPcevXqhaIorF+/nhYtWvDvv//yzTffAPDGG2+wadMmvvrqKwICArC1teWZZ54hKyvLZOdfsmQJb7zxBl9//TWtWrXC0dGRL7/8kv3795vsHHeysjIe7dVoNPnmxzws9HqFN1ccIzkjB2dbK5Iysg1J9g/bL1DTw54lo1vh4ajL97opa08D8EzTKrzbvQ4AN1Oz2Hk+jpNRibQJcKdDUNkehc6bl77v4nXm7gpn5uZzJN26WNG7sS+Dg+9dPiWEEKYgI+lCiFIjOUeRVJScQ5L0h4SNjQ1PPfUUixYtIiwsjKCgIJo2VcuNd+/ezfDhw+nbty+gzveIiIgo8rHr1KnDb7/9RkZGhuHK1r59+4z22b17N61bt+bll182bLtw4YLRPtbW1uTm5t7zXH/88QeKohiubO3evRtHR0eqVKlS5JgfJr/tu8TusOvYWFmw6uXWuNhZs/NcHNtCr7H17DUuxKUy8tdDLBn1CLbWWsPr/jwWRcjlBOyttbzZNciwvZK9Nb0bV6Z348rmeDvFVtfHiUp2VtxMy+aTdacN2z7qVZfgGm5mjk4IUZHkLcN2TUbShRAVlOQcD4Z0d3+IDBkyhPXr1zNv3jyGDBli2B4YGMjKlSsJCQnh2LFjDB48uFhXgAYPHoxGo2HUqFGcPn2aDRs28NVXXxntExgYyKFDh/j77785d+4cH3zwAQcPHjTax8/Pj+PHjxMaGkp8fDzZ2dn5zvXyyy9z+fJlXnnlFc6ePcuff/7JRx99xMSJEw1zQ8q7qIR09PqiddG8EJfCtL/OAPBu9zrU8HDA1d6aPk0qM3NgE9aMa4uLnRXHLicwYWmI4bhpWTl8/lcoAC93DMDTyabQc5R1FhYaOt4a7Xe1t2Zq3wasfaWtJOhClBE7d+6kV69e+Pr65uvSW5Dhw4cbOu7eeatXr55hn8mTJ+d7vnbt2qX8Tu7NMJKeLCPpQoiKS3KO0mf+CITJPPbYY7i6uhIaGsrgwYMN26dPn06lSpVo3bo1vXr1omvXroYrXkXh4ODA2rVrOXHiBE2aNOG9997j888/N9pnzJgxPPXUUwwYMIDg4GCuX79udIULYNSoUQQFBdG8eXM8PDzYvXt3vnNVrlyZDRs2cODAARo1asSLL77ICy+8wPvvv1/MT6Ns+nVvBG3+bysztpy/5745uXomLjtGRraedoHuPBucfx1wf3d7fnquOdZaCzaeiuHzjWcBmLPjIjFJGVR1teWFtv4mfx8P2ke96jFzYGO2vd7h1trrMgddiLIiNTWVRo0aMXv27CLtP3PmTKKjow23y5cv4+rqSr9+/Yz2q1evntF+u3btKo3wi8XLUU3SYxIzClwWUgghKgLJOUqfRqlgv2WSkpJwdnYmMTERJycno+cyMjIIDw/H39/fqGGBKP/Kwp9tfEomHb/cTnJmDo42luyb9Dj2usJnnHy75TzTN53D0caSfyY8eteO7n+GRDF+SQgArz4eyI87LpCZo+eHIU3p1sDH1G9FCGFid/vdVJ5oNBpWrVpFnz59ivya1atX89RTTxEeHk716urFyMmTJ7N69Wqj9WyLqzQ+04zsXGp/sBGAYx92wdmu8NUwhBDibsrCd1Nhenf7cy3O7yWzj6TPnj0bPz8/bGxsCA4O5sCBA4Xum52dzZQpU6hZsyY2NjY0atSIjRs3PsBohSi56ZvOkZypNjxLzshh5dGoQvc9cSWRb2+Ntn/Su/49l1zr3bgyEzvXAtTkPjNHT7C/K0/UL3zpCSGEKAvmzp1Lp06dDAl6nvPnz+Pr60uNGjUYMmTIPbsDZ2ZmkpSUZHQzNRsrLS63EnMpeRdCCFFazJqkL126lIkTJ/LRRx9x5MgRGjVqRNeuXQtta//+++/z448/8t1333H69GlefPFF+vbty9GjRx9w5EIUz5noJMOSYU828gVgwe7wAuem5+oV3v7jODl6he4NvOnd2LdI53jlsQCebqo2utBo4IOedY2WlRBCiLLm6tWr/PXXX4wcOdJoe3BwMAsWLGDjxo388MMPhIeH065dO5KTkws91rRp03B2djbcqlatWiox55W8S4d3IYQQpcWsSfr06dMZNWoUI0aMoG7dusyZMwc7OzvmzZtX4P6//fYb7777Lt27d6dGjRq89NJLdO/ena+//voBRy5E0SmKuhSaXoEeDX34rG99HHSWXIhLZVdYfL79lx+6zOnoJBxtLPmkd/0iJ9oajYZpTzVgzKM1mNq3AfUrO5v6rQghhEktXLgQFxeXfOXx3bp1o1+/fjRs2JCuXbuyYcMGEhISWLZsWaHHmjRpEomJiYbb5cuXSyVmz1sd3mWtdCGEEKXFbEl6VlYWhw8fplOnTreDsbCgU6dO7N27t8DXZGZm5qvtt7W1vWszmQdR/iYqno0no9l0OrZI+/59Kpa9F69jbWnBpG61cbSx4plm6oj3gj0RRvsmZ2Tz1T9qV/bxjwfi5qD77+HuytrSgknd6zCopawdLoQo2xRFYd68eTz33HNYW1vfdV8XFxdq1apFWFhYofvodDqcnJyMbqVB1koXQghR2syWpMfHx5Obm4uXl5fRdi8vL2JiYgp8TdeuXZk+fTrnz59Hr9ezadMmVq5cSXR0dKHnKUn5WwXrpVchmPLPNDoxnZcWHWH0b4fYe+H6XffNzMll6gZ1CbXR7WpQpZIdAMNa+wGw9ew1wuNTDfvP2hpGfEoWNdztGdrKz2QxCyFEWbNjxw7CwsJ44YUX7rlvSkoKFy5cwMfH/I0wvSVJF0KYkOQdDxdT/XmavXFcccycOZPAwEBq166NtbU148aNY8SIEXddy6445W9WVmozmLS0NJPHLswr788078/4fuw6H4+igKLAG8uPkZyRf+3FPPN3RxB5Iw1PRx0vdahp2O7vbk/HIA9AXZYNICI+lXm7wwF4v2cdrC3L1T9PIUQFlZKSQkhIiKETe3h4OCEhIYZGb5MmTWLo0KH5Xjd37lyCg4OpX79+vufeeOMNduzYQUREBHv27KFv375otVoGDRpUqu+lKLwM5e6SpAshSk6r1QJqdbF4eOT9eeb9+ZZU4es/lTJ3d3e0Wi2xscYlw7GxsXh7F9yR2sPDg9WrV5ORkcH169fx9fXlnXfeoUaNGoWeR6fTodMVrWRYq9Xi4uJiaFxnZ2cnjbfKOUVRSEtL49q1a7i4uNz3PxiA3XfMI49KSGfK2tN82a9Rvv0u30hj1la1NPPtJ2rnW25teBt/toXGsfzQFV7vEsTUDWfIzlV4tJYHHYM87ztOIYR4EA4dOkTHjh0NjydOnAjAsGHDWLBgAdHR0fk6sycmJvLHH38wc+bMAo955coVBg0axPXr1/Hw8KBt27bs27cPDw+P0nsjReRpGEmXOelCiJKztLTEzs6OuLg4rKys7jroKMoHvV5PXFwcdnZ2WFreX5pttiTd2tqaZs2asWXLFkPDGL1ez5YtWxg3btxdX2tjY0PlypXJzs7mjz/+oH///iaLK+8CQWEd5kX55OLiUujFn+JQFIXdt0rcJ3auxTebz7H88BU61/WiS73bxz9xJZHnFx4kJTOHRlVd6Nukcr5jPRroTk0Pey7EpfL2iuP8czoWrYWGD3rUkYtDQohyo0OHDnct71uwYEG+bc7OznetWluyZIkpQisVMiddCGEKGo0GHx8fwsPDuXTpkrnDESZiYWFBtWrV7vu7vNmSdFCvtg8bNozmzZvTsmVLZsyYQWpqKiNGjABg6NChVK5cmWnTpgGwf/9+oqKiaNy4MVFRUUyePBm9Xs9bb71lspjy/sF4enqSnV14GbMoP6ysrEwygg5w/loKccmZ2FhZMKZ9DVIzc/hx50UmrTxB0+qVcHfQsfVsLOMWHyUtK5fa3o7MebYpFhb5/6FqNBqGt/bjgz9Psf6E2lfh2eBqBHo5miRWIYQQppc3J/1aciZ6vVLg/+9CCFEU1tbWBAYGSsn7Q8Ta2tokVRFmTdIHDBhAXFwcH374ITExMTRu3JiNGzcamslFRkYavcmMjAzef/99Ll68iIODA927d+e3337DxcXF5LFptVqTJXbi4bHrvFrq3sLPFZ2lloldarE9NI7Q2GTeW3WCdoEefPjnSfQKtAt05/shTXG0KXwe/FNNq/DFxlCSM3NwtrXitU61HtRbEUIIUQLuDtZoNJCrV7iemoWHY/FW4RBCiDtZWFjkW71KCLMm6QDjxo0rtLx9+/btRo/bt2/P6dOnH0BUQhQsbz562wB3AHSWWqYPaESf2bv5+1Qsf59Seyz0b16Fz/o2wEp79ytp9jpLRrT159st55nUrTaV7O++DJEQQgjzstRa4O6gIy45k9ikDEnShRBCmJx0KBCiiLJz9ey7qM5Hb3MrSQeo5+tsNAL+eudafP50w3sm6HleezyQ/e8+zkBZ21wIIcqFvA7v15JlXroQQgjTM/tIuhDlxfErCaRm5VLJzoq6Pk5Gz415tAZ21lr83OzpWLt4ndktLDSGRkRCCCHKPm8nG05GJRGTKB3ehRBCmJ4k6UIU0a7z6ih665ru+RoFWWotGNHG3xxhCSGEeMA8pcO7EEKIUiTl7kIUUd589DtL3YUQQlQ8Xo55Hd4lSRdCCGF6kqQLUQSpmTkcibwJ3G4aJ4QQomLycVGT9Cs3080ciRBCiIeRJOlCFMGBiBvk6BWqVLKlmpuducMRQghhRjXc7QG4GJdq5kiEEEI8jCRJF6IIdp83XnpNCCFExeV/K0m/mphORnaumaMRQgjxsJEkXYgi2CXz0YUQQtziam+Ns60VigLh8TKaLoQQwrQkSRcVwvbQazT7ZBP9f9zLD9svEBqTjKIoRXptfEomZ2OSAWhd0600wxRCCFEOaDQaanhIybsQQojSIUuwiQph7q5wrqdmcT38BgfCb/D5xrNUdrGlQ5AHHYM8aR3ghp11wf8c9lxQl16r4+OEm4PuQYYthBCijKrh7sDRyATC41PMHYoQQoiHjCTp4qGXmJ7N3luJ9oROtQi5fJM9F64TlZDOov2RLNofibWlBY/UcKNjkAct/FyxtrxdZPLPqRgA2gbIKLoQQgiVjKQLIYQoLZKki4fe9tBr5OgVAjwdGN8pEID0rFz2XbzO1rPX2Hr2GlEJ6ew8F8fOc3GFHkfmowshhMiT1+H9gsxJF0IIYWKSpItyLTUzh0/WnaZVTTd6N65c4D7/nI4FoEtdL8M2W2stHWt70rG2J1MUhbBrKWwLVRP2sGsp/He6ei0vRx6pISPpQgghVDU8HAAIj0tBURQ0Go2ZIxJCCPGwkCRdlGvzdoWz5OBl1h67SsfanjjZWBk9n5mTy/az1wDoUs+7wGNoNBoCvRwJ9HJk9KM1Sz1mIYQQ5V91Nzs0GkjKyOF6ahbu0rNECCGEiUh3d1FupWTm8MuucABSs3JZcehKvn32XLhOalYuXk46GlZ2ftAhCiGEeEjZWGmp7GILyLx0IYQQpiVJuii3ft0bQWJ6NpYWaonhwr0R6PXGder/nFJL3TvX9cLCQkoRhRBCmE5eyfvFOOnwLoQQwnQkSRflUlpWDr/8q46iT36yHk42lly6nsa20GuGfXL1CptuzUfvWkipuxBCCFFSec3jwqV5nBBCCBOSJF2US4v2RXIjNYvqbnYMbFGVgS2rAbBgT4Rhn5DLN4lPycTRxpJgf2n6JoQQwrTylmG7IOXuQgghTEiSdFHupGfl8uPOiwCM7RiApdaC5x6pjoUG/j0fz/nYZOB2qftjtT2N1j0XQgghTKGG+61y93gpdxdCCGE6krmIcuf3A5HEp2RSpZItfZuoy65VdbWjUx11ibWFeyNQFIW/T8UA0KWulLoLIYQwvbyR9MjraeTk6s0cjRBCiIeFJOmiXMnIzuXHnRcAeLlDAFba23+FR7TxB+CPw1EcibxJxPU0rLUWtA/yMEusQgghHm7eTjbYWmnJ0Stcvplu7nCEEEI8JCRJF+XK8kOXiU3KxNfZhqebVTZ67pEartT2diQ9O5fXloYA0CbADQedpRkiFUII8bCzsNDgd6t5nHR4F0IIYSqSpIsH7s+QKPp+v5spa0/z7/k4MnNy77q/Xq9wMiqRb7ecZ8bm8wC82KEmOkut0X4ajYbhrf0AuHxDHdHoIl3dhRCiVO3cuZNevXrh6+uLRqNh9erVd91/+/btaDSafLeYmBij/WbPno2fnx82NjYEBwdz4MCBUnwXJZdX8i5rpQshhDAVGWIUD5Rer/D5X2e5mpjB0cgE5u0Ox85aS5sAd5pVr2RY8zxPaEwy28/FEZecadjm725P/+ZVCzx+78aV+b+NZ0lIy0ajwTBPXQghROlITU2lUaNGPP/88zz11FNFfl1oaChOTk6Gx56enob7S5cuZeLEicyZM4fg4GBmzJhB165dCQ0NNdqvLKiZN5Iuy7AJIYQwEUnSxQN1JPImVxMzcNBZ0r2BN9tC1QR80+lYw5rmBbGz1tI2wJ2OtT3pXt8HGyttgfvZWmsZ2KIac3ZcoFm1Sng46krrrQghhAC6detGt27div06T09PXFxcCnxu+vTpjBo1ihEjRgAwZ84c1q9fz7x583jnnXfuJ1yT8/eQcnchhBCmZfYkffbs2Xz55ZfExMTQqFEjvvvuO1q2bFno/jNmzOCHH34gMjISd3d3nnnmGaZNm4aNjc0DjFqU1NpjVwHoUs+LL55phF6vcDo6ia1nr3GhgC84Hg46OgR50sK/Ur7y9sKMeywAjQaebORr0tiFEEKYTuPGjcnMzKR+/fpMnjyZNm3aAJCVlcXhw4eZNGmSYV8LCws6derE3r17Cz1eZmYmmZm3q66SkpJKL/g73F6GTUbShRBCmIZZk/TilrMtXryYd955h3nz5tG6dWvOnTvH8OHD0Wg0TJ8+3QzvQBRHTq6e9SeiAeh1K4G2sNBQv7Iz9Ss7m+w8DjpL3n6itsmOJ4QQwnR8fHyYM2cOzZs3JzMzk19++YUOHTqwf/9+mjZtSnx8PLm5uXh5GU9X8vLy4uzZs4Ued9q0aXz88celHX4+eXPS45IzSc7IxtHG6oHHIIQQ4uFi1sZxd5az1a1blzlz5mBnZ8e8efMK3H/Pnj20adOGwYMH4+fnR5cuXRg0aFCZbSYjjO0Pv0F8ShYudla0DXA3dzhCCCHMICgoiDFjxtCsWTNat25tuPD+zTff3NdxJ02aRGJiouF2+fJlE0V8d442VoapVeEymi6EEMIEzJak55WzderU6XYw9yhna926NYcPHzYk5RcvXmTDhg1079690PNkZmaSlJRkdBPmkVfq3q2+j9H65kIIISq2li1bEhYWBoC7uztarZbYWOM+JbGxsXh7F75ih06nw8nJyej2oPi7S4d3IYQQpmO2TOlu5Wz/XYYlz+DBg5kyZQpt27bFysqKmjVr0qFDB959991CzzNt2jScnZ0Nt6pVC+4KLkpXVo6ev06qf669GvmYORrxUAj5Hf4YCUnR5o6kZLLSYN0E2PklKIq5oxHCrEJCQvDxUX83WFtb06xZM7Zs2WJ4Xq/Xs2XLFlq1amWuEO+qpjSPE0IIYUJmbxxXHNu3b2fq1Kl8//33BAcHExYWxvjx4/nkk0/44IMPCnzNpEmTmDhxouFxUlKSJOpmsCssjsT0bDwcdQT7u5k7HFHenVoFq19U78efgxF/gbW9eWMqDr0eVo2BM2vUx4oC7d8yb0xClFBKSophFBwgPDyckJAQXF1dqVatGpMmTSIqKopff/0VUBvA+vv7U69ePTIyMvjll1/YunUr//zzj+EYEydOZNiwYTRv3pyWLVsyY8YMUlNTDd3eyxppHieEEMKUzJakl6Sc7YMPPuC5555j5MiRADRo0IDU1FRGjx7Ne++9h4VF/sIAnU6HTifLcJnb2mPqaGePBj5o/7MWuhDFcvkArByj3tdoIfoYrHgBBi4Ci6KtAGB2mz9SE3SNFpRc2PYZVPKDhv3NHZkQxXbo0CE6duxoeJx3YXzYsGEsWLCA6OhoIiMjDc9nZWXx+uuvExUVhZ2dHQ0bNmTz5s1GxxgwYABxcXF8+OGHxMTE0LhxYzZu3Jiv+q6sqOEh5e5CCCFMx2zl7iUpZ0tLS8uXiGu16pdyRcpFy6z0rFz+OZVX6i7Loon7cCMcfh8EuZlQqxsMXw9aHZz7C/5+z9zRFc2hebDnW/V+3znQ+hX1/p9j4dIe88UlRAl16NABRVHy3RYsWADAggUL2L59u2H/t956i7CwMNLT07l+/Trbtm0zStDzjBs3jkuXLpGZmcn+/fsJDg5+QO+o+PLmpIfHp6LXy/cRIYQQ98es3bsmTpzIzz//zMKFCzlz5gwvvfSSUTnb0KFDjdZJ7dWrFz/88ANLliwhPDycTZs28cEHH9CrVy9Dsi7Knm2h10jNyqWyiy1Nq7mYOxxRXqXfhMX9IS0evBvC079A9VZqoguw/wfY/6N5Y7yX85th/Rvq/Q7vqiPnnaZAnV6QmwVLBkN82N2PIYQoc6q62mFpoSE9O5eYpAxzhyOEEKKcM+uc9HuVs0VGRhqNnL///vtoNBref/99oqKi8PDwoFevXnz22WfmeguiCPK6uvds5INGI6XuFUZOJiSbqKmbooc1r6rzz50qw+BloFPngFL/KbgZAVs+ho3vgEt1CHri7sfLTFGT/XuxtAVHE5XXxpyE5cPV8vZGg27PQbewgL4/QVJPiDoMi/vBwMVgZXv341nZg4OHaWITQtwXK60F1dzsuBiXSnh8Kr4u9/j3K4QQQtyFRqlgdeJJSUk4OzuTmJj4QJdnqaiSM7Jp/ulmMnP0rHulLfUrO5s7JPEgxJ+Hhb1Ml6TnsXaA5/8G7/rG2xUF1rwCR39Tk9dXDoNTIasIJF6B71tBZhGXY2w5Grp9AfdzgSkpGn7pBElXwK8dPLsSLK2N90m5Bj8/DomRBR8jHw10mAQd3i55XKLMkN9NpvegP9ORCw+y+cw1Puldj+da+ZX6+YQQQpQvxfm9JItVi1K1+UwsmTl6anjYU89XvnhWCKnxsOgZNUG3sAIrO9PcnKtB/1/zJ+igJtA9vwHfppCdqs77LsyBn9UE3cLy3ucEOPAT7J5Z8s8jKxV+H6Am6G6B6nv4b4IO4OAJQ5aDZ72ifR4osH0qHP1fyWMTQphMTQ+1uudcrCzDJoQQ4v6UqyXYRPmSk6vnh+0XAHiyka+UulcE2elqY7ebEWrZ+cgtD64kW2sFbcbD8mFqkv7oG2D5n5UdstLgyEL1fv/foHb3ux9z3xzY+Lbajb2SH9TrU7yY9LnqWu7Rx8DODYYsAzvXwvf3rA0vF7F53NZP1TXW144H56pQo33xYhNCmFTdWxeiT15NNHMkQgghyjsZSRel5vcDkZyLTaGSnRUjWvubOxxR2vR6WP0SXDkANs7qqPCDnjNdu6c6Zz0tHk6uzP/8iWVqAzqX6lCr672P98iL0PLWcm+rxsDlg8WL55/3IXSD2oF+4O/gWqN4r7+bju9B/WdAnwNLn4O4UNMdWwhRbA1uTec6E51ETq7ezNEIIYQozyRJFyUSl5zJU9/vNoyU/1diWjbTN50DYGLnWjjbWT3I8IQ5bP0ETq1SS9wHLAKPoAcfg9YSWoxU7++fo85Vz6Mot7u/txxd9DXVn5gGtZ6AnAz4faBaJVAU+3+Cfd+r9/v+ANVMvHyURgO9Z0PVRyAzUZ1ikHLNtOcQQhSZn5s9DjpLMrL1XCiv66UrivH/m0IIIcxCyt1Fiaw6eoUjkQkciUzAydaSIcHVjZ6fseUcN9OyqeXlwKCW1cwUpShUdjqcWA6pcaY5XnIsHLiVAD/5Lfi3M81xS6LpMNjxOUSHwOUDt5PjiH/h2mm1sVyTZ4t+PAstPD0X5neDmOPwv2eg8aC7vyYz+fY89sc/hPpPl+it3JOVjdoJ/pfH4eatNeSHrQVru+IdJ2yz2pSv2iOlE6cQFYCFhYa6vk4cCL/BiahEgrwdzR2SSq+H1GuQGAVJUZB09fbP1GuQkQgZSWqvjowkdTUNGyewcVGromycwcFLrQS682bnen8NNYUQQhRKknRRIttDbyd3H/55iiqV7GhfSy1tDruWwm97LwHwQc+6WGqlYKNM0efC8hFw7i/TH/vRt6DxYNMftzjs3aBBP7XT+/45t5P0vFH0xoPA1qV4x9Q5wOClavf16+dhy5Siva7Js9B2YvHOVVz2bjBkhZqoRx1Sy/L7LVSXdiuKkN9h9YtqSf6Ek2oDOyHEbQmX4fw/kJul3nKyIDdTXWYyJwOy09R+F9npTE29wRkrDV77KkNSoNqLwt4NHH1u36xsTBufXg8pMWqVz523xCvqLekq6LOLd8z0m+rtbuw9oUpzqNwUKt/6aSMruAghhClIki6KLSUzh4MRNwBoF+jOv+fjGbvoCMtfbEUdHyc+W3+aHL1CpzqetAuUdZzLnH/eVxN0rQ4aPGO6kRDfJtD8BdMc634Fj1GT9NN/qqNH+mx1bjiope4l4eQLw9epyX52EUpZ3QKh1dgHM9LkHqCOqP/WB86sgS2ToXMRLiRE7FKXrgM16Ti84Pb67UIIVVworC/axbYAIEALxAE7CtnJ1hUcvUHndGuk2km9r3MASxu14WXeT0VRK5+y0yHn1s/0m+oqGqlx6s+0ePXiwd1oLNQLBE6+t26Vb1008FZj0DndjsNCq46oZyRCRgKkJ0DyVbhxEW6Eqz+TotRR+NANt/9vRQO+jdXpQbW6gk9jGWkXQogSkiRdFNvusHiycxX83Oz4ZVhzhs49wP7wGzy/4CATOtdiW2gcVloN7/Woa+5QxX8ZzZOeA/WfMm88pcW7AVRvC5d2waG56oiXooeaj93fXHm3mtD9C9PFaUp+bdQ56itHqaX2lfyh+YjC948/D0uGqBcwXGvCjQtwcC60ea3gJeKEqKgcvdSmlJY60Fqrt7z7Vra3bnZgZUtsqp45fx/BU5vKmBYuWKRfh5Q4NclNilYvhqXfUG+mpNGCcxV1FQpXf7U5pks1dZtzFXDwVnt2FJWT792fz0pTp/9cOQRRh9UqnoRIuHpUvW2fpl4ECOyiTvfxa1f06h4hhBCSpIviyyt17xDkic5Sy4/PNeOpH/ZwMS6Vt1YcB2BYKz/83e3NGab4r3N/q8uJATz+0cOboOcJHnMrSZ8PSu6tbS+aN6bS1rC/OtK1fSqsf139kh7weP798tayz0hQy1SfWwWzWqgls2fWqBUWQgiVdwMYuKhIu7rrFZZu9iUtK5dOLR8l0OuOeemKoo6CG+aC3zEPPDMJslJvldCnqz+z09URcCs7tUTe0lb9aVsJ7NzB3kMtpbf3UBNi7QNs0Gptp/awuLOPRVK02t/i3Ea4sA2So9UlL48sVC8aNHlWnQ7lXOXBxSmEEOWUJOmiWBRFYXuo2kG6Q5Bayu5iZ8384S3o+/0ebqRm4WpvzSuPB5ozTPFf0cfVeeiKHpo8B20nmDui0hfUXV0/PPGy+riSPwR0Nm9MD0L7t9Ry1ONLYNkweOEf8LqjqiU7A5YMvr2W/aAlaplrixdg22fqPH5J0oUoEa2Fhro+Thy6dJOTVxONk3SNRm22ZudqvgBLk5MPNH1OveVkqtNpzqyFk39AwiX1/5dtU9WKpg7vQNWW5o5YCCHKLEnSRbGci00hOjEDnaUFj9RwM2yv7mbP3GHN+XT9GUY/WgNnW1lyzSxyMmHVi2op850SItV51DU6QM9vKsY8wbzl2DZ/pD4OHlMxyi01GrXDfuIVtZJgfjf1YkWezET178N/17JvNhx2fglXDsKVw1ClmVnCL5P0ubDhDbVRVsdJ5o5GlHH1Kztz6NJNTlxJom8Tc0djJpY6tYon4HHoOlWt0Dn6P3WVjQtb1FvDAdDpYzW5F0IIYUSSdFEs226Noreq6YaNlfE6002qVeKPl1qbIyyR5+RKOLWy4Oc860L/Xx9sSaS5NR0Ku2eo8zXN3XX+QbLUwYDfYF5XiD+nlrXfSWsN/X8znp/v4KnOHT32u7qcXpWfHmjIZdrl/XBonnq/Ti/wrm/eeESZ1qCy2uH8ZFSimSMpI6ztoNFA9XbjIvz7NRxdBMeXwpl18Ojr8MhY03e9F0KIckySdFEseaXuHYNkmaYyR1HUUmVQO5gHdbv9nEarzh201JknNnOxc4WX9wGairc0kJ0rjPlXTTDz5uTn8ahdcGOo4DFqkn5yJXT+RG2YJeD8ptv3D/wIT35nvlhEmVf/VpJ+6moier2ChUUFqFwqKtcaaoPL5i/AX2/DlQPqkpZHflW3+7U1d4RCCFEmSJIuiiw5I5tDEeq6qXnz0UUZcvkARIeoS6u1f0dtKCTUJYYqKisbqNG+6Pv7NoGqwbdHjqW0W3Vnkn58mVqi+7DOKxb3raaHPTZWFqRm5RJ+PZWaHg7mDqnsqdxU7ZdxfJk6JelmBCzoCW1ehY7vVbwLykII8R8VYIKmMJXdYfHk6BVquNtT3U06t5c5eaPoDftJgi5KLniM+vPQPLXHQUWXFA2xJwANuAVATobarVqIQlhqLajr4wRIyftdaTTQaACMO6g2NEVRl4/85XG4dsbc0QkhhFlJki6KLG/ptfYyil72JEbB6T/V+y3HmDcWUb7VeRIcfdUlog7Ng7hzxb8lx5r7XZhO2Gb1Z+Vm0Haiev/AL5CbY76YRJlXX+alF53OEXrPggGLwM4NYk7Aj+1h3xx1GpcQQlRAUu4uikRdeu32+uiijDk0T513XL0N+DQ0dzSiPNNaqcuxbf0ENr5TsmNoLKDzFGj9imljM4fz/6g/AzurjfU2fQhJVyB0PdTtbd7YRJmVl6SfkCS96Or0hCot4M+xELYJNr4NWcnw6JvmjkwIIR44GUkXRXI2JpmYpAxsrCwI9pe5mGVKdgYcnq/eD5ZRdGECzZ8Hn8ZgW6n4NxsXUPTwz/twarWZ38h9ys2Gi9vV+wGd1Tn+zUeoj/f/aLawRNlX3/dW87ioJPR6GQ0uMkcvdWnITpPVx1s/VZvKCSFEBSMj6aJI8kbRW9d0z7f0mjCzk39A2nVwqgJBPcwdjXgY2LnCmB0lf/2Gt9Qu6KvGgFNlqNrCdLE9SJf3Q2YS2LmrTfVAvYCx6xu4tBuij0vliihQoJcD1pYWJGfmEHkjDT936eNSZBoNtJ0AGUmwazqsHQ/2HsYrlgghxENORtJFkeStjy5d3csYo2XXRoJWrruJMuCJaVDrCbXJ2u8D1c7N5VFeV/eAx8Hi1q9LJ9/bZe4ymi4KYaW1oM6t5nFS8l5Cj38IjYeolTnLh0PkfnNHJIQQD4wk6eKekjKyOXLp1tJrtWQ+epkSuQ9ijoOlLTQdZu5ohFBZaOHpueDdENLiYVE/SL9p7qiKL69pXEBn4+3BL6o/TyyH1PgHG5MoN+r7Sof3+6LRQK+ZENhFveC3uD/EhZo7KiGEeCBk2E3c07HLCeToFaq52lHNzc7c4Tx8kmPh2O+QnV781+YlEQ37y7rNomzROcDgZepySvHnYFF/qPnYvV9XLbho+91L6nU4u04d9bZ1Kf7rE6Mg9iSgUUfS71SlhVr+fvUoHF4Aj75x//GKh06DvA7vVyVJLzGtFfRbAAufhKhD8NtT8PxGcKlq7siEEKJUSZIu7unU1SQA6ld2MnMkD6G0G7CgO1wPu7/jSMM4URY5+cDgpTDvCbhyQL0VxVM/qxeeSiojCRb2gmun4Oj/YNgasLIt3jHyLoBVaZ7/AphGo46mrxoDB+dCm/FqMiHEHW4vw5aEoihoNBozR1ROWdurF/zmdYXr5+G3PjBiIzjI9DshxMNLknRxT3lJer1b3WqFieRkwtJn1QTdqXLJm+JUbg5e9UwbmxCm4t0ARmyAkMWgv8fa4gmX4fzf6hJMzlWgeuviny83G5YPUxN0UC8MrH4Jnp53e155UYTdmo8e2KXg5+v1hePLoEG/4sf4kNm5cydffvklhw8fJjo6mlWrVtGnT59C91+5ciU//PADISEhZGZmUq9ePSZPnkzXrl0N+0yePJmPP/7Y6HVBQUGcPXu2tN6GydXycsRaa0FiejaXb6RLJdr9sHeDoavVC37Xw+B/fWHYupJVyQghRDkgSbq4p1O3SvXq+spIuskoCqx5Ve0Qbe2oLjkjibZ4WPk0Um/3oterCfaZNbBkMIzcAm41i34eRYENb8KFrWBlB49/dGspuFVQyR86fVS04+RkwYXt6v2ATgXvY6mD51YWPbaHWGpqKo0aNeL555/nqaeeuuf+O3fupHPnzkydOhUXFxfmz59Pr1692L9/P02aNDHsV69ePTZv3mx4bGlZvr6yWFtaEOTtyImoRE5EJUqSfr+cq8DQP9VEPeYELB4Az60Ca/lchRAPnzLROG727Nn4+flhY2NDcHAwBw4UXhLZoUMHNBpNvluPHrL0VGlIy8ohPD4VgHqSpJvOji/g+BLQaKH/QknQhQB1pLvvj1C5mdpobtEz6pSQotrzHRyeD2jUxnWPvAi9Z6nP7Zpe9PWWL++HrGR12SefxsV9FxVOt27d+PTTT+nbt2+R9p8xYwZvvfUWLVq0IDAwkKlTpxIYGMjatWuN9rO0tMTb29twc3d3v+txMzMzSUpKMrqZW32Zl25abjXVi2M2znB5n1qNlpNl7qiEEMLkzJ6kL126lIkTJ/LRRx9x5MgRGjVqRNeuXbl27VqB+69cuZLo6GjD7eTJk2i1Wvr1k5LD0nAmOhlFAQ9HHZ6ONuYO5+FwbClsn6re7zk9f1MqISoyazsYtAScq8GNi+qIek7mvV93+k/Y9IF6/4lpULu7er/RQGj/jnp/3QS4sO3exzr/j/ozoFPxSuRFiej1epKTk3F1NZ77f/78eXx9falRowZDhgwhMjLyrseZNm0azs7OhlvVquZvLmZoHicd3k3HuwEMXq5Wy1zYAitHqVU4QgjxEDF77dj06dMZNWoUI0aMAGDOnDmsX7+eefPm8c477+Tb/7+/xJcsWYKdnZ0k6SWgKApf/B1KRHwqMwY2RmepzbfP6VtX/2UU3USuHoU149T7bcZDs+FmDUeIMsnBU50CMrcLRO6FmY3V5lF3k3BJ/dly9O0l0vJ0eAduhsPxpfD7ILVs9m6SotSfhZW6C5P66quvSElJoX//280Cg4ODWbBgAUFBQURHR/Pxxx/Trl07Tp48iaOjY4HHmTRpEhMnTjQ8TkpKMnuinpekn4hKlOZxplQtGAYuUkveT6+GLX7Q+eN7vUoIIcoNsybpWVlZHD58mEmTJhm2WVhY0KlTJ/bu3VukY8ydO5eBAwdib1/wF7jMzEwyM2+PwpSF8reyYuaW8/yw/QIA/cLieay2V759bjeNkyTdJEJ+h9wstRnV45PNHY0QZZdnbRjwq7p0W/LVor2mVjfoOk3tvn4njQae/A6SrkLEv2qH6HvROUuVywOwePFiPv74Y/788088PT0N27t1u91Is2HDhgQHB1O9enWWLVvGCy+8UOCxdDodOp2u1GMujlreDlhpNSSkZXPlZjpVXWX+tMnUfAyenAWrRsPuGWopfNOh5o5KCCFMwqxJenx8PLm5uXh5GSeHXl5eRergeuDAAU6ePMncuXML3WfatGn5OsQKWHX0CjM23/6iuuv89Xsk6dLZ3SSiDqk/G/SXMloh7qVGB3jtBNy4cO99LW3UtcsLG6m01KlNp64eVS+U3Uslf7CtVKxwRfEsWbKEkSNHsnz5cjp1unvVgouLC7Vq1SIs7D6Xq3zAdJZagrwdORmVxMmoREnSTa3RAPX/hx2fq9NZXKpDjfbmjkoIIe6b2cvd78fcuXNp0KABLVu2LHSfslj+Zm77Ll7nrRXHAWhc1YWQywnsDovPt192rp7QmGRARtJNIidT7UgLUKWZeWMRorxw9FJvpmChVdc9F2b3+++/8/zzz7NkyZIiNX5NSUnhwoULPPfccw8gOtNqUNmZk1FJnIhKpFsDH3OH8/DpMAmuX4CTK2DZc/DCZvCoZe6ohBDivph1KM/d3R2tVktsbKzR9tjYWLy9ve/62tTUVJYsWVJo2VsenU6Hk5OT0a0iuxCXwpjfDpOdq9CjgQ9zhzVHo4HQ2GSuJWcY7Rt2LYWsXD2OOkuqVpKr//ct5oQ6gmfnpo7SCSHEQyAlJYWQkBBCQkIACA8PJyQkxNDobdKkSQwdersMefHixQwdOpSvv/6a4OBgYmJiiImJITHxdnO1N954gx07dhAREcGePXvo27cvWq2WQYMGPdD3Zgr175iXLkqBRgO9Z0PVYMhIhMX9IPW6uaMSQoj7UqIkfdu2InTHLQJra2uaNWvGli1bDNv0ej1btmyhVatWd33t8uXLyczM5NlnnzVJLBXB9ZRMRsw/SGJ6Nk2qufB1/0a4Oeio66NeuNh7wfiXWl6pex1fJywspNnNfbtyq9S9cvPCS3KFEKKcOXToEE2aNDGscT5x4kSaNGnChx9+CEB0dLRRZ/affvqJnJwcxo4di4+Pj+E2fvx4wz5Xrlxh0KBBBAUF0b9/f9zc3Ni3bx8eHh4P9s2ZwJ0d3hVFMXM0DykrGxi4WC13vxkBy4aCPtfcUQkhRImVqNz9iSeeoEqVKowYMYJhw4bdV/n4xIkTGTZsGM2bN6dly5bMmDGD1NRUQ7f3oUOHUrlyZaZNm2b0urlz59KnTx/c3NxKfO6K5v3VJ4m8kUZVV1t+HtocGyu1m3vbAHdOXU1i1/l4ejeubNj/lHR2N628+ehSbiuEeIh06NDhrsnnggULjB5v3779nsdcsmTJfUZVdgR5O2Kl1XAzLZuohHSqSGVa6bB3V1eF+PkxuLQLdn6pruwghBDlUIlG0qOiohg3bhwrVqygRo0adO3alWXLlpGVVYRmPP8xYMAAvvrqKz788EMaN25MSEgIGzduNDSTi4yMJDo62ug1oaGh7Nq1656l7uK25IxstpxR157/fnAz3B1ud8BtE+AOwO6weKMvWnkj6Xkj7eI+GUbSZT66EEJUFDpLLbW81GXjZL30UuYRBD2/Ue/v+Bwidps3HiGEKKESJenu7u5MmDCBkJAQ9u/fT61atXj55Zfx9fXl1Vdf5dixY8U63rhx47h06RKZmZns37+f4OBgw3Pbt2/PdxU+KCgIRVHo3LlzScKvkHaciyMrV08Nd3vqVzZOulv4uWKtteBqYgbh8amAuob6Gensbjqp19V1mkGSdCGEqGDySt6PX5EkvdQ17A+Nh4Cih5WjIO2GuSMSQohiu+/GcU2bNmXSpEmMGzeOlJQU5s2bR7NmzWjXrh2nTp0yRYzCBP45pTbn61zPC81/5kPbWmtpWt0FgN235qVfvpFOcmYO1loLAr0cHmisD6Wow+pPt0CwdTFrKEIIIR4saR73gHX7Qv19mxQFf44F6QUghChnSpykZ2dns2LFCrp370716tX5+++/mTVrFrGxsYSFhVG9enX69etnylhFCWXl6Nl2Vi1171K34K75bfNK3s+rS7HlzUev5e2AlVbW875vMh9dCCEqLGke94DpHOCZeaC1htANcOBnc0ckhBDFUqLs65VXXsHHx4cxY8ZQq1Ytjh49yt69exk5ciT29vb4+fnx1VdfcfbsWVPHK0pg38XrJGfm4O6go0lVlwL3yZuXvudCPLl6xTAfvZ6PlLqbhMxHF0KICivI2xFLi9vN48QD4NMQunyq3v/nPYg+bt54hBCiGEqUpJ8+fZrvvvuOq1evMmPGDOrXr59vH3d3d5Mt1Sbuzz+nYwDoXNer0KXUGlR2xtHGkqSMHE5GJd7u7F5ZmsbdN73+drm7jKQLIUSFY2MlzePMouVoCOoOuVnwx0jIlgskQojyoURJ+pYtWxg0aBA6na7QfSwtLWnfvn2JAxOmodcrbDqtzkfvUs+r0P0stRY8UkNdzm73hfjbI+my/Nr9u3EBMhLA0ga88l/QEkII8fBrIPPSHzyNBnrPBgdviA+FTR+ZOyIhhCiSEiXp06ZNY968efm2z5s3j88///y+gxKmczwqkdikTOyttbSuefc15fPmpa8Jucq15Ew0GqjtLUn6fcsrdfdpBFor88YihBDCLOpXyUvSk8wcSQVj5wp9Zqv3D/wI5zebNx4hhCiCEiXpP/74I7Vr1863vV69esyZM+e+gxLFk6tXOBmVSK4+fzOaf06ppe4danuis9Te9Th589LPxiQD4O9uj73O0sTRVkB5TeMqS6m7EEJUVNI8zowCOkHLMer9P19Wl0UVQogyrERJekxMDD4+Pvm2e3h4EB0dfd9BieJZevAyPb/bxZjfDudL1P/JK3WvW3ipe56aHvZ4O9kYHsv66CaSN5JeRZrGCSFERVX7VvO4G6lZXE3MMHc4FU/nj8E9CFJiYe2rsiybEKJMK1GSXrVqVXbv3p1v++7du/H19b3voETxHAhXrwhvPhPLZ+vPGLZfiEsh7FoKVloNHWt73vM4Go2G1gG3S+JlProJZKdD7En1voykCyFEhWVjpSXwVvO4E1dkXvoDZ2ULT/8MFlZwdh2ELDJ3REIIUagSJemjRo3itddeY/78+Vy6dIlLly4xb948JkyYwKhRo0wdo7iH0NgUw/15u8NZuCcCwNAwrlVNd5xsijYXOm9eOkiSbhLRx0GfA/Ye4FLN3NEIIYQwowa3VkyRDu9m4tMIHntPvf/X23DjonnjEUKIQpRowvGbb77J9evXefnll8nKygLAxsaGt99+m0mTJpk0QHF3Obl6LlxTk/QhwdVYtD+Sj9eeokolW8N89KKUuudpE+CORgMaoK6PJOn37c756JqCl78TQghRMTSo7MyyQ1ekw7s5tX4Vzm+CS7th47sweIm5IxJCiHxKlKRrNBo+//xzPvjgA86cOYOtrS2BgYF3XZJNlI6I62lk5eqxt9bySe/65OQqLD10mVd+P0p6di6gro9eVF5ONswc2AQANwf587xvMh9dCCHELfX/0zxOIxdvHzwLLfSaCbOD4dxfELkPqj1i7qiEEMJIicrd8zg4ONCiRQvq168vCbqZhN7qxB7o5YiFhYZP+9anTYAbaVm5KAo0ruqC1x3N4IriyUa+PNlIeguYhHR2F0IIcUsdHye0Fhqup2YRLc3jzMc9EJo8q97fPFmayAkhypwSr6916NAhli1bRmRkpKHkPc/KlSvvOzBRNKGxapIedKsZjZXWgu+HNOOZH/Zw/loK3Rt4mzO8ii0lDhIiAQ1UbmruaIQQQpiZjZWWer5OHL+SyNf/nOPr/o3MHVLF1eEdOL4UIvfCub8h6AlzRySEEAYlGklfsmQJrVu35syZM6xatYrs7GxOnTrF1q1bcXaWZbsepHO3RtJreTsatjnbWrF0TCum92/E8Nb+5gpNXDmg/nSvBTby70IIIQS8270OFhr448gVlh26bO5wKi4nXwi+tXb6lo9Bn2veeIQQ4g4lStKnTp3KN998w9q1a7G2tmbmzJmcPXuW/v37U62adLB+kM79ZyQ9j6u9NU81rYK15X3NaBD348JW9adfG/PGIYQQosx4pIYbEzvXAuDDP08apq0JM2g7Qb2Ifu00nFhu7miEEMKgRBnchQsX6NGjBwDW1takpqai0WiYMGECP/30k0kDFIXLyM4l4noqALW8HcwcjTCiKHD+H/V+YBfzxiKEEAVYuHAh69evNzx+6623cHFxoXXr1ly6dMmMkT38Xu4QwKO1PMjI1vPyosOkZuaYO6SKybYStHlNvb/1M8jJNGs4QgiRp0RJeqVKlUhOVq/8Vq5cmZMnTwKQkJBAWlqa6aITdxV2LQW9ApXsrPCQTuxlS/x5dT661hr8HzV3NEIIkc/UqVOxtbUFYO/evcyePZsvvvgCd3d3JkyYYOboHm4WFhq+6d8IbycbLsSl8t6qEyjSvMw8gl8ERx9IjIRD880djRBCACVM0h999FE2bdoEQL9+/Rg/fjyjRo1i0KBBPP744yYNUBQur9S9lpejLONS1oSp/z6o3hqs7c0bixBCFODy5csEBAQAsHr1ap5++mlGjx7NtGnT+Pfff80c3cPPzUHHd4OboLXQsDrkKksOyvx0s7C2g/Zvq/d3fgmZMv1ACGF+JUrSZ82axcCBAwF47733mDhxIrGxsTz99NPMnTvXpAGKwhk6u3s73mNP8cBJqbsQooxzcHDg+vXrAPzzzz907twZABsbG9LT080ZWoXRws+VN7oEATB5zSniU6Tc2iyaPAuuNSEtHv75wNzRCCFE8ZP0nJwc1q1bh1arVQ9gYcE777zDmjVr+Prrr6lUqZLJgxQFM3R295IkvUzJTIFLe9T7AZ3NG4sQQhSic+fOjBw5kpEjR3Lu3Dm6d+8OwKlTp/Dz8zNvcBXImEdrUL+yE5k5ev46GWPucComrRX0+BrQwOH5cFKWEhZCmFexk3RLS0tefPFFMjIySiMeUQznYlMAGUkvcyL+hdwscKkO7oHmjkYIIQo0e/ZsWrVqRVxcHH/88Qdubm4AHD58mEGDBpk5uorDwkJD70aVAVh//KqZo6nAanaEdhPV+2vHw42L5o1HCFGhWZbkRS1btiQkJITq1aubOh5RRMkZ2UQlqOWItTwlSS9TDKXunUF6BQghyigXFxdmzZqVb/vHH39shmgqtm4NvPlswxn2h9/gWnIGno425g6pYurwLkTshsv7YPkIeOEfsJTGvEKIB69Ec9JffvllJk6cyKxZs9i7dy/Hjx83uonSlzeK7u1kg7OdlZmjEQaKAuc3q/el1F0IUYZt3LiRXbt2GR7Pnj2bxo0bM3jwYG7evFnk4+zcuZNevXrh6+uLRqNh9erV93zN9u3badq0KTqdjoCAABYsWJBvn9mzZ+Pn54eNjQ3BwcEcOHCgyDGVN1Uq2dGkmguKAhul5N18tJbwzFx1abboENg82dwRCSEqqBIl6QMHDiQ8PJxXX32VNm3a0LhxY5o0aWL4KUqfobO7lLqXLXGh6jIuWh34tzN3NEIIUag333yTpKQkAE6cOMHrr79O9+7dCQ8PZ+LEiUU+TmpqKo0aNWL27NlF2j88PJwePXrQsWNHQkJCeO211xg5ciR///23YZ+lS5cyceJEPvroI44cOUKjRo3o2rUr165dK96bLEd6NPABYN3xaDNHUsE5V4E+P6j3930PZzeYNx4hRIVUonL38PBwU8chiin0VtO4IC8HM0cijOQtvebXRpZeE0KUaeHh4dStWxeAP/74g549ezJ16lSOHDliaCJXFN26daNbt25F3n/OnDn4+/vz9ddfA1CnTh127drFN998Q9euXQGYPn06o0aNYsSIEYbXrF+/nnnz5vHOO+8U+VzlSfcGPny6/gwHI24Qm5SBl5OUvJtNUDdoNQ72zoLVL8FLu9XkXQghHpASjaRXr179rrfiKG45W0JCAmPHjsXHxwedTketWrXYsKHiXeW8c410UYacv5Wky9JrQogyztramrS0NAA2b95Mly7q/1uurq6GEfbSsHfvXjp16mS0rWvXruzduxeArKwsDh8+bLSPhYUFnTp1MuxTkMzMTJKSkoxu5Ymviy3NqldCUeCvEzKabnaPfwS+TSEjAVa/DHq9uSMSQlQgJRpJ//XXX+/6/NChQ4t0nLxytjlz5hAcHMyMGTPo2rUroaGheHp65ts/KyuLzp074+npyYoVK6hcuTKXLl3CxcWlJG+jXDsna6SXPZnJsvSaEKLcaNu2LRMnTqRNmzYcOHCApUuXAnDu3DmqVCm9UcOYmBi8vLyMtnl5eZGUlER6ejo3b94kNze3wH3Onj1b6HGnTZtW7pve9Wjgw+FLN1l/IprhbfzNHU7FZmkNT/0Mc9pC+A44+DMEjzF3VEKICqJESfr48eONHmdnZ5OWloa1tTV2dnZFTtKLW842b948bty4wZ49e7CyUpulVcS1XONTMolPyUKjgQBPKXcvM8J3gj4bKvmDW01zRyOEEHc1a9YsXn75ZVasWMEPP/xA5crqMmB//fUXTzzxhJmjK75JkyYZzaVPSkqiatWqZoyo+Lo38GHKutMcjLhJTGIG3s5S8m5W7gHQ5RPY8AZs+hBqdASPWuaOSghRAZSo3P3mzZtGt5SUFEJDQ2nbti2///57kY5RknK2NWvW0KpVK8aOHYuXlxf169dn6tSp5ObmFnqe8l7+VpC8UfRqrnbYWZfoOosoDYZSd1l6TQhR9lWrVo1169Zx7NgxXnjhBcP2b775hm+//bbUzuvt7U1sbKzRttjYWJycnLC1tcXd3R2tVlvgPt7e3oUeV6fT4eTkZHQrb7ydbWjhVwmADVLyXja0GKkm5zkZsGoM5OaYOyIhRAVQoiS9IIGBgfzf//1fvlH2wsTHxxdazhYTU/DyIxcvXmTFihXk5uayYcMGPvjgA77++ms+/fTTQs8zbdo0nJ2dDbfydlW9IHlN42Q+ehmiKLeTdCl1F0KUE7m5ufzxxx98+umnfPrpp6xatequF75NoVWrVmzZssVo26ZNm2jVqhWgzpVv1qyZ0T56vZ4tW7YY9nmY5XV5Xy9Jetmg0UCf78HGGa4egV3TzR2REKICMFmSDmBpacnVq1dNeUgjer0eT09PfvrpJ5o1a8aAAQN47733mDNnTqGvmTRpEomJiYbb5cuXSy2+B8UwH12S9LIj7iwkXQFLG/Bra+5ohBDinsLCwqhTpw5Dhw5l5cqVrFy5kmeffZZ69epx4cKFIh8nJSWFkJAQQkJCALVrfEhICJGRkYD6e/jOaXAvvvgiFy9e5K233uLs2bN8//33LFu2jAkTJhj2mThxIj///DMLFy7kzJkzvPTSS6Smphqmxz3MujXwQaOBw5ducjUh3dzhCAAnX+hxKznf8TlcPWreeIQQD70S1UqvWbPG6LGiKERHRzNr1izatGlTpGOUpJzNx8cHKysrtFqtYVudOnWIiYkhKysLa2vrfK/R6XTodLoixVReGEbSpWlc2XH+H/WnX1uwtjNvLEIIUQSvvvoqNWvWZN++fbi6ugJw/fp1nn32WV599VXWr19fpOMcOnSIjh07Gh7nzQsfNmwYCxYsIDo62pCwA/j7+7N+/XomTJjAzJkzqVKlCr/88oth+TWAAQMGEBcXx4cffkhMTAyNGzdm48aN+arvHkZeTja08HPlQPgNNpyIZmS7GuYOSQDUfxrOroNTq2DlGBizE6ykZ4AQonSUKEnv06eP0WONRoOHhwePPfaYYd3Te7mznC3veHnlbOPGjSvwNW3atGHx4sXo9XosLNQigHPnzuHj41Nggv4wUhSFc7EpgIyklylS6i6EKGd27NhhlKADuLm58X//939FvuAO0KFDBxRFKfT5BQsWFPiao0fvPho5bty4Qr8PPOx6NvThQPgN1h6XJL3M0GjU0fRLeyE+FHZ+AY9/aO6ohBAPqRKVu+v1eqNbbm4uMTExLF68GB8fnyIf517lbEOHDmXSpEmG/V966SVu3LjB+PHjOXfuHOvXr2fq1KmMHTu2JG+jXLqamEFKZg6WFhr83e3NHY4AyEiCyH3q/UBJ0oUQ5YNOpyM5OTnf9pSUlApz4buseqK+N1oLDccuJ3Aw4oa5wxF57Fyhx63BqN0zIeaEeeMRQjy0TDonvbgGDBjAV199xYcffkjjxo0JCQkxKmeLjIwkOvp245SqVavy999/c/DgQRo2bMirr77K+PHjC1yu7WF1KioRgBoe9lhbmvWPT+QJ36EuveZaQ5ZeE0KUGz179mT06NHs378fRVFQFIV9+/bx4osv8uSTT5o7vArN09GG/s3VRrdfbgy9a6WCeMDq9IS6vUGfA3+Ok27vQohSUaIs7+mnn+bzzz/Pt/2LL76gX79+xTrWuHHjuHTpEpmZmezfv5/g4GDDc9u3b89XJteqVSv27dtHRkYGFy5c4N133zWao/4w0+sVZm8LA+CRGm5mjkYYGJZe62LeOIQQohi+/fZbatasSatWrbCxscHGxobWrVsTEBDAjBkzzB1ehffq4wFYW1pwIOIGO87FmTsccaduX4KNC0SHwL7vzR2NEOIhVKIkfefOnXTv3j3f9m7durFz5877DkoUbNXRKI5dScRBZ8m4xwLMHY4Adem1sM3qfZmPLoQoR1xcXPjzzz85d+4cK1asYMWKFZw7d45Vq1bh4uJi7vAqPB9nW4a1qg7Al3+HotfLaHqZ4egFXT9T72+bCjcumjceIcRDp0SN4wqbr2ZlZUVSUtJ9ByXyS83M4Yu/zwIwtmMAno7SUbRMuHYakqJuLb1W9EZLQghhDnmd1wuzbds2w/3p02U9aHN7qUMAvx+4zKmrSfx1MoYeDYve90eUssZD4Pgydcrb2vEwdI3aXE4IIUygREl6gwYNWLp0KR9+aNzVcsmSJdStW9ckgQljc3ZcIDYpk6qutoxo42fucESevFJ3/0fByta8sQghxD3cq6N6Ho0kG2WCq701I9v5M2Pzeb7eFErXel5YaqUfTZmg0UCvmfB9KwjfCUd/g6ZDzR2VEOIhUaIk/YMPPuCpp57iwoULPPbYYwBs2bKF33//neXLl5s0QAFXbqbx0061lOq97nWwsaoYc/DLBVl6TQhRjtw5Ui7Khxfa+rNwTwQX41L548gVBrSoZu6QRB5Xf3jsPfjnffj7ffWCfSU/c0clhHgIlOhybK9evVi9ejVhYWG8/PLLvP7661y5coXNmzfnW0Nd3L//++ssmTl6HqnhStd63uYOR+TJSILLeUuvdTJvLEIIIR5KjjZWjO2o9qGZufk8Gdm5Zo5IGAl+Caq0hMxEWPE85GSZOyIhxEOgxDVTPXr0YPfu3aSmphIfH8/WrVtp3769KWMTwMGIG6w7Ho2FBj7sWU9KEMuSi9vVJVjcAtTl14QQQohS8Owj1fF2suFqYgaL9keaOxxxJ60lPDNX7fYedRi2fGzuiIQQD4ESJekHDx5k//79+bbv37+fQ4cO3XdQQqXXK0xZexqAAS2qUdfXycwRCSPn/1F/Sqm7EEKIUmRjpWV8p0AApv8TyrnYZDNHJIy4VIM+t5Zi2zsLQjeaNx4hRLlXoiR97NixXL58Od/2qKgoxo4de99BCdX6E9GciErEUWfJ611qmTsccSdFgbAt6n0pdRdCCFHK+jWrwiM1XEnNymXUr4dISJOy6jKldg+19B1g9YuQeMW88QghyrUSJemnT5+madOm+bY3adKE06dP33dQQh1Fn7U1DICR7Wrg7qAzc0TCSOwpSL4KlrZQva25oxFCCPGQs9Ra8P2QZlSpZMul62mMW3yUnFy9ucMSd+o8BXybQPpNWPEC5OaYOyIhRDlVoiRdp9MRGxubb3t0dDSWliVqGC/+45/TMYTGJuOos2S4LLlW9uSVuvs/ClayZr0QQojS52pvzc9Dm2NnrWVXWDyfbThj7pDEnSyt4Zl5oHNSG8tu+9TcEQkhyqkSJeldunRh0qRJJCYmGrYlJCTw7rvv0rmzzM+9X4qiMHOLOoo+oo0fzrZWZo5I5BO2Wf0ZKH/fhRBCPDh1fJyY3r8xAPN3R7DsYP7ph8KMXGvAk9+q93d9AyG/mzceIUS5VKIk/auvvuLy5ctUr16djh070rFjR/z9/YmJieHrr782dYwVzuYz1zgTnYS9tZbn2/qbOxzxXxmJEHlr6bUAmY8uhBDiwXqivjev3Wok997qE2wPvWbmiISRen2h7QT1/ppXIGKXeeMRQpQ7JUrSK1euzPHjx/niiy+oW7cuzZo1Y+bMmZw4cYKqVauaOsYKRVEUvtt6HoChrf1wsbM2c0QinwvbQMkFt0BwlYsoQgghHrxXHwukW31vsnMVhs8/yNB5Bzh2OcHcYYk8j30IdfuAPhuWDIH48+aOSAhRjpR4Arm9vT1t27alWrVqZGWpHUb/+usvAJ588knTRFcBbT8Xx/EridhaaRkpo+hlU9gm9WdgF/PGIYQQosKysNDwdf9GuDlYs+TAZXaei2PnuTg61/ViYuda1PGRZVvNysIC+s6BpCi4chAWPQMjt4C9u7kjE0KUAyVK0i9evEjfvn05ceIEGo0GRVHQaDSG53Nzc00WYEWiKAozN6tXWp9rVR036ehe9igKnM+bjy6l7kIIIczHztqST/s0YHS7mszccp5VR6+w6XQsm07H8kmf+jz3SHVzh1ixWdnCwN/hl8fhZgT8PgiGrZWGs0KIeypRufv48ePx9/fn2rVr2NnZcfLkSXbs2EHz5s3Zvn27iUOsOHaFxRNyOQGdpQUj28koepkUcwJSYsDKDqq3MXc0QgghBNXc7Pi6fyP+mdCebvW9Afhk3WnCriWbOTKBgwcMWQ42znDlAKwaA3pZOk+IBy49AXIyzR1FkZUoSd+7dy9TpkzB3d0dCwsLtFotbdu2Zdq0abz66qumjrHC+HaLOoo+OLgano5ylbVMyit1928PllLpIIQQouwI8HTg+yFNaV/Lg6wcPa8vOyZrqZcFHkEw4H9gYQWnV8PGd9TKPCFE6bsZAYsHwOfV4VNP+L9q8F0zmNcN/hwHyTHmjrBAJUrSc3NzcXR0BMDd3Z2rV68CUL16dUJDQ00XXQUSnZjOwYibaC00jHm0prnDEYU5nzcfXUrdhRBClD0ajYbPn26Ik40lx64kMmfHBXOHJAD8H1XnqAMc+BF2zzBrOEI89HIyYeeXMDsYzm28vT0jEa6HQeQeOPobLOh590Q94TKEbiz8+VJSojnp9evX59ixY/j7+xMcHMwXX3yBtbU1P/30EzVq1DB1jBXChWupAPi52eHtLKPoZVL6Tbh8QL0fIOujCyGEKJu8nW2Y/GQ9Ji47xswt53msthd1faWRnNk1eAZSrsHfk2DzZHDwgsaDzR2VEOalz4XwHRB3Dmyc1KkhRjcX0DnCHf3PAMjNgYwEtYxdn3PH8xq4GQ5/v6sm46BeJOv+Fdh7QGo8pF5TE/PNk+H6eVjYC4atA0cv43OcWAHrJqqrNIz5F9wDSvWjuFOJkvT333+f1FQ1qZwyZQo9e/akXbt2uLm5sXTpUpMGWFGEx6cA4O/uYOZIRKHyll5zD4JK0oxHCCFE2dW3SWU2nozhn9OxTFwWwppxbbG2LFEBpTClVi9DcjTs+VYttbX3lOo8UTHdjICjiyBkMSRdufu+Gi3YuqgJuz5bTcwzk+59Dgcv6DoV6j99O4m3cwWPWur9ys3UkfT4c2qiPnwdOHiqx9/wJpxYdmu/5mChLdHbLKkSJeldu3Y13A8ICODs2bPcuHGDSpUqGXV5F0V3IU696FHDw97MkYhCheV1dZdRdCGEEGWbRqPhs74NOBhxg7MxyXy39Tyvdwkyd1gCoNPHkBILx5fCsqFqx/cqzcwdlRD3Jzcboo+rA1mFLTWYdgNCN6h/98N33t5u4wJ+bSE7XU2+MxLVRDkjAXKz1EGytOvq7b90TmCRl9Iqar8HC0u1cqXju+pofGFc/WH42luJeqiaqD/2PmycBImXQWMBj74Fj74BWquSfS4lVOJ10v/L1dXVVIeqkMLjbyXp7pKkl0l6vSTpQgghyhUPRx2f9W3Ay4uO8P32CzjoLBkUXA0nmwf7ZVP8h4UFPDkLUuPgwlb431MwfD141zd3ZKKi0+dCxC61FNxSpy4jaGmj/nQPBNtKBbxGD6dWwrbP4MZFdZt3A6jREWp0APda6nfoM2vUxFyfc+uFGvX5ps9BUI+ClyZUFDVxzytrT78JWms1DjtXNUHX3mc661pDvVC2oCfEnYWlz6rbK/nBUz9D1Zb3d/wSMlmSLu5PXpLuL0l62RRzXL3qbWUP1VqZOxohhChTZs+ezZdffklMTAyNGjXiu+++o2XLgr/YdOjQgR07duTb3r17d9avXw/A8OHDWbhwodHzXbt2ZePGB9+8p7zr3sCHPo19WR1ylWl/neW7rWEMaFGVEW38qFLJztzhVVyW1tD/N/itr7o02299YPiG22W4QhQmJ1NdKcDChNNX4sMgZBEcWwLJVwveR6OF6q0hqDsEdVOT2LDNsOVjdYliUL8nZ6eqj2NOqNM6/surPtTtDY0Ggku1u8el0YC1nXpz8r2vt3hXbjXVUvcFPdTpKI2fhW7/p86FNxNJ0suAzJxcrtxMA8Bfyt3Lpryl12p0kKXXhBDiDkuXLmXixInMmTOH4OBgZsyYQdeuXQkNDcXT0zPf/itXriQrK8vw+Pr16zRq1Ih+/foZ7ffEE08wf/58w2OdTv7vLakv+zWidU13fv73IuevpTB3VzgL9kTQs6EPU56sj7OdjKybhc5BXUN9YS91MODX3vD8X2ryI8R/ZSSpo9UHfwGtTr2g41kXPGqDVz21OVpxS7LPrFMT6cv7b2+zrQQ+jdSLAdnpkJMBmSnqvPGIf9Xb35PAwRtSbnVF1zlB61fhkZcgOw0u7oCL29R+TslXwacx1H0S6vR+oM3XisWtJry8FxKvqJUAZiZJehkQeT0NvQIOOks8HORLSJkkS68JIUSBpk+fzqhRoxgxYgQAc+bMYf369cybN4933nkn3/7/nR63ZMkS7Ozs8iXpOp0Ob2/v0gu8ArHSWtC/RVX6Na/CjnNx/PzvRXaHXefPkKucj03hfyODcbW3NneYFZOtCzy3GhZ0V0ttF/aCERvBubK5IxNlhaLA2XWw4a3bo9z6HLh6VL3lcQ+CJ6ZBwOP3PqZeryb8/36lPtZYQEAnaDxEHSUvaEDqRjiE/qXOKb+0R03QtToIHg1tJ6rl56BefGrYT70pipq0W5eTQUjbSgWX9JtBmWjzOXv2bPz8/LCxsSE4OJgDBw4Uuu+CBQvQaDRGNxub8r1k2cX4203jpPFeGZR2A64cVO/L0mtCCGGQlZXF4cOH6dTp9gVMCwsLOnXqxN69e4t0jLlz5zJw4EDs7Y2/xG3fvh1PT0+CgoJ46aWXuH69gIZBd8jMzCQpKcnoJoxpNBo6BHmyaOQjrB7bBncHHaejkxj00z7ikjPNHV7FZe8GQ/9U58YmRMKvT9593WZRcSRchiWD1XnSyVehkj88+weMPQj9f4UOk6BuH7B1VRuf/e8pWDwQrl8o/JjZ6fDHC7cT9Edehgmn1aqOen0Krxh19VdXJxi+Dt4MgyF/wPgQ6PLp7QT9vzSa8pOglzFmT9LzyuQ++ugjjhw5QqNGjejatSvXrl0r9DVOTk5ER0cbbpcuXXqAEZuezEcv4y5uA0UPHnXApaq5oxFCiDIjPj6e3NxcvLyM15b18vIiJubeScaBAwc4efIkI0eONNr+xBNP8Ouvv7JlyxY+//xzduzYQbdu3cjNzS30WNOmTcPZ2dlwq1pV/r++m8ZVXVgy+hE8HXWExiYz8Ke9xCZlmDusisvRG4auAeeq6trOv3SC2NPmjkqYy/ULsP4NmNVCHbm2sIR2b6jl2AGd1FL3ur2hwzvQfyG8ehQeGavud+4vmB0MG99VR7yz0m4fN+WaWq1xaqU6r733bHX03cmnePHZuarVpaU5T7yCM3uSfmeZXN26dZkzZw52dnbMmzev0NdoNBq8vb0Nt/9+OShvLsblrZEuSXqZJKXuQghRKubOnUuDBg3yNZkbOHAgTz75JA0aNKBPnz6sW7eOgwcPsn379kKPNWnSJBITEw23y5cvl3L05V+ApwPLxrTC19mGC3GpDPhxL1cT0s0dVsXlUlXtMu0WoC7/NK+r2v1dVAyKAhG74ffB8F0zOPgz5KSrDYtf3AWPf6B2WS+IrQs8MRVe2guBXdS1xPfNhvndYFoV+PFRWDcRfnlcrQ61cYHnVkGTZx/kOxTFYNYkvaRlcikpKVSvXp2qVavSu3dvTp06Vei+5aH8zbD8moeDmSMR+SjK7V+QUuouhBBG3N3d0Wq1xMbGGm2PjY2953zy1NRUlixZwgsvvHDP89SoUQN3d3fCwsIK3Uen0+Hk5GR0E/fm527P0jGtqFLJlojraQz4aS/JGdnmDqvicvWHFzZBtdbqetGL+sGRX80dlShtEbvVBHpBdwhdDygQ2FWtrhjxF3jWKdpxPGqpZetDVkDtnmpzNyUXoo/BobnqdArXGjByC/i3K9W3JO6PWZP0kpTJBQUFMW/ePP7880/+97//odfrad26NVeuXClw//JQ/iZrpJdhCZfUpdcsrKBqsLmjEUKIMsXa2ppmzZqxZcsWwza9Xs+WLVto1eruy1UuX76czMxMnn323iM5V65c4fr16/j4FLMkUxRJVVc7lo5pRWUXWy7fSGf5oYK/U4kHxM4Vhq6GBv3UBmFrXoHNkyEn616vFGVNbjacXgN7v4fo4+rgz52uX1Dnmy/oDlGH1TXJm41Q55wPWQY12qvzuosrsDMMXASvn4UJp6DfAmg1DtqMVxP0stphXRiUu+7urVq1MvrF37p1a+rUqcOPP/7IJ598km//SZMmMXHiRMPjpKSkMpWoJ6ZnE5+i/qfrJ0l62XPlkPrTuz5Yle8GhUIIURomTpzIsGHDaN68OS1btmTGjBmkpqYaur0PHTqUypUrM23aNKPXzZ07lz59+uDm5ma0PSUlhY8//pinn34ab29vLly4wP+3d9/hUZVpH8e/k0nvvRJIIHQInRDAtYCisipWdFEQ27qKq2Jv2AVFfZHVFdbewYLYQUVAkd576ISWBukhbea8fxwYjYQSSDIz5Pe5rnNlPG3uZ8bw5D5Pu//++0lJSWHQoEGNVq6mJiHUj1vPasVj09fy/oIdXN83CQ8PTWbrNJ4+cNkb5kRhv74A8/4P1nwB/e8yuyhrOdiGU1Fidgm3VZmfs5efmTx7B5jL43lYj3+PgkxY9h6s+MBs7DksMNYcU54ywEzKF002u6ZbPKDH9eZEcIFHLl150iwWCGlmbh0vrb/7SoNzapJ+Kt3kDvPy8qJbt25H7QLn4+Pj0murHm5Fjwn2IdDH7Z6ZnP72LDN/Nuvl3DhERFzU0KFDyc3NZcyYMWRlZdG1a1dmzJjh6CWXmZmJh0fNjnsZGRnMmzePH3/88Yj7Wa1WVq9ezXvvvUdBQQHx8fGcd955PP300y5dn58OLuuWwAszNrJjfxlzN+dydtt6TBak7iwWOOcRiGwNMx+Bwkz4bjT8+qKZrHcffvQxynLi7Ie6g2/9xVzXe9ciM3GujW+IuR55y7Oh5Vlm13Fbldnzcv9WOLANts2BzT8Ch1rNA2PMdbcPL1u28kNzO6zVABj07Il3aZcmwalZ4Z+7yQ0ZMgT4o5vcqFGjTugeNpuNNWvWcOGFFzZgpA1ne54mjXNph1vSE3o6Nw4RERc2atSoo9bbtU321rZtW4y/dvs8xM/Pj5kzZ9ZneHKCAnw8ubJHIm//vp335u9Qku4qUq+C9heZLbO/TzCX4vrhfnOd6+bp5nC85ukQ3029/k5UVbmZTG/8xlz7u+wvSzyGNjeXNauuMCdvqyqH8kJz2/CNuYF5TnmBuQrQXyWfCb1uhLYXgtXLvNfO32Hzz+YDAe8As+VcExNLLZzedFvXbnJPPfUUffr0ISUlhYKCAsaPH8/OnTuPWL7FXWzPPbz8miaNcznVleaTVYBmStJFROT0Nzy9Be/M386cjFy255WqEcFVePlBn1vNLtErP4R5E8wZ4DfNMDcAqzfEpkJCd4jvbv6MaA0eTl/MyTkMAyqKoTTXXHqsNMf8uWMebPkZKkv+ONcn2Gwhb3U2tDrHbCH/K1s17FtpLs27dY7Z4n7wgHnMO9Cc9C+8JUS1g85XHTnu29PHvHercxqqxHIacXqSXtducvn5+dx8881kZWURFhZGjx49mD9/Ph06dHBWEU7JVk0a57qy14KtAvzCav/HWkRE5DSTFBnAWW2imJ2Ry/sLdvD4RR2dHZL8mZcv9LoJul8PWasgcxFkLoDMhWYSumepuR3mHQRRbc2W4dBE82dIc4jrAkFusIRx1hpY/5W5fnzzPsd/6FBVbibhm2bAppnmEIGjCU6AdoPNWdBb9DVbu4/F6mk22jTrCX+7DypLIW8TBMWb48hPZoI3kaOwGEfrb3aaKioqIiQkhMLCQpdYnuXCV35j/b4i3hrRkwHt3eAfy6Zk8Rvw/b3mBB/XfuHsaETkNOZqddPpQJ/pyZuTkcP17ywhyMeThQ8PIEBz5rg+w4D87bB7GexdDnuWm70Bq4+x7n1Ea0jqB0lnmElqYMyJTYrWGPJ3wOznYPWnOMZ2g9lw0qw3xHcFixVsleb4cVsV5O80W7mrymreyzsQAqLMRDogynxo0W6w2dtAibU0orrUS/pX14kMw3BMHKfuZC5I49FFRKQJ+lvrKJIjA9ieV8q05bu5Lj2pxvH80kr8vK34erlIQidmshne0txSrzT32aohL8OczKwg89C2y/zv3I2wf7O5LXv3j/t4eP0xm7mnj7kEnL3aTILt1eDhaSbIiX0gsbc5sa5vMNjt5rjukiwozobK4kPLjRl/LDvmG2K2Xoc0M6+pTWke/Doelrz1x+Rtbc43Z1zfswwO5sPmmeZ2NEHx0GaQeV1SP/AJOsUPV6TxKUl3oqyicg5W2fD0sJAY7u/scOSvDncX03h0ERFpQjw8LAxPb8GT36znvQU7ubZPCywWC/tLKpg4azMfLcokJtiXN0f0pH2ceim4LKsnxHQ0t786mG92kd8xD3b8ZnYrN+xmYlxRBRVFR7/vtjnmBoDFbKEu228m8SfKJwRCEsyW+6qDUFlmtoBXFINhM89peRYMfMKcEA/MuYKy1sCuheZDBovV7KJu9TYfHviFmuO9Y1PVQi5uT0m6Ex2eNK55uD9e1iY6qYerKjsA+w8t65fQw7mxiIiINLIrejTjxZkZbMkpYdaGHDKyi3l9zlZKKsxEbE/BQS5/fT4ThnblvI4ntmyuuBC/MGh7gbmBOZa7svSPmcyry83ZyD0OJcIeXmbSf3gN8V2LzC1/R811wP0jzW7zviFmomz509+35QVQuNt8QFBRCDmFtccW19VMzludXXO/pzc062FuIqc5JelOtE1d3V3XnuXmz/CW4B/u3FhEREQaWZCvF1f0aMZ7C3Zy0/t/TETWMT6Yuwe24d35O5i3JY9/friMe89ry21ntcKi1kv35eV74su3xaWaS4sBFGdB8T4IiDZb1I83+RqYiX7RHjNhxwCvALOLvXeAuQXFqSVcmjwl6U6k8egubI/Go4uISNM2vG8S7y3YCUBCqB/3DmrDJV0S8PCwcGbbKJ7+dj3vL9jJ+EMt7mMv66xx6k1NUKy51YVPoDl5W1TbholJ5DSgJN2JtuWa6zMmRylJdzm7NR5dRESatlZRgbw+rDsHyiq5vHuzGgm4l9WDpy7pROuYIJ74eh1frtiDzW4w8ZpuToxYROT0oIHQTrTdsUZ6oJMjkRoMw5xBFNSSLiIiTdoFneMYltbiqC3k1/VpwTvX9wLg29V72XWgrNbzRETkxClJd5LKaju78s21K1uqJd21HNgGBw+Ys4XGdnJ2NCIiIi7tb22iOKN1JHYDPly409nhiIi4PSXpTpJ5oAyb3SDA20p0kI+zw5E/O9yKHptqrhEqIiIixzTi0FrqU5bs4mClzbnBiIi4OSXpTuKYNC4qQLOhuhqNRxcREamTs9tFkxjuR+HBKr5aucfZ4YiIuDUl6U6yPe/QpHEaj+56NLO7iIhInVg9LAzvkwTAu/N3YBiGcwMSEXFjStKdZFuull9zSdUVkLXGfN2sh3NjERERcSNX9UzE18uDjVnFLNmR7+xwRETclpJ0J9l2qLt7K00a51qy1oCtEvwjICzZ2dGIiIi4jRB/Ly7tlgDAe/N3ODcYERE3piTdSdSS7qJ2/6mru+YKEBERqZMRfZMAmLEui32FB50bjIiIm1KS7gQHSivJK6kAoGWUxqS7lN1LzJ+aNE5ERKTO2sUGk5Ycjs1u8NHCTGeHIyLilpSkO8HGfUUAtIjwJ9DH08nRiMOWn2Hdl+br5unOjUVERMRNXX+oNf2TxZmUV2k5NhGRulKS7gQbsooBaBcb5ORIxCF7HXx6PRg2SL0akvo7OyIRERG3dG6HGOJCfNlfWsm05VqOTUSkrpSkO8HhlvR2scFOjkQAKM6Cj66CymJo0R8unqjx6CIiIifJ0+rB8PQkAB77ai3v/L5dS7KJiNSBknQn2JBlJunt45SkO11lKXw8FIp2Q0RrGPoBePo4OyoRERG3dmP/ZC7rnoDNbvDkN+t58Is1VFQf2fXdMAyqbHYnRCgi4ro0ILqRVdvsbMouAaB9nLq7O5XdBl/cBPtWmkuuDfsU/MOdHZWIiIjb8/b04KUru9AhLpjnvt/A1KW72JpbwqTreuDrZeX3LXnM3ZTL3IxccksqmDC0Kxd2jnN22CIiLkFJeiPbsb+Uymo7Ad5WEsP8nR1O0/bri5DxPVh94OpPILylsyMSERE5bVgsFm46oyUp0YHc8ckKlu7MZ8BLcymtqKbaXrP7+51TVuDnbeXsttFOilZExHWou3sjW7/PnDSubWwQHh4a9+w0laWw8DXz9d//D5qnOTceERGR09RZbaOZfns/WkYGUHiwimq7QVKEP9f3TeKd63vx99Q4qmwGt36wjIXb9js7XBERp1NLeiNzTBqn8ejOtfpTKC+EsCTocrWzoxERETmttYoKZPqofszNyKVzQghJkQGOY/1SIimrtPHLxhxuem8pH9+cRmqzUOcFKyLiZGpJb2QbDy2/1l7LrzmPYcCiyebr3reAh9W58YiIuLnXXnuNpKQkfH19SUtLY/HixUc9991338VisdTYfH19a5xjGAZjxowhLi4OPz8/Bg4cyObNmxu6GNLAgn29uKhLfI0EHczx6/8d1p0+LcMpqahm+NuLyTj095KISFOkJL2Rbdinmd2dbvuvkLsBvAKg6zBnRyMi4tamTp3K6NGjefzxx1m+fDldunRh0KBB5OTkHPWa4OBg9u3b59h27txZ4/gLL7zAxIkTmTRpEosWLSIgIIBBgwZRXl7e0MURJ/H1svLmiF50SQyloKyKa99aRE7xsb/vJTsOsPHQijkiIqcTl0jS6/IE/s+mTJmCxWJhyJAhDRtgPSkoq2RfoVnhtFFLuvMcbkXveg34hTo1FBERd/fyyy9z8803M3LkSDp06MCkSZPw9/fn7bffPuo1FouF2NhYxxYTE+M4ZhgGEyZM4NFHH+WSSy4hNTWV999/n7179zJ9+vRGKJE4S6CPJ++N7EXr6EByiyt48pv1Rz13/pY8rpy0gEtfm8/W3JJGjFJEpOE5PUk/mSfwADt27ODee+/ljDPOaKRIT93hru7NwvwI9vVycjRNVP4Oc0Z3MLu6i4jISausrGTZsmUMHDjQsc/Dw4OBAweyYMGCo15XUlJCixYtSExM5JJLLmHdunWOY9u3bycrK6vGPUNCQkhLSzvmPSsqKigqKqqxifsJ9ffm/4Z2xcMC363ex+yNR/49WFJRzX2frwbgYJWNu6aspLJaa62LyOnD6Un6yTyBt9lsDBs2jCeffJKWLd1n2azDXd3bxaqru9MsfgMwoNU5ENXW2dGIiLi1vLw8bDZbjZZwgJiYGLKysmq9pm3btrz99tt89dVXfPjhh9jtdvr27cvu3bsBHNfV5Z4AY8eOJSQkxLElJiaeStHEiTolhHBDv2QAHp2+lrLK6hrHn/t+A3sKDpIQ6keovxdr9hTyfz9vckaoIiINwqlJ+sk+gX/qqaeIjo7mxhtvPO57uNKT9Y2Hll/rEKeu7k5RUQLLPzBfp/3LubGIiDRR6enpDB8+nK5du3LmmWcybdo0oqKimDx58ind96GHHqKwsNCx7dq1q54iFme4+9w2JIT6safgIBN+/mPSwF835fLxokwAxl+ZythLOwMwae5WFmzV8m0icnpwapJ+Mk/g582bx1tvvcUbb7xxQu/hSk/WD09uouXXnGT1FKgohPCWkDLw+OeLiMgxRUZGYrVayc7OrrE/Ozub2NjYE7qHl5cX3bp1Y8uWLQCO6+p6Tx8fH4KDg2ts4r4CfDx5ZkgnAN6at511ewspKq/iwS/Mbu4j0lvQt1UkF3SOY2jPRAwDRn+6ksKyquPe2zAMCg8e/zwREWdxenf3uiguLua6667jjTfeIDIy8oSucZUn6za7QUa22ZLeTpPGNb4ay679Ezzc6n99ERGX5O3tTY8ePZg1a5Zjn91uZ9asWaSnp5/QPWw2G2vWrCEuLg6A5ORkYmNja9yzqKiIRYsWnfA95fRwdrtoBqfGYbMbPDRtDU99s569heW0iPDngQvaOc4bc1EHkiMD2FdYzsNfrsEwjKPec+f+Uka+u4QuT/7If+dsaYxiiIjUmacz37yuT+C3bt3Kjh07uOiiixz77HZzohBPT08yMjJo1apVjWt8fHzw8fFpgOjrZsf+Usqr7Ph5WWkREXD8C6R+bZsNeZvAOxC6/sPZ0YiInDZGjx7NiBEj6NmzJ71792bChAmUlpYycuRIAIYPH05CQgJjx44FzCFrffr0ISUlhYKCAsaPH8/OnTu56aabAHPm97vuuotnnnmG1q1bk5yczGOPPUZ8fLzbrOYi9efxizrw66ZcVu8uZPXuQiwWGH9FF/y9//gTNsDHkwlDu3L56/P5bs0+Wv0UwJU9E0kM93ecU1FtY9Kcbbw2Z4tjkrmXf9zEmW2i6Bgf0ujlEhE5Fqcm6X9+An+44j38BH7UqFFHnN+uXTvWrFlTY9+jjz5KcXExr7zyiktPEnN4PHqb2CCsHhYnR9MEOZZdGwa+6gIpIlJfhg4dSm5uLmPGjCErK4uuXbsyY8YMx1C2zMxMPP7Ueyk/P5+bb76ZrKwswsLC6NGjB/Pnz6dDhw6Oc+6//35KS0u55ZZbKCgooH///syYMQNfX99GL584V3SQLw9e0I5HvlwLwA39kumdHH7EeV0SQ7n73DaMn5nBxF+2MPGXLaREB3JWmyjaxAbx+pytbM8rBaB/SiRWDwtzN+Vy32er+WpUP7ys6mEnIq7DYhyrT1AjmDp1KiNGjGDy5MmOJ/CffvopGzduJCYm5ogn8H91/fXXU1BQcMJrpxYVFRESEkJhYWGjjld7cWYGr87ewtW9Ehl3eWqjva8A+7fCf3oABoxaBpEpzo5IRKQGZ9VNpzN9pqcPu93g3s9XUVBWxWv/6I6ft7XW82x2gw8X7uS71ftYlpmPzV7zT9zoIB8e+3sH/p4aR15JJef+31wKyqq459w23DGgdWMURUSasLrUS05tSYe6P4F3V4cnjWuvSeMa35I3AQNSzlWCLiIi4mY8PCy8fFXX455n9bAwom8SI/omUXiwinmb85idkcPaPYX0bRXJ3ee2JsjXC4CoIB+evLgjd05ZycRfNnNex1jaas4gEXERTm9Jb2zOerLeb9wv7Ck4yNRb+pDWMqLR3rfJqyiGlztARREM+wJaa1Z3EXE9avWtf/pM5XgMw+Dm95fx84ZsUpuFMO1fffFUt3cRaSB1qZf0L1EjKDxYxZ6CgwC0i9UfCo1q1RQzQY9IgVbnODsaERERcREWi4XnLu1EsK8nq3cX8sZv250dkogIoCS9UWRkmZPGJYT6EeLv5eRomhC7XcuuiYiIyFFFB/vy+EUdAfi/nzaxJafEyRGJiChJbxSHx6NrffRGtu0X2L8ZvIOg6zXOjkZERERc0GXdEzi7bRSVNjvjftjo7HBERJSkN4YN+w4l6XFK0hvV4Vb0bteCjz57EREROZLFYuHRv3fA6mHh5w3ZLN1xwNkhiUgTpyS9Eazfq5ndG93+rbD5R8ACvW92djQiIiLiwlpFBXJVz2YAPD9jI01sXmURcTFK0htYWWU16w4l6V2ahTo3mKZk8f/Mn63Pg4hWzo1FREREXN6dA9rg4+nBkh35/LIxx9nhiEgT5vR10k93y3bmU203iA/xpVmYn7PDOf0YBmyfCwf+PCOrASs+Ml+m/dMpYYmIiIh7iQ3x5fp+SUyeu40XZmRwVttorB4WZ4clIk2QkvQGtmibOa4prWUEFov+oa93816GWU/VfiyyjZZdExERkRN225kpfLIok4zsYr5auYfLujdzdkgi0gQpSW9gi7cfStKTw50cyWlo7Rd/JOgtzwbvgD+OeVihz22gByMiIiJygkL8vfjXWSk8P2MjL/24icGpcfh4Wms9d39JBZuyS9iSW0KrqAD6too85r135JUSG+KLr1ft9xMROUxJegMqr7KxclcBYLakSz3KXARf/st83ec2OH+sc+MRERGR08L1fZN4d/529hQc5KOFmYzom8S23BLW7ClkzZ5CNuwrYnN2CftLKx3XWD0sfDOqPx3ia58k+OtVe/n3JysY2D6GN0f0bKyiiIibUpLegFZkFlBpsxMd5ENShL+zwzl9HNgGU64BWwW0HQznPePsiEREROQ04edt5c4BbXj4yzW8MHMjL/6YQVml7YjzLBZIDPPH6mFhe14pD01bzbTb+h0xjj23uIIxX60F4OcN2azIzKdb87BGKYuIuCcl6Q1o0fb9gMaj16uyA/DRVVC2H+K6wuVvmF3bRUREROrJVT2b8da8bWzNLQXA39tKx/hgOiWE0DE+hLYxQbSKDsDf25OconIGvDyXVbsLeXf+Dm7sn1zjXk98vY6CsirHf0+ctZl3RvZu1PKIiHtRkt6AHJPGNZXx6Hlb4LcXodu1kNT/1O5lGDDv/2DrLzX3F+6G/O0Q3Az+MbXmOHQRERGReuBp9eDDm9JYkVlAm5hAkiMDjzrTe3SwLw9d0J6Hv1zDSz9mMKhjDM3CzB6UM9bu47s1+7B6WJh4dTfu+GQ5szNyWb27gFQtzSsiR6F10htIRbWN5Zn5APRp2QSS9JIc+PBSWPUJfHQl7F15aveb+zzMehJ2/FZzy98O3kEw7FMIiq2X0EVERET+Ki7Ejws7x5ESHXTcpdiu7pVI76RwyiptPDZ9LYZhUFBWyaPT1wFw65ktGZwax5CuCYDZmi4icjRqSW8gq3cXUlFtJzLQm1ZRgc4Op2FVlsEnV0NBJmCBqjL4eCjcPAtCTmLpklVTYc6hieDOfACi2tY83rwvBMedctgiIiIi9cHDw8Jzl3Xmwld+Y3ZGLt+s3secjBzySipIiQ7kjnNaA3D7OSlMX7mHnzfksHZPIZ0SQpwcuYi4IrWkN5BF28zx6L2Tw0/v8eh2O3z5T9izDHxDzcQ8qj2UZJmJekVx3e6343f46nbzdb+74OyHodPlNTcl6CIiIuJiUqIDuf3sFAAenraGacv3YLHAC1ekOpZdaxUVyEVd4gH4zy9qTReR2ilJbyCLHOujn+ZLr816AjZ8DR5ecPXHkNDD7IoeEA3Za+GzkWCrPrF75W2GKf8AexV0uAQGPN6goYuIiIjUp1vPaklKdCAlFebfPjf0S6b7X2Zyv+OcFCwWmLkumw37ipwRpoi4OCXpDaDKZmfZTnM8elpjjUcvL4KS3JpbXVux62rpO/D7K+brS16DpH7m69Dm8I8p4OkHW36CH+4/Mra/bge2mWPZywsgoSdcOhk89L+niIiIuA8fTyvjLuuMp4eFlpEB3Hte2yPOSYkOYnBns1egWtNFpDYak94A1u4ppKzSRqi/F22igxrhDafB5zcARs39Fg+4Zgq0GVT/77llFnx3j/n6rIehy9CaxxN6mMujTb0Olr5lbicitLkZs5df/cYrIiIi0gh6JoXzyz1nEeLnhZ937cvE3nFOa75dvY/v12SRkVVM29gT/3ux8GAV/52zhZyiCp64uCMhfl71FbqIuAg1VTaAw13deyWF43Gc2UBPmWHA3Bc4IkEHMOzw6/j6f8/s9fDpCDBskHo1nHl/7ee1vwgGvwTeJzhxXkQKDPscAqPqL1YRERGRRtY8wp8Q/6Mnz21jg7iws7lKzb8+XMauA2XHvafdbjB1SSbnvDiHyXO38eWKPfzzg6VUVNvqLW4RcQ1qSW8AhyeNa5T10bf/CrkbwCsARq8Hv1Bzf0kOvNwBdi+B3cugWY/6eb/iLPj4Kqgshhb94eKJcKyJ8XrdaG4iIiIi4vDQBe1ZtauQbXmlXPb6fN4b2ZsO8cG1nrs8M58nvl7H6t2FALSKCiC7qIKF2w4weuoq/nNNt4ZvGBKRRqOW9Hpmsxss3XF4ffRGmDRu0WTzZ9dr/kjQAQKjzZnQARZPrp/3qiw1l1or3AURrWHoB+DpUz/3FhEREWlCEsP9+eJffWkXG0RucQVDJy9g/pY8x/Fqm52f1mdz03tLuOy/81m9u5AgH08eHdyeGXf9jf9d1wMvq4Xv1uzjqW/XYxi19KoUEbekJL2erd9bRHFFNUG+nrSPq/1paL3J3wEZ35uve99y5PG0Q/vWToPi7FN7L7sNvrgZ9q4A/whzBnf/RpoUT0REROQ0FBviy9R/ppOWHE5xRTUj3lnMBwt2MO6HjaSP+4Wb31/KzxtyALiiRzNm3XsmN53REi+rB31TInnpqq4AvDt/B5PmbnNiSUSkPilJr2eLtptd3XslhWNt6G5Hi98ADGh1DkQdOXsoCT2gWW9zSbNl75z4ffeugIWTam5f3goZ34HVB67+BMJb1lsxRETEvb322mskJSXh6+tLWloaixcvPuq5b7zxBmeccQZhYWGEhYUxcODAI86//vrrsVgsNbbzzz+/oYsh4hQhfl68d0NvLuwcS5XN4LGv1jFp7lZyiyuICPDm5jOS+Xn033jxyi5EB/nWuPbiLvE89vcOADw/YyMfLtxJtc3ujGKISD3SmPR6drire++GHo9eUQLLPzBfp9169PPS/gm7F8OSt6D/aPD0PvZ9N800u7QbR/kH/tLXoXnaycUsIiKnnalTpzJ69GgmTZpEWloaEyZMYNCgQWRkZBAdHX3E+XPmzOGaa66hb9+++Pr68vzzz3Peeeexbt06EhISHOedf/75vPPOHw+YfXw0vEpOX75eVv5zTXfiQjbw/oId9E+JZGivRM5pF4O357Hb1G7sn0x2UTn/+3Ubj05fy4s/ZnBu+xjO7xRLv5RIfL1qn2FeRFyXxXCBASyvvfYa48ePJysriy5duvCf//yH3r1713rutGnTeO6559iyZQtVVVW0bt2ae+65h+uuu+6E3quoqIiQkBAKCwsJDq7/7ugDX57LlpwSPrixN2e0bsBZype8Bd+NhrBkuGP50dcUt1XBhM5QvA8uewNSrzr6PfethrfPh6pSSEyDkGZ/OmiBDhdDh0vqtRgiItLwdVNDSktLo1evXrz66qsA2O12EhMTueOOO3jwwQePe73NZiMsLIxXX32V4cOHA2ZLekFBAdOnTz/puNz5M5WmzW436jwJnN1u8PJPm/h4cSYHSisd+wO8raS3iqBb8zC6Nw+jS2II/t5qoxNxhrrUS07/La3rE/jw8HAeeeQR2rVrh7e3N99++y0jR44kOjqaQYMaYD3wOqiy2dm5vxSAVlEnuOzYyTCMPyaMS/vn0RN0AKsX9LwRZj8DiyYdPUkv3GPO2l5VCslnwrVfmNeKiIgcRWVlJcuWLeOhhx5y7PPw8GDgwIEsWLDghO5RVlZGVVUV4eE1e6DNmTOH6OhowsLCOOecc3jmmWeIiDj6hKwVFRVUVFQ4/ruoqKiOpRFxDSczS7uHh4V7B7XlroGtWbIjn5nrspixNousonJ+3pDjGNdu9bDQLjaIvq0iGNA+hp4twvC0avSriKtxekv6qT6BB+jevTuDBw/m6aefPu65DflkfVtuCee8NBd/bytrnxjUcEthbJ0NHwwx1x8fvR58Q459fkku/F8HsFXCTbOgWc+axyuK4e0LIHsNRLWDG2bWnCleREQalLu2+u7du5eEhATmz59Penq6Y//999/P3LlzWbRo0XHvcdtttzFz5kzWrVuHr6853nbKlCn4+/uTnJzM1q1befjhhwkMDGTBggVYrbV33X3iiSd48sknj9jvbp+pSH2x2w3W7ClkyY4DLM/MZ/nOArKKymucE+zryVltoxnQPpouzUJJDPdv+DmVRJoot2lJP9Un8IZh8Msvv5CRkcHzzz9f6zmN+WR9a67Zit4yKqBh16p0LLs27PgJOkBgFHS+ElZ+BAtfhyve+uOYrRo+v8FM0AOi4R+fKkEXEZFGMW7cOKZMmcKcOXMcCTrA1Vdf7XjduXNnUlNTadWqFXPmzGHAgAG13uuhhx5i9OjRjv8uKioiMTGx4YIXcXEeHha6JIbSJTHUsW9vwUGW7sxnzsYcZmfkkF9Wxder9vL1qr0AeHt60DIygJToQFpGBhwxHr7KZlB4sIqCskryy6ooOFiFt9XCgPYxDO4cR2K4f2MWUeS05dQkPS8vD5vNRkxMTI39MTExbNy48ajXFRYWkpCQQEVFBVarlf/+97+ce+65tZ47duzYWp+sN4QtOSVAA3d1P7ANNs0wX9e27NrR9L7FTNLXT4c3M//YX1EEuRvB0w+umQJhLeo1XBEROX1FRkZitVrJzq65zGd2djaxsbHHvPbFF19k3Lhx/Pzzz6Smph7z3JYtWxIZGcmWLVuOmqT7+PhocjmR44gP9ePiUD8u7hKPzW6wIjOfWRtz+G1zLpuzS6iotrMxq5iNWcV1uu+SHfmM+2Ejqc1CGNw5jsGpcTQLU8IucrKcPib9ZAQFBbFy5UpKSkqYNWsWo0ePpmXLlpx11llHnNuYT9a35jZCkr74TcCAlIEQmXLi18V3haQzYMdv5mzvNVjg8jegWY96DFRERE533t7e9OjRg1mzZjFkyBDAHLY2a9YsRo0addTrXnjhBZ599llmzpxJz549j3reYbt372b//v3ExcXVV+giTZ7Vw0LPpHB6JoXzwPntsNkNdueXsSWnhC05JWQeKMP+l1GxFouFUD8vQv29CPX3Jszfm+yicr5fs4+F2/azenchq3cXMm7GRs5uG821fZpzZpvoE+5CbxgGFou624s4NUk/2SfwHh4epKSYCWrXrl3ZsGEDY8eOrTVJb8wn6w2epFcUw4rDy679q+7XD/0AMhceubxaZNu6JfwiIiKHjB49mhEjRtCzZ0969+7NhAkTKC0tZeTIkQAMHz6chIQExo4dC8Dzzz/PmDFj+Pjjj0lKSiIrKwuAwMBAAgMDKSkp4cknn+Tyyy8nNjaWrVu3cv/995OSkuL0CWJFTmdWDwstIgJoERHAgPYxx7/gT67t04Lc4gpmrsvi29V7WbjtAL9szOGXjTk0C/NjWFoLhvZKJDzg6EsBf7NqL09/ux5PDwsdE0LoFB9Cp4RgOieEEB3se9TrRE5HTk3ST/YJ/F/Z7fYa486dwTAMth7u7h4d0DBvsmqK2T09IgVanVP36/3CoO0F9R+XiIg0WUOHDiU3N5cxY8aQlZVF165dmTFjhmMoW2ZmJh5/WoXk9ddfp7KykiuuuKLGfR5//HGeeOIJrFYrq1ev5r333qOgoID4+HjOO+88nn76aXVnF3FhUUE+XNunBdf2acH2vFI+WriTz5btZnf+QZ6fsZFJc7cy/opUzut4ZEPc2/O289S36x3/vbewnJ/Wm414Fgs8dEE7bvlbq0Yri4izOX1296lTpzJixAgmT57seAL/6aefsnHjRmJiYo54Aj927Fh69uxJq1atqKio4Pvvv+fBBx/k9ddf56abbjru+zXUDLq5xRX0evZnLBbY8NT5+HrVPvvsSbPb4bXesH8zXDAe0uowHl1ERFyau87u7sr0mYo438FKG9+s3stbv20nI9sc5z4ivQUPXdgeXy8rhmHw/IwMJs3d6jg2ODWetXsKWbu3kLV7CtmUXYLFAu+N7M3f2kQ5szgip8RtZneHuj+BLy0t5bbbbmP37t34+fnRrl07PvzwQ4YOHeqsIgB/dHVPDPOv/wQdYNsvZoLuHQRdr6n/+4uIiIiI1CM/bytX9UxkSNcExs/cyBu/bee9BTtZvCOfCUO7MvnXrUxbvgeA+wa15bazWmGxWOidHO64x0PT1vDJ4kzunLKCb+7orwnppElwekt6Y2uoJ+sfLdrJI1+u5ey2Ubwzsne93fePN7gSNv9ojkW/YFz9319ERJxGrb71T5+piOuZvTGHez5bxYHSSsc+q4eFsZd25qpetU/sXF5l48pJC1izp5AuzUL49NZ0fDwboEFMpIHVpV7yOOZROWFbc8w10htk0rj9W80EHQv0vrn+7y8iIiIi0sDObhfND3eeQd9WEQD4eHow+doeR03QAXy9rPx3WHdC/b1YtbuQp75Zf8Q5drtBRbWtweIWaWxO7+5+unDM7B7dAEn64v+ZP1ufBxGaNENERERE3FNMsC8f3JjGzHVZtI4OpHVM0HGvSQz3Z8LQrox8dwkfLcqke/MweieH8/uWPOZtyWPB1v2UVdp4Z2Qv+rSMaIRSiDQsJen1pMGWXysvghUfma/T/lm/9xYRERERaWRWDwsXdo6r0zVntY3m3+e05pVZm7n381XUNmD3to+W880d/UkI9aunSEWcQ93d68HBSht7Cg4C0CqqnpdfW/UJVBZDZJuTW3ZNREREROQ08O8BrTmzTRSGYSb6PVuE8e8Brfn45jQ6xgdzoLSSf36wlPIqdX0X96aW9FNRmgczHiSz26MYBoT6exEe4F1/97fbYdFk83XvW8yFIkVEREREmiCrh4X/De/Bmt2FtI0NIsjXy3Fs8nU9uPjV31m7p4iHpq3h5au6YNHfzuKmlKSfii9uhG1ziNqzFR/+TauosPr9x2DrLDiwFXyCoYuWXRMRERGRps3H00rPpPAj9jcL8+e1f3Tn2rcW8eWKPXRKCOHG/sk1zjlYacPTasHLevTOxDa7cWiMezV+3lZ8vaz4eVkJ9vUiMdzvqH/rG4bBf37Zwvdr9nFmmygu6hJPx/hgPSiQk6Ik/VRc8AK8eS7hB5Yz3msyv0eOrd/7L5pk/ux2Hfg0wIR0IiIiIiKnifRWETw6uD1PfrOe577fQHSQD9V2O8t3FrBsZz4bs4oI8PbkmrTmXN83ifg/jV2vttn5auVeXpu9hW15pbXe/4zWkbx0ZReig31r7K+stvPAF6v5coW55vvGrGIm/7qNpAh//p4azyVd409ogjyRw7RO+qnaNhfb+5dixcbyFjfSfeTLp35PgLzN8GpPwAL/XgHhyce9RERE3JPW9K5/+kxFmibDMLjns1VMW77nmOdZPSwM7hzHyH5JbM4p4bXZW9i5vwyAED8vWkYFcLDSRnmVjfIqO/tLK6iyGUQEePPilV04u100AIUHq7j1g2Us2LYfTw8Lt52dwtacEmZtzKa8yu54v3PaRXP72Sn0aBHWcIUXl1aXekkt6aeq5ZlM9B/F3WWv0H3nW7CiO3S79tTve3jZtbYXKEEXERERETkBFouF5y7tzK4DZazcVUDH+BB6tAijR4swujUPZcO+It74dTsLtu3n61V7+XrVXse14QHe3HxGS65Lb0GgT800aUtOMaM+XsHGrGJGvruEkf2SGJ6exD8/WMqm7BICvK28fm0P/tYmCoDSimp+3pDNN6v28svGHMeW3jKCUeek0LdVBJU2O3klleQWV7C/pII2MUEkhvs36uclrkkt6afIbjfo8PgMbjemcIfndPDwhGunQcszT/6m5YXwcgeoLIHhX0HLs045ThERcV1q9a1/+kxFmjbDMLDZDTyPMv587Z5C3p63na9X7SXU35t//q0lw/o0x9/76G2Y5VU2xv2wkXfn7wDMOZ0NA2KCfXj7+l50jA+p9brteaW8PmcL05bvodpupl4B3lZKK2vOQh/i58UPd55Roxv+n5VWVDNlyS76p0TSNlbd591NXeolJemnaHd+Gf2fn42PFTZ0/RyPddPAKwAiWh3/4rgu5rh27788MVvwX5j5EES1g9sWalZ3EZHTnBLK+qfPVEROREW1DU8PD6weJ/739qwN2dz3+WoOlFbSNiaId0b2Ompi/Wd7Cg7yv7lbmbJkFxXVZld4L6uFqEAfKqrt7C+tpHdSOJ/c0ueIeGx2g5veW8LsjFy8PT14+pKODO3VvG6FPQmlFdWMn5lBUoQ/w9OT8KjD5yQ1qbt7I9qaa04s0SIyEI8hr0PxPshcAFmrj39x1mo4mA9XfQAeh57y2W2w+NCya2n/VIIuIiIiItJAfDytdb5mQPsYZtx1Br9uymNQx5gaS8EdS0KoH09e0onR57Ylt6ScyEAfQvy8sFgs7NxfyuCJ81i84wCv/rKFOwe2rnHtCzM2MjsjFzg8Ud0alu7I5+khnfD1qnsZTkSVzc6/PlrOr5vM952dkcvLV3UhItDnhK43DIPc4oojJtqT41OSfoq25JQA0CoqELx8YcS3ZpJuqzj2hSW58M2/YeO38PMYOO8Zc//mnyB/B/iGQOrQhg1eRERERETqLDrIlyt6NDupa0P8vQjxr5nYt4gI4Jkhnbhr6kpembWJvikR9Dq01Ny05buZ/Os2ACZe041dB8p46ccMPlu2m7V7i3h9WHcig3xYu6eQVbsKWLW7gF0HDhIb4ktimD/Nw/1IDPcn1N+LA6VVHCitIK+kkv0llcSF+HJdeosjEn3DMHjgi9X8uikXXy+zMXHuplwunPgb/7mmO72Tj1wG7882ZhXxyJdrWbYzn9RmIQxPT+LvqXEN9kDhdKMk/RRtzf1Tkg5g9YTkM07sYquXudb6/P9AWDL0uvGPZde6DwfvgAaIWEREREREXM2Qbgn8ujmXacv3cOcnK/jhzr+xLa+EB6etAWDU2Slc3CUegG6Jofx7ygo27CvivAm/UmWz89dBzGv2FJ7Q+05duouXr+pCarNQx74XZmYwbfkerB4W/jusOwmh/tz20TK25pZy9f8WMPrcNtx2VsoR3d8PVtp4ZdZm3vxtm2P8/erdhdz72Sqe/W49V/VK5Nq0Fpog7zg0Jv0UDZ28gEXbD/B/Q7twabeTeJo2dzzMfgYsVjjvaZj5MFg84N8rIazFKccnIiKuT+On658+UxFxRyUV1fx94m/s2F/GWW2jWLe3iNziCs7tEMPka3vUSIqzCssZ9fFylu7MByA+xJcuiaF0SQwlOTKAnKJyMg+UsevAQTIPlFFUXkVEgDcRgT5EBHgT6u/Flyv2kldSgdXDwu1np3DHOSl8tHAnT3yzHoAXrkjlqp6JgDk+/bHpa5l2aD34IF9POsYH0zE+hE4JwXhZPRj3w0Z25x8E4LwOMdw5sDVzN+Xy0cJM9hSY+60eFq7pncjdA9uccNf504EmjjuG+q60ez7zM3klFXw9ql+Np08nzDBg+m2w6uM/9rX7O1z90SnHJiIi7kEJZf3TZyoi7mr17gIuf30+VTYzTWsbE8QXt/U9Ylk4gGqbnY1ZxUQH+xAdVPex3/mllTz61Vq+W70PgJZRAWzPK8Uw4N7z2jDqnJpj4w3D4LNlu3n6m/UUV1TXes+EUD+euLgj53aIceyz2Q1mbcjm/QU7mbclDzCT/H+f05oRfZPw9qx9Fv66+mHNPib9uo1z2kZz61ktT2rOgYaiJP0Y6rPSLiyrostTPwKw9slBtf7inJDqSvjwMtjxm/nfI7498S7zIiLi9pRQ1j99piLizv7361ae+34jYf5efD2qf4N3D/9m1V4e+2otBWVVAFzXpwVPXdIRy1Emsa6y2dmcXcK6vYWs21vEur2F7C0oZ3BqHHcOaE3AMfKi+VvzeObbDazfVwRAi0Mzx1dW28kvqyS/tJL8siqC/TzpnxJJ/9aRx30AUVRexRNfrXO08gMkRwbw9CWd6N868rjlLyirZN6WPEorqjm/Uxwhfic2GWBdKEk/hvqstJdn5nPZf+cTG+zLwocHnFpgB/Nh6nUQGA2Xv6VZ3UVEmhAllPVPn6mIuDPDMJidkUObmCCahTXO+O2conLGz8wgxM+Lhy5sX6dl6erKZjf4Ytluxv+YQW7xcSbcBtrFBnFG60h6JYXTIT6YhFA/xwOE+VvyuPezVewtLMfDAlf2SOSXjBzHfS/uEs+jf2/vSPQrqm2UVdjIPFDG3E25zMnIYeWuAg4Nocff28pVPRO5oV8yzSPq77NXkn4M9Vlpf7Z0F/d9vpp+KRF8dFOfeopQRESaGiWU9U+fqYiI6yupqOat37azZk8BIX7ehPl7EXZovPzu/IPM25zH2r2FR0yKF+zrSYf4YMIDvPl+TRZgtsi/fFUXerQIp6i8ipdmZvD+wp0YBvh6eeDrZaW0otoxjOCv2sQEYhiw+dDqXR4WGNQxlpvOSKZHi2PPZn8itE56Izm8RrpjZncRERERERE5IYE+nkesCf9nD5wPB0or+X1LHvM257F6TyGbs4spKq9m4bYDjvP+kdacRy5s7+hmH+zrxZOXdOLyHs145Mu1rNlTSHmVvca9g3w9SW8ZwVltozmzbRQJoX4YhsFvm/N4c952ft2Uyw9rs/hhbRYf35xG31bH7zZfX5Skn4LYYB+6NQ+lU3yIs0MRERERERE57YQHeHNRl3guOrT8XEW1jS05JazfW8T2vFL6too86rjz1GahTL+9H5tzirFaLPj7eBLo7Ym/jxUv65GT1VksFv7WJoq/tYkiI6uYt+ZtY/XuQvokRzRoGY+IQ93dRUREnEt1U/3TZyoiIvWh2mbHs5aEvq7qUi/Vz1z3IiIiIiIiIqeZ+kjQ60pJuoiIiJyS1157jaSkJHx9fUlLS2Px4sXHPP+zzz6jXbt2+Pr60rlzZ77//vsaxw3DYMyYMcTFxeHn58fAgQPZvHlzQxZBRETEZShJFxERkZM2depURo8ezeOPP87y5cvp0qULgwYNIicnp9bz58+fzzXXXMONN97IihUrGDJkCEOGDGHt2rWOc1544QUmTpzIpEmTWLRoEQEBAQwaNIjy8vLGKpaIiIjTuESSXpcn8G+88QZnnHEGYWFhhIWFMXDgwOM+sRcREZGG8fLLL3PzzTczcuRIOnTowKRJk/D39+ftt9+u9fxXXnmF888/n/vuu4/27dvz9NNP0717d1599VXAbEWfMGECjz76KJdccgmpqam8//777N27l+nTpzdiyURERJzD6Ul6XZ/Az5kzh2uuuYbZs2ezYMECEhMTOe+889izZ08jRy4iItK0VVZWsmzZMgYOHOjY5+HhwcCBA1mwYEGt1yxYsKDG+QCDBg1ynL99+3aysrJqnBMSEkJaWtpR7wlQUVFBUVFRjU1ERMQdOT1Jr+sT+I8++ojbbruNrl270q5dO958803sdjuzZs1q5MhFRESatry8PGw2GzExMTX2x8TEkJWVVes1WVlZxzz/8M+63BNg7NixhISEOLbExMQ6l0dERMQVODVJP5kn8H9VVlZGVVUV4eHhtR7Xk3UREZHT30MPPURhYaFj27Vrl7NDEhEROSlOTdJP5gn8Xz3wwAPEx8cf0XXuMD1ZFxERaRiRkZFYrVays7Nr7M/OziY2NrbWa2JjY495/uGfdbkngI+PD8HBwTU2ERERd+T07u6nYty4cUyZMoUvv/wSX1/fWs/Rk3UREZGG4e3tTY8ePWoMOTs8BC09Pb3Wa9LT048YovbTTz85zk9OTiY2NrbGOUVFRSxatOio9xQRETmdeDrzzU/mCfxhL774IuPGjePnn38mNTX1qOf5+Pjg4+NTL/GKiIhITaNHj2bEiBH07NmT3r17M2HCBEpLSxk5ciQAw4cPJyEhgbFjxwJw5513cuaZZ/LSSy8xePBgpkyZwtKlS/nf//4HgMVi4a677uKZZ56hdevWJCcn89hjjxEfH8+QIUOcVUwREZFG49Qk/c9P4A9XvIefwI8aNeqo173wwgs8++yzzJw5k549ezZStCIiIvJXQ4cOJTc3lzFjxpCVlUXXrl2ZMWOGYyhbZmYmHh5/dNzr27cvH3/8MY8++igPP/wwrVu3Zvr06XTq1Mlxzv33309paSm33HILBQUF9O/fnxkzZhy115yIiMjpxGIYhuHMAKZOncqIESOYPHmy4wn8p59+ysaNG4mJiTniCfzzzz/PmDFj+Pjjj+nXr5/jPoGBgQQGBh73/QoLCwkNDWXXrl0aryYiIi6hqKiIxMRECgoKCAkJcXY4pwXV9yIi4krqUtc7tSUd6v4E/vXXX6eyspIrrriixn0ef/xxnnjiieO+X3FxMYAmkBMREZdTXFysJL2eqL4XERFXdCJ1vdNb0hub3W5n7969BAUFYbFYTuleh5+GuPNTepXB+dw9flAZXIG7xw9NuwyGYVBcXEx8fHyNB9Ny8lTf/8Hd4weVwRW4e/ygMrgCd48fGqeud3pLemPz8PCgWbNm9XrP02GpF5XB+dw9flAZXIG7xw9NtwxqQa9fqu+P5O7xg8rgCtw9flAZXIG7xw8NW9frcb2IiIiIiIiIi1CSLiIiIiIiIuIilKSfAh8fHx5//HG3XoddZXA+d48fVAZX4O7xg8ogrsvdv1d3jx9UBlfg7vGDyuAK3D1+aJwyNLmJ40RERERERERclVrSRURERERERFyEknQRERERERERF6EkXURERERERMRFKEkXERERERERcRFK0k/Ba6+9RlJSEr6+vqSlpbF48WJnh3RUv/76KxdddBHx8fFYLBamT59e47hhGIwZM4a4uDj8/PwYOHAgmzdvdk6wtRg7diy9evUiKCiI6OhohgwZQkZGRo1zysvLuf3224mIiCAwMJDLL7+c7OxsJ0V8pNdff53U1FSCg4MJDg4mPT2dH374wXHc1eP/q3HjxmGxWLjrrrsc+1y9DE888QQWi6XG1q5dO8dxV4//sD179nDttdcSERGBn58fnTt3ZunSpY7jrvz7nJSUdMR3YLFYuP322wH3+A5sNhuPPfYYycnJ+Pn50apVK55++mn+PA+rK38HUjeq6xuP6nrXo7reedy5rgf3r++dXtcbclKmTJlieHt7G2+//baxbt064+abbzZCQ0ON7OxsZ4dWq++//9545JFHjGnTphmA8eWXX9Y4Pm7cOCMkJMSYPn26sWrVKuPiiy82kpOTjYMHDzon4L8YNGiQ8c477xhr1641Vq5caVx44YVG8+bNjZKSEsc5t956q5GYmGjMmjXLWLp0qdGnTx+jb9++Toy6pq+//tr47rvvjE2bNhkZGRnGww8/bHh5eRlr1641DMP14/+zxYsXG0lJSUZqaqpx5513Ova7ehkef/xxo2PHjsa+ffscW25uruO4q8dvGIZx4MABo0WLFsb1119vLFq0yNi2bZsxc+ZMY8uWLY5zXPn3OScnp8bn/9NPPxmAMXv2bMMw3OM7ePbZZ42IiAjj22+/NbZv32589tlnRmBgoPHKK684znHl70BOnOr6xqW63rWornced6/rDcP963tn1/VK0k9S7969jdtvv93x3zabzYiPjzfGjh3rxKhOzF8rbrvdbsTGxhrjx4937CsoKDB8fHyMTz75xAkRHl9OTo4BGHPnzjUMw4zXy8vL+OyzzxznbNiwwQCMBQsWOCvM4woLCzPefPNNt4q/uLjYaN26tfHTTz8ZZ555pqPidocyPP7440aXLl1qPeYO8RuGYTzwwANG//79j3rc3X6f77zzTqNVq1aG3W53m+9g8ODBxg033FBj32WXXWYMGzbMMAz3+w7k6FTXO5fqeudRXe9cp1tdbxjuV987u65Xd/eTUFlZybJlyxg4cKBjn4eHBwMHDmTBggVOjOzkbN++naysrBrlCQkJIS0tzWXLU1hYCEB4eDgAy5Yto6qqqkYZ2rVrR/PmzV2yDDabjSlTplBaWkp6erpbxX/77bczePDgGrGC+3wHmzdvJj4+npYtWzJs2DAyMzMB94n/66+/pmfPnlx55ZVER0fTrVs33njjDcdxd/p9rqys5MMPP+SGG27AYrG4zXfQt29fZs2axaZNmwBYtWoV8+bN44ILLgDc6zuQo1Nd73yq651Hdb1znU51Pbhnfe/sut7zlO/QBOXl5WGz2YiJiamxPyYmho0bNzopqpOXlZUFUGt5Dh9zJXa7nbvuuot+/frRqVMnwCyDt7c3oaGhNc51tTKsWbOG9PR0ysvLCQwM5Msvv6RDhw6sXLnSLeKfMmUKy5cvZ8mSJUccc4fvIC0tjXfffZe2bduyb98+nnzySc444wzWrl3rFvEDbNu2jddff53Ro0fz8MMPs2TJEv7973/j7e3NiBEj3Or3efr06RQUFHD99dcD7vH/EMCDDz5IUVER7dq1w2q1YrPZePbZZxk2bBjgfv+mSu1U1zuX6nrnUV3vfKdTXQ/uWd87u65Xki5u5/bbb2ft2rXMmzfP2aHUWdu2bVm5ciWFhYV8/vnnjBgxgrlz5zo7rBOya9cu7rzzTn766Sd8fX2dHc5JOfz0EyA1NZW0tDRatGjBp59+ip+fnxMjO3F2u52ePXvy3HPPAdCtWzfWrl3LpEmTGDFihJOjq5u33nqLCy64gPj4eGeHUieffvopH330ER9//DEdO3Zk5cqV3HXXXcTHx7vddyDiqlTXO4fqetdwOtX14J71vbPrenV3PwmRkZFYrdYjZiDMzs4mNjbWSVGdvMMxu0N5Ro0axbfffsvs2bNp1qyZY39sbCyVlZUUFBTUON/VyuDt7U1KSgo9evRg7NixdOnShVdeecUt4l+2bBk5OTl0794dT09PPD09mTt3LhMnTsTT05OYmBiXL8NfhYaG0qZNG7Zs2eIW3wFAXFwcHTp0qLGvffv2jq587vL7vHPnTn7++Wduuukmxz53+Q7uu+8+HnzwQa6++mo6d+7Mddddx913383YsWMB9/kO5NhU1zuP6nrnUV3vGk6Xuh7ct753dl2vJP0keHt706NHD2bNmuXYZ7fbmTVrFunp6U6M7OQkJycTGxtbozxFRUUsWrTIZcpjGAajRo3iyy+/5JdffiE5ObnG8R49euDl5VWjDBkZGWRmZrpMGWpjt9upqKhwi/gHDBjAmjVrWLlypWPr2bMnw4YNc7x29TL8VUlJCVu3biUuLs4tvgOAfv36HbEk0aZNm2jRogXgHr/PAO+88w7R0dEMHjzYsc9dvoOysjI8PGpWn1arFbvdDrjPdyDHprq+8amudz7V9a7hdKnrwX3re6fX9ac89VwTNWXKFMPHx8d49913jfXr1xu33HKLERoaamRlZTk7tFoVFxcbK1asMFasWGEAxssvv2ysWLHC2Llzp2EY5hICoaGhxldffWWsXr3auOSSS1xqGYd//etfRkhIiDFnzpwayzmUlZU5zrn11luN5s2bG7/88ouxdOlSIz093UhPT3di1DU9+OCDxty5c43t27cbq1evNh588EHDYrEYP/74o2EYrh9/bf4846thuH4Z7rnnHmPOnDnG9u3bjd9//90YOHCgERkZaeTk5BiG4frxG4a5JI6np6fx7LPPGps3bzY++ugjw9/f3/jwww8d57j677PNZjOaN29uPPDAA0ccc4fvYMSIEUZCQoJjWZZp06YZkZGRxv333+84x9W/Azkxqusbl+p616S6vvGdDnW9Ybh3fe/sul5J+in4z3/+YzRv3tzw9vY2evfubSxcuNDZIR3V7NmzDeCIbcSIEYZhmMsIPPbYY0ZMTIzh4+NjDBgwwMjIyHBu0H9SW+yA8c477zjOOXjwoHHbbbcZYWFhhr+/v3HppZca+/btc17Qf3HDDTcYLVq0MLy9vY2oqChjwIABjkrbMFw//tr8teJ29TIMHTrUiIuLM7y9vY2EhARj6NChNdYcdfX4D/vmm2+MTp06GT4+Pka7du2M//3vfzWOu/rv88yZMw2g1pjc4TsoKioy7rzzTqN58+aGr6+v0bJlS+ORRx4xKioqHOe4+ncgJ051feNRXe+aVNc7h7vX9Ybh3vW9s+t6i2EYxqm3x4uIiIiIiIjIqdKYdBEREREREREXoSRdRERERERExEUoSRcRERERERFxEUrSRURERERERFyEknQRERERERERF6EkXURERERERMRFKEkXERERERERcRFK0kVERERERERchJJ0EWlUc+bMwWKxUFBQ4OxQREREpIGovhc5eUrSRURERERERFyEknQRERERERERF6EkXaSJsdvtjB07luTkZPz8/OjSpQuff/458EfXtO+++47U1FR8fX3p06cPa9eurXGPL774go4dO+Lj40NSUhIvvfRSjeMVFRU88MADJCYm4uPjQ0pKCm+99VaNc5YtW0bPnj3x9/enb9++ZGRkNGzBRUREmhDV9yLuS0m6SBMzduxY3n//fSZNmsS6deu4++67ufbaa5k7d67jnPvuu4+XXnqJJUuWEBUVxUUXXURVVRVgVrZXXXUVV199NWvWrOGJJ57gscce491333VcP3z4cD755BMmTpzIhg0bmDx5MoGBgTXieOSRR3jppZdYunQpnp6e3HDDDY1SfhERkaZA9b2IGzNEpMkoLy83/P39jfnz59fYf+ONNxrXXHONMXv2bAMwpkyZ4ji2f/9+w8/Pz5g6daphGIbxj3/8wzj33HNrXH/fffcZHTp0MAzDMDIyMgzA+Omnn2qN4fB7/Pzzz4593333nQEYBw8erJdyioiINGWq70Xcm1rSRZqQLVu2UFZWxrnnnktgYKBje//999m6davjvPT0dMfr8PBw2rZty4YNGwDYsGED/fr1q3Hffv36sXnzZmw2GytXrsRqtXLmmWceM5bU1FTH67i4OABycnJOuYwiIiJNnep7Effm6ewARKTxlJSUAPDdd9+RkJBQ45iPj0+Nivtk+fn5ndB5Xl5ejtcWiwUwx8+JiIjIqVF9L+Le1JIu0oR06NABHx8fMjMzSUlJqbElJiY6zlu4cKHjdX5+Pps2baJ9+/YAtG/fnt9//73GfX///XfatGmD1Wqlc+fO2O32GmPeREREpPGovhdxb2pJF2lCgoKCuPfee7n77rux2+3079+fwsJCfv/9d4KDg2nRogUATz31FBEREcTExPDII48QGRnJkCFDALjnnnvo1asXTz/9NEOHDmXBggW8+uqr/Pe//wUgKSmJESNGcMMNNzBx4kS6dOnCzp07ycnJ4aqrrnJW0UVERJoM1fcibs7Zg+JFpHHZ7XZjwoQJRtu2bQ0vLy8jKirKGDRokDF37lzHJC/ffPON0bFjR8Pb29vo3bu3sWrVqhr3+Pzzz40OHToYXl5eRvPmzY3x48fXOH7w4EHj7rvvNuLi4gxvb28jJSXFePvttw3D+GMimfz8fMf5K1asMABj+/btDV18ERGRJkH1vYj7shiGYTjzIYGIuI45c+Zw9tlnk5+fT2hoqLPDERERkQag+l7EtWlMuoiIiIiIiIiLUJIuIiIiIiIi4iLU3V1ERERERETERaglXURERERERMRFKEkXERERERERcRFK0kVERERERERchJJ0ERERERERERehJF1ERERERETERShJFxEREREREXERStJFREREREREXISSdBEREREREREX8f9bZoEEpjFb0AAAAABJRU5ErkJggg==",
      "text/plain": [
       "<Figure size 1200x400 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Train and Validation accuracy and loss\n",
    "f = plt.figure(figsize = (12,4))\n",
    "\n",
    "plt.subplot(1,2,1)\n",
    "plt.title('train validation accuracy')\n",
    "plt.plot(history11.history['accuracy'])\n",
    "plt.plot(history11.history['val_accuracy'])\n",
    "plt.legend(['train','validation'])\n",
    "plt.xlabel('epoch')\n",
    "plt.ylabel('accuracy')\n",
    "\n",
    "\n",
    "plt.subplot(1,2,2)\n",
    "plt.title('train validation loss')\n",
    "plt.plot(history11.history['loss'])\n",
    "plt.plot(history11.history['val_loss'])\n",
    "plt.legend(['train','validation'])\n",
    "plt.xlabel('epoch')\n",
    "plt.ylabel('loss')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "8a6d039a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3/3 [==============================] - 0s 3ms/step\n"
     ]
    }
   ],
   "source": [
    "y_prob = model11.predict(X2_test_scaled)\n",
    "y_pred = np.argmax(y_prob, axis = -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "05c0ae4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Accuracy\n",
    "acc_1d = round(accuracy_score(y_pred,y2_test),2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "470773d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Confusion Matrix\n",
    "cm_1d = confusion_matrix(y2_test,y_pred)\n",
    "cm_display_1d = ConfusionMatrixDisplay(confusion_matrix = cm_1d, display_labels = ['Angry', 'Happy', 'Relaxed', 'Sad'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "f25642c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "model11.save('Features1D/Model/Conv1D.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "7f32ad08",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "       Angry       0.95      0.90      0.92        20\n",
      "       Happy       0.83      0.95      0.88        20\n",
      "     Relaxed       0.79      0.75      0.77        20\n",
      "         Sad       0.79      0.75      0.77        20\n",
      "\n",
      "    accuracy                           0.84        80\n",
      "   macro avg       0.84      0.84      0.84        80\n",
      "weighted avg       0.84      0.84      0.84        80\n",
      "\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "print(classification_report(y2_test, y_pred, target_names = ['Angry', 'Happy', 'Relaxed', 'Sad']))\n",
    "cm_display_1d.plot()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e0531d5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
